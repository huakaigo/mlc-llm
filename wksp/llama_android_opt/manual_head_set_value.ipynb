{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tvm\n",
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm import autotvm, auto_scheduler\n",
    "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
    "from tvm import meta_schedule as ms\n",
    "from tvm.ir import IRModule\n",
    "from tvm import relax\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils, ndk\n",
    "vocab_size = 50000\n",
    "@I.ir_module\n",
    "class ModuleToManual:\n",
    "    @T.prim_func\n",
    "    def main(lv1803: T.Buffer((T.int64(512), T.int64(vocab_size)), \"uint32\"), lv1804: T.Buffer((T.int64(128), T.int64(vocab_size)), \"float16\"), lv3152: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"), p_output0_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(vocab_size)), \"float32\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        p_output0_intermediate_1 = T.alloc_buffer((T.int64(4096), T.int64(vocab_size)), \"float16\")\n",
    "        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(vocab_size)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(vocab_size)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv1803[v_i // T.int64(8), v_j], lv1804[v_i // T.int64(32), v_j])\n",
    "                T.writes(p_output0_intermediate_1[v_i, v_j])\n",
    "                p_output0_intermediate_1[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv1803[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv1804[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(vocab_size), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv3152[v_i0, v_i1, v_k], p_output0_intermediate_1[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv3152[v_i0, v_i1, v_k] * p_output0_intermediate_1[v_k, v_i2]\n",
    "        for i, j, k in T.grid(T.int64(1), T.int64(1), T.int64(vocab_size)):\n",
    "            with T.block(\"set_padding_value\"):\n",
    "                v_i, v_j, v_k = T.axis.remap(\"SSS\", [i, j, k])\n",
    "                T.reads(var_matmul_intermediate[v_i, v_j, v_k])\n",
    "                T.writes(p_output0_intermediate[v_i, v_j, v_k])\n",
    "                p_output0_intermediate[v_i, v_j, v_k] = T.if_then_else(v_k < T.int64(49954), T.Cast(\"float32\", var_matmul_intermediate[v_i, v_j, v_k]), T.float32(-65504))\n",
    "\n",
    "@I.ir_module\n",
    "class ModuleSrc:\n",
    "    @T.prim_func\n",
    "    def main(lv1803: T.Buffer((T.int64(512), T.int64(vocab_size)), \"uint32\"), lv1804: T.Buffer((T.int64(128), T.int64(vocab_size)), \"float16\"), lv3152: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"), p_output0_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(vocab_size)), \"float32\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        p_output0_intermediate_1 = T.alloc_buffer((T.int64(4096), T.int64(vocab_size)), \"float16\")\n",
    "        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(vocab_size)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(vocab_size)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv1803[v_i // T.int64(8), v_j], lv1804[v_i // T.int64(32), v_j])\n",
    "                T.writes(p_output0_intermediate_1[v_i, v_j])\n",
    "                p_output0_intermediate_1[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv1803[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv1804[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(vocab_size), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv3152[v_i0, v_i1, v_k], p_output0_intermediate_1[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv3152[v_i0, v_i1, v_k] * p_output0_intermediate_1[v_k, v_i2]\n",
    "        for i, j, k in T.grid(T.int64(1), T.int64(1), T.int64(vocab_size)):\n",
    "            with T.block(\"set_padding_value\"):\n",
    "                v_i, v_j, v_k = T.axis.remap(\"SSS\", [i, j, k])\n",
    "                T.reads(var_matmul_intermediate[v_i, v_j, v_k])\n",
    "                T.writes(p_output0_intermediate[v_i, v_j, v_k])\n",
    "                p_output0_intermediate[v_i, v_j, v_k] = T.if_then_else(v_k < T.int64(49954), T.Cast(\"float32\", var_matmul_intermediate[v_i, v_j, v_k]), T.float32(-65504))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// Function: main_kernel\n",
      "#ifdef cl_khr_fp16\n",
      "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
      "#elif defined(cl_amd_fp16)\n",
      "#pragma OPENCL EXTENSION cl_amd_fp16 : enable\n",
      "#else\n",
      "#error \"Half precision floating point not supported by OpenCL implementation on your device.\" \n",
      "#endif\n",
      "\n",
      "__kernel void main_kernel(__global uint* restrict lv1803, __global half* restrict lv1804, __global half* restrict lv3152, __global float* restrict p_output0_intermediate) {\n",
      "  __local half lv3152_shared[4096];\n",
      "  half var_matmul_intermediate_local[4];\n",
      "  half4 lv1804_local[1];\n",
      "  uint4 lv1803_local[1];\n",
      "  for (int ax2_0 = 0; ax2_0 < 11; ++ax2_0) {\n",
      "    for (int ax2_2_s = 0; ax2_2_s < 4; ++ax2_2_s) {\n",
      "      if (((ax2_0 * 25) + ((convert_int(get_local_id(0))) >> 2)) < 256) {\n",
      "        lv3152_shared[(((ax2_0 * 400) + ((convert_int(get_local_id(0))) * 4)) + ax2_2_s)] = lv3152[(((ax2_0 * 400) + ((convert_int(get_local_id(0))) * 4)) + ax2_2_s)];\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  vstore4(((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f)), 0, var_matmul_intermediate_local + 0);\n",
      "  barrier(CLK_LOCAL_MEM_FENCE);\n",
      "  for (int k_0 = 0; k_0 < 512; ++k_0) {\n",
      "    lv1804_local[0] = vload4(0, lv1804 + ((((k_0 >> 2) * 50000) + ((convert_int(get_group_id(0))) * 400)) + ((convert_int(get_local_id(0))) * 4)));\n",
      "    for (int k_1 = 0; k_1 < 8; ++k_1) {\n",
      "      lv1803_local[0] = vload4(0, lv1803 + (((k_0 * 50000) + ((convert_int(get_group_id(0))) * 400)) + ((convert_int(get_local_id(0))) * 4)));\n",
      "      vstore4((vload4(0, var_matmul_intermediate_local + 0) + (((half4)(lv3152_shared[((k_0 * 8) + k_1)], lv3152_shared[((k_0 * 8) + k_1)], lv3152_shared[((k_0 * 8) + k_1)], lv3152_shared[((k_0 * 8) + k_1)])) * (((convert_half4(((lv1803_local[0]  >>  ((uint4)(((convert_uint(k_1)) * (uint)4), ((convert_uint(k_1)) * (uint)4), ((convert_uint(k_1)) * (uint)4), ((convert_uint(k_1)) * (uint)4))))  &  ((uint4)((uint)15, (uint)15, (uint)15, (uint)15))))) - ((half4)((half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f))) * lv1804_local[0]))), 0, var_matmul_intermediate_local + 0);\n",
      "    }\n",
      "  }\n",
      "  for (int ax2_s = 0; ax2_s < 4; ++ax2_s) {\n",
      "    p_output0_intermediate[((((convert_int(get_group_id(0))) * 400) + ((convert_int(get_local_id(0))) * 4)) + ax2_s)] = ((((((convert_int(get_group_id(0))) * 200) + ((convert_int(get_local_id(0))) * 2)) + (ax2_s >> 1)) < 24977) ? (convert_float(var_matmul_intermediate_local[ax2_s])) : -6.550400e+04f);\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ref to mlc-llm/dispatch/dispatch_tir_operator_adreno.py\n",
    "def sch_manual_opt(func):\n",
    "    sch = tvm.tir.Schedule(func)\n",
    "    b0 = sch.get_block(name=\"decode\", func_name=\"main\")\n",
    "    b1 = sch.get_block(name=\"matmul\", func_name=\"main\")\n",
    "    l2, l3, l4, l5 = sch.get_loops(block=b1)\n",
    "    l6 = sch.fuse(l2, l3, l4, preserve_unit_iters=True)\n",
    "    # v7, v8, v9 = sch.sample_perfect_tile(\n",
    "    #     loop=l6, n=3, max_innermost_factor=4, decision=[None, 100, 4]\n",
    "    # )\n",
    "\n",
    "    # l10, l11, l12 = sch.split(loop=l6, factors=[v7, v8, v9], preserve_unit_iters=True)\n",
    "    l10, l11, l12 = sch.split(loop=l6, factors=[None, 100, 4], preserve_unit_iters=True)\n",
    "    v13, v14, v15 = sch.sample_perfect_tile(\n",
    "        loop=l5, n=3, max_innermost_factor=8, decision=[512, 8, 1]\n",
    "    )\n",
    "    l16, l17, l18 = sch.split(\n",
    "        loop=l5, factors=[v13, v14, v15], preserve_unit_iters=True\n",
    "    )\n",
    "    sch.reorder(l10, l11, l16, l17, l18, l12)\n",
    "    sch.bind(loop=l10, thread_axis=\"blockIdx.x\")\n",
    "    sch.bind(loop=l11, thread_axis=\"threadIdx.x\")\n",
    "    sch.compute_inline(block=b0)\n",
    "    b19 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")\n",
    "    sch.reverse_compute_at(block=b19, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    b20 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"local\")\n",
    "    b21 = sch.cache_read(block=b1, read_buffer_index=2, storage_scope=\"local\")\n",
    "    b22 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")\n",
    "    sch.compute_at(block=b22, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    v23 = sch.sample_categorical(\n",
    "        candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=1\n",
    "    )\n",
    "    sch.annotate(\n",
    "        block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v23\n",
    "    )\n",
    "    sch.compute_at(block=b20, loop=l17, preserve_unit_loops=True, index=-1)\n",
    "    sch.compute_at(block=b21, loop=l16, preserve_unit_loops=True, index=-1)\n",
    "    l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)\n",
    "    sch.vectorize(loop=l29)\n",
    "    l30, l31, l32, l33, l34 = sch.get_loops(block=b21)\n",
    "    sch.vectorize(loop=l34)\n",
    "    l35, l36, l37, l38, l39 = sch.get_loops(block=b19)\n",
    "    sch.vectorize(loop=l39)\n",
    "    sch.vectorize(loop=l12)\n",
    "    b40 = sch.decompose_reduction(block=b1, loop=l16)\n",
    "    b41 = sch.get_block(name=\"set_padding_value\", func_name=\"main\")\n",
    "    sch.reverse_compute_inline(block=b41)\n",
    "    # b42 = sch.get_block(name=\"set_padding_value\", func_name=\"main\")\n",
    "    # sch.reverse_compute_inline(block=b42)\n",
    "    sch.enter_postproc()\n",
    "    sch.unannotate(block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\")\n",
    "    l42, l43, l44, l45, l46 = sch.get_loops(block=b22)\n",
    "    l47, l48, l49 = sch.split(\n",
    "        loop=l46, factors=[None, 100, 4], preserve_unit_iters=True\n",
    "    )\n",
    "    sch.vectorize(loop=l49)\n",
    "    sch.bind(loop=l48, thread_axis=\"threadIdx.x\")\n",
    "    return sch.mod[\"main\"].with_attr(\"tir.is_scheduled\", 1)\n",
    "\n",
    "sch_manual = tvm.tir.Schedule(ModuleToManual)\n",
    "sch_manual.mod['main'] = sch_manual_opt(sch_manual.mod['main'])\n",
    "# print(sch_manual.mod.script())\n",
    "# print(\"================================================\")\n",
    "rt_mod = tvm.build(sch_manual.mod, target=\"opencl\")\n",
    "print(rt_mod.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda -keys=cuda,gpu -arch=sm_61 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12050/4205193854.py:28: RuntimeWarning: overflow encountered in cast\n",
      "  Output_np = np.empty((1, 1, vocab_size)).astype(\"float16\")\n"
     ]
    }
   ],
   "source": [
    "# run and compare with cuda\n",
    "import numpy as np\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "Output_np = np.empty((1, 1, vocab_size)).astype(\"float16\")\n",
    "# W_nd = tvm.nd.array(W_np, dev)\n",
    "# S_nd = tvm.nd.array(S_np, dev)\n",
    "# Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Traceback (most recent call last):\n  10: 0xffffffffffffffff\n  9: 0x00000000005b7412\n  8: __libc_start_main\n  7: ffi_call\n  6: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:511\n  5: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:471\n  4: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:415\n  3: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:286\n  2: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/ir/transform.cc:101\n  1: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:721\n  0: tvm::tir::StorageLegalizer::Legalize(tvm::tir::PrimFunc)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:478\n  File \"/data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc\", line 478\nInternalError: Check failed: func->buffer_map.size() == 0 (4 vs. 0) : This pass must be called after MakePackedAPI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# cuda未优化版本测试\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m manual_rt_mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mbuild(sch_manual\u001b[39m.\u001b[39mmod, target\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m manual_evaluator \u001b[39m=\u001b[39m manual_rt_mod\u001b[39m.\u001b[39mtime_evaluator(\u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m, dev, number\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmanual_evaluator GEMV-Blocking: \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m GFLOPS\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (num_flop \u001b[39m/\u001b[39m manual_evaluator(W_nd, S_nd, Input_nd, Output_nd)\u001b[39m.\u001b[39mmean \u001b[39m/\u001b[39m \u001b[39m1e9\u001b[39m))\n",
      "File \u001b[0;32m/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/driver/build_module.py:281\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    277\u001b[0m     target_host \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m tvm\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39menabled(\u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstackvm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 281\u001b[0m rt_mod_host \u001b[39m=\u001b[39m _driver_ffi\u001b[39m.\u001b[39mtir_to_runtime(annotated_mods, target_host)\n\u001b[1;32m    283\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(target_host, Target):\n",
      "File \u001b[0;32m/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/_ffi/_ctypes/packed_func.py:238\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    226\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    237\u001b[0m ):\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    240\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mInternalError\u001b[0m: Traceback (most recent call last):\n  10: 0xffffffffffffffff\n  9: 0x00000000005b7412\n  8: __libc_start_main\n  7: ffi_call\n  6: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:511\n  5: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:471\n  4: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:415\n  3: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:286\n  2: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/ir/transform.cc:101\n  1: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:721\n  0: tvm::tir::StorageLegalizer::Legalize(tvm::tir::PrimFunc)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:478\n  File \"/data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc\", line 478\nInternalError: Check failed: func->buffer_map.size() == 0 (4 vs. 0) : This pass must be called after MakePackedAPI"
     ]
    }
   ],
   "source": [
    "\n",
    "# cuda未优化版本测试\n",
    "manual_rt_mod = tvm.build(sch_manual.mod, target=\"cuda\")\n",
    "manual_evaluator = manual_rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "print(\"manual_evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / manual_evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Traceback (most recent call last):\n  10: 0xffffffffffffffff\n  9: 0x00000000005b7412\n  8: __libc_start_main\n  7: ffi_call\n  6: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:511\n  5: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:471\n  4: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:415\n  3: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:286\n  2: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/ir/transform.cc:101\n  1: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:721\n  0: tvm::tir::StorageLegalizer::Legalize(tvm::tir::PrimFunc)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:478\n  File \"/data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc\", line 478\nInternalError: Check failed: func->buffer_map.size() == 0 (4 vs. 0) : This pass must be called after MakePackedAPI",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m target:\n\u001b[1;32m      4\u001b[0m     src_gpu_mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mtir\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mDefaultGPUSchedule()(sch\u001b[39m.\u001b[39mmod) \u001b[39m##\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m rt_mod \u001b[39m=\u001b[39m tvm\u001b[39m.\u001b[39mbuild(src_gpu_mod, target\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/driver/build_module.py:281\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    277\u001b[0m     target_host \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m tvm\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39menabled(\u001b[39m\"\u001b[39m\u001b[39mllvm\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstackvm\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 281\u001b[0m rt_mod_host \u001b[39m=\u001b[39m _driver_ffi\u001b[39m.\u001b[39mtir_to_runtime(annotated_mods, target_host)\n\u001b[1;32m    283\u001b[0m annotated_mods, target_host \u001b[39m=\u001b[39m Target\u001b[39m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    285\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(target_host, Target):\n",
      "File \u001b[0;32m/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/_ffi/_ctypes/packed_func.py:238\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    226\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     _LIB\u001b[39m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    237\u001b[0m ):\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    239\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    240\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mInternalError\u001b[0m: Traceback (most recent call last):\n  10: 0xffffffffffffffff\n  9: 0x00000000005b7412\n  8: __libc_start_main\n  7: ffi_call\n  6: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:511\n  5: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:471\n  4: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:415\n  3: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/driver/driver_api.cc:286\n  2: tvm::tir::transform::PrimFuncPassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/ir/transform.cc:101\n  1: operator()\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:721\n  0: tvm::tir::StorageLegalizer::Legalize(tvm::tir::PrimFunc)\n        at /data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc:478\n  File \"/data/workspace/llm/github/new_wksp/tvm-unity/src/tir/transforms/unsupported_dtype_legalize.cc\", line 478\nInternalError: Check failed: func->buffer_map.size() == 0 (4 vs. 0) : This pass must be called after MakePackedAPI"
     ]
    }
   ],
   "source": [
    "# cuda优化版本测试\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "rt_mod = tvm.build(src_gpu_mod, target=\"cuda\")\n",
    "# print(rt_mod.get_source())\n",
    "# evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "# print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TVM_NDK_CC\"]=\"/home/sensetime/Android/Sdk/ndk/25.2.9519653/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android33-clang++\"\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.4.236.32\"\n",
    "rpc_port = 9190\n",
    "comp_target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-android\")  # TODO: Only support arm64 for now\n",
    "\n",
    "def test_opencl(mod: tvm.IRModule, name_hint: str):\n",
    "    # mod = tvm.lower(sch_manual.mod)\n",
    "    print(\"Build ...\")\n",
    "    android_rt_mod = tvm.build(mod, target=\"opencl\", target_host=\"llvm -mtriple=aarch64-linux-android\")\n",
    "    # print(android_rt_mod.imported_modules[0].get_source())\n",
    "    temp = utils.tempdir()\n",
    "    path_dso_cl = temp.relpath(\"dev_lib_cl.so\")\n",
    "    android_rt_mod.export_library(path_dso_cl, ndk.create_shared)\n",
    "\n",
    "    print(\"Run GPU(OpenCL Flavor) test ...\")\n",
    "    # Establish remote connection with target hardware\n",
    "\n",
    "    tracker = rpc.connect_tracker(rpc_host, rpc_port)\n",
    "    remote = tracker.request(device_key, priority=0, session_timeout=60)\n",
    "    print(\"Connect to device done.\")\n",
    "    dev = remote.cl(0)\n",
    "    remote.upload(path_dso_cl)\n",
    "    f1 = remote.load_module(\"dev_lib_cl.so\")\n",
    "\n",
    "    W_nd = tvm.nd.array(W_np, dev)\n",
    "    S_nd = tvm.nd.array(S_np, dev)\n",
    "    Input_nd = tvm.nd.array(Input_np, dev)\n",
    "    Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n",
    "    test_number = 10\n",
    "    time_f = f1.time_evaluator(f1.entry_name, dev, number=test_number)\n",
    "    cost = time_f(W_nd, S_nd, Input_nd, Output_nd).mean\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %f ms with loop %d\" % (name_hint, cost * 1000, test_number))\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %f GFLOPS\" % (name_hint, num_flop / cost / 1e9))\n",
    "\n",
    "    return Output_nd.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[source] GEMV-Blocking: 240.796954 ms with loop 10\n",
      "evaluator[source] GEMV-Blocking: 5.101422 GFLOPS\n",
      "[[[ -6628.  -6336.  -6248. ... -65504. -65504. -65504.]]]\n"
     ]
    }
   ],
   "source": [
    "# 未优化版本opencl测试\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "src_output = test_opencl(src_gpu_mod, \"source\")\n",
    "print(src_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[opted] GEMV-Blocking: 3.256704 ms with loop 10\n",
      "evaluator[opted] GEMV-Blocking: 377.193256 GFLOPS\n",
      "[[[ -5624.  -6124.  -6040. ... -65504. -65504. -65504.]]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not equal\n\n(shapes (1, 1, 50000), (1, 1, 49984) mismatch)\n x: array([[[ -5624.,  -6124.,  -6040., ..., -65504., -65504., -65504.]]],\n      dtype=float32)\n y: array([[[ -6628.,  -6336.,  -6248., ..., -65504., -65504., -65504.]]],\n      dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m opt_output \u001b[39m=\u001b[39m test_opencl(sch_manual\u001b[39m.\u001b[39mmod, \u001b[39m\"\u001b[39m\u001b[39mopted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(opt_output)\n\u001b[0;32m----> 4\u001b[0m np\u001b[39m.\u001b[39mtesting\u001b[39m.\u001b[39massert_equal(opt_output, src_output)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlc-chat-venv/lib/python3.11/contextlib.py:81\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     79\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m     80\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlc-chat-venv/lib/python3.11/site-packages/numpy/testing/_private/utils.py:717\u001b[0m, in \u001b[0;36massert_array_compare\u001b[0;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict)\u001b[0m\n\u001b[1;32m    711\u001b[0m         reason \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m(dtypes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m mismatch)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    712\u001b[0m     msg \u001b[39m=\u001b[39m build_err_msg([x, y],\n\u001b[1;32m    713\u001b[0m                         err_msg\n\u001b[1;32m    714\u001b[0m                         \u001b[39m+\u001b[39m reason,\n\u001b[1;32m    715\u001b[0m                         verbose\u001b[39m=\u001b[39mverbose, header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m    716\u001b[0m                         names\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m), precision\u001b[39m=\u001b[39mprecision)\n\u001b[0;32m--> 717\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(msg)\n\u001b[1;32m    719\u001b[0m flagged \u001b[39m=\u001b[39m bool_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    720\u001b[0m \u001b[39mif\u001b[39;00m isnumber(x) \u001b[39mand\u001b[39;00m isnumber(y):\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nArrays are not equal\n\n(shapes (1, 1, 50000), (1, 1, 49984) mismatch)\n x: array([[[ -5624.,  -6124.,  -6040., ..., -65504., -65504., -65504.]]],\n      dtype=float32)\n y: array([[[ -6628.,  -6336.,  -6248., ..., -65504., -65504., -65504.]]],\n      dtype=float32)"
     ]
    }
   ],
   "source": [
    "#优化版本opencl测试\n",
    "opt_output = test_opencl(sch_manual.mod, \"opted\")\n",
    "print(opt_output)\n",
    "np.testing.assert_equal(opt_output, src_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.158.176.30\"\n",
    "rpc_port = 5001\n",
    "# remote = autotvm.measure.request_remote(device_key, \"10.158.176.30\", 5001, timeout=10000)\n",
    "# dev = remote.device(str(target), 0)\n",
    "\n",
    "# num_flop = 1228406784\n",
    "# W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "# S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "# Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# # Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# W_nd = tvm.nd.array(W_np, dev)\n",
    "# S_nd = tvm.nd.array(S_np, dev)\n",
    "# Input_nd = tvm.nd.array(Input_np, dev)\n",
    "# Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc_config = ms.runner.RPCConfig(tracker_host=rpc_host, tracker_port=rpc_port, tracker_key = device_key)\n",
    "runner= ms.runner.RPCRunner(rpc_config)\n",
    "# ms.builder.LocalBuilder()\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "database = ms.tune_tir(\n",
    "    mod=ModuleSrc,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_first\",\n",
    "    cost_model=\"xgb\",\n",
    "    runner = runner\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, sch.mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.script import relax as R\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def main(A: R.Tensor((3, 4), dtype=\"float32\"), B: R.Tensor((4, 5), dtype=\"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((3, 5), dtype=\"float32\") = R.matmul(A, B)\n",
    "            gv: R.Tensor((3, 5), dtype=\"float32\") = lv\n",
    "            R.output(gv)\n",
    "        return gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto_scheduler test\n",
    "from tvm import auto_scheduler\n",
    "import numpy as np\n",
    "a_np = np.random.rand(3, 4).astype(\"float32\")\n",
    "b_np = np.random.rand(4, 5).astype(\"float32\")\n",
    "a_nd = tvm.runtime.NDArray(a_np)\n",
    "b_nd = tvm.runtime.NDArray(b_np)\n",
    "sch = tvm.tir.Schedule(Module)\n",
    "\n",
    "params = {\"A\": a_np, \"B\": b_np}\n",
    "## 报错，这里只支持relay\n",
    "# tasks = auto_scheduler.extract_tasks(sch.mod, params, target=target)\n",
    "tasks = ms.relax_integration.extract_tasks(sch.mod, target=target, params=params)\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mod_deploy import Module as ModuleAll\n",
    "params_all = {}\n",
    "tasks_all = auto_scheduler.extract_tasks(ModuleAll, params_all, target=target)\n",
    "print(len(tasks_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_file = \"tune.json\"\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "W_nd = tvm.nd.array(W_np, dev)\n",
    "S_nd = tvm.nd.array(S_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "new_mod = sch.mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = auto_scheduler.SearchTask(func=sch.mod['fused_fused_decode11_fused_matmul5_cast2'], args=sch.mod['fused_fused_decode11_fused_matmul5_cast2'].params, target=target)\n",
    "\n",
    "# tune_option = auto_scheduler.TuningOptions(\n",
    "#     num_measure_trials=10,\n",
    "#     measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "\n",
    "database = ms.tune_tir(\n",
    "    mod=new_mod,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_45593_1\",\n",
    "    cost_model=\"xgb\"\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, new_mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sch1.trace)\n",
    "# print(sch1.mod.script())\n",
    "rt_mod = tvm.build(sch1.mod, target=\"cuda\")\n",
    "\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (1228406784 / evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_database = ms.Database.create(kind='json', work_dir='./tune_45593_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sch = ms.tir_integration.compile_tir(record_database, new_mod, target)\n",
    "\n",
    "record_rt_mod = tvm.build(record_sch.mod, target=\"cuda\")\n",
    "\n",
    "record_evaluator = record_rt_mod.time_evaluator(\"main\", dev, number=20)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / record_evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(record_sch.trace)\n",
    "print(record_sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Dict, List, Optional, Union, Callable\n",
    "from tvm import runtime\n",
    "if TYPE_CHECKING:\n",
    "    import numpy as np  # type: ignore\n",
    "    from tvm.ir import IRModule\n",
    "    from tvm.meta_schedule.runner import EvaluatorConfig, RPCConfig\n",
    "    from tvm.runtime import Device, Module, NDArray\n",
    "    from tvm.target import Target\n",
    "    from tvm.runtime.vm import Executable\n",
    "\n",
    "\n",
    "def f_measurement(\n",
    "    rt_mod: runtime.Module, device: runtime.ndarray.Device, input_data: Dict[str, runtime.NDArray]\n",
    "):\n",
    "    vm = relax.VirtualMachine(rt_mod, device=device)\n",
    "    vm.save_function(\"main\", \"measure_func\", **input_data, include_return=False)\n",
    "    evaluator = vm.time_evaluator(\n",
    "        func_name=\"measure_func\",\n",
    "        dev=device,\n",
    "        repeat=100,\n",
    "        number=1,\n",
    "        min_repeat_ms=500,\n",
    "    )\n",
    "    return evaluator()\n",
    "\n",
    "def run_module_via_rpc(\n",
    "    rpc_config: \"RPCConfig\",\n",
    "    lib: Union[\"Module\", \"Executable\"],\n",
    "    dev_type: str,\n",
    "    args: Union[Dict[int, \"np.ndarray\"], Dict[str, \"np.ndarray\"]],\n",
    "    continuation: Callable,\n",
    "    backend: Optional[str] = \"graph\",\n",
    "):\n",
    "    \"\"\"Execute a tvm.runtime.Module on RPC remote\"\"\"\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    from tvm.contrib.tar import tar\n",
    "    from tvm.runtime import ndarray\n",
    "\n",
    "    # pylint: enable=import-outside-toplevel\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + tar.output_format)\n",
    "        filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + \"so\")\n",
    "        if backend == \"vm\":\n",
    "            code, lib = lib.save(filename, fmt=\"so\")\n",
    "        from tvm.contrib import ndk\n",
    "        lib.export_library(filename, ndk.create_shared)\n",
    "        session = rpc_config.connect_server()\n",
    "        print(type(session._sess))\n",
    "        session.upload(filename)\n",
    "        _, filename = os.path.split(filename)\n",
    "        rt_mod = session.load_module(filename)\n",
    "        \n",
    "        if backend == \"vm\":\n",
    "            rt_mod = session.get_function(\"runtime.Load_Executable\")(code, rt_mod)\n",
    "            # rt_mod = session.get_function(\"runtime.module.loadfile_relax.Executable\")(filename)\n",
    "        dev = session.device(dev_type=dev_type, dev_id=0)\n",
    "        # print(dev)\n",
    "        # create the remote runtime module\n",
    "        print(rt_mod)\n",
    "        print(rt_mod['main'])\n",
    "        from tvm.contrib import graph_executor as runtime\n",
    "        module = runtime.GraphModule(rt_mod[\"main\"](dev))\n",
    "        print(module)\n",
    "        for k, v in args.items():\n",
    "            module.set_input(k, tvm.nd.array(v))\n",
    "        return module.run()\n",
    "        # nd_args = {k: ndarray.array(v, dev) for k, v in args.items()}\n",
    "        nd_args = {k: ndarray.empty(v.shape, v.dtype, dev) for k, v in args.items()}\n",
    "        return continuation(rt_mod, dev, nd_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
