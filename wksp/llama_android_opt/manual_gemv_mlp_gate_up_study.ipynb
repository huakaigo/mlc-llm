{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tvm\n",
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm import autotvm, auto_scheduler\n",
    "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
    "from tvm import meta_schedule as ms\n",
    "from tvm.ir import IRModule\n",
    "from tvm import relax\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils, ndk\n",
    "x_shape = 4096\n",
    "w_w_x = 512\n",
    "w_s_x = 128\n",
    "w_y = 11008*2\n",
    "func_name = \"main\"\n",
    "@I.ir_module\n",
    "class ModuleSrc:\n",
    "    @T.prim_func(private=False)\n",
    "    def main(lv1611: T.Buffer((T.int64(512), T.int64(w_y)), \"uint32\"),\n",
    "             lv1612: T.Buffer((T.int64(128), T.int64(w_y)), \"float16\"),\n",
    "             lv1622: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"),\n",
    "             p_output0_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(w_y)), \"float16\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        var_decode_intermediate = T.alloc_buffer((T.int64(4096), T.int64(w_y)), \"float16\")\n",
    "        var_matmul_intermediate = T.alloc_buffer(\n",
    "            (T.int64(1), T.int64(1), T.int64(w_y)), \"float16\"\n",
    "        )\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(w_y)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv1611[v_i // T.int64(8), v_j], lv1612[v_i // T.int64(32), v_j])\n",
    "                T.writes(var_decode_intermediate[v_i, v_j])\n",
    "                var_decode_intermediate[v_i, v_j] = (\n",
    "                    T.Cast(\n",
    "                        \"float16\",\n",
    "                        T.bitwise_and(\n",
    "                            T.shift_right(\n",
    "                                lv1611[v_i // T.int64(8), v_j],\n",
    "                                T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4),\n",
    "                            ),\n",
    "                            T.uint32(15),\n",
    "                        ),\n",
    "                    )\n",
    "                    - T.float16(7)\n",
    "                ) * lv1612[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(w_y), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv1622[v_i0, v_i1, v_k], var_decode_intermediate[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = (\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2]\n",
    "                    + lv1622[v_i0, v_i1, v_k] * var_decode_intermediate[v_k, v_i2]\n",
    "                )\n",
    "        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(w_y)):\n",
    "            with T.block(\"T_update\"):\n",
    "                v_ax0, v_ax1, v_ax2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                T.writes(p_output0_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                p_output0_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2]\n",
    "\n",
    "@I.ir_module\n",
    "class ModuleToManual:\n",
    "    @T.prim_func(private=False)\n",
    "    def main(lv1611: T.Buffer((T.int64(512), T.int64(w_y)), \"uint32\"),\n",
    "             lv1612: T.Buffer((T.int64(128), T.int64(w_y)), \"float16\"),\n",
    "             lv1622: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"),\n",
    "             p_output0_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(w_y)), \"float16\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        var_decode_intermediate = T.alloc_buffer((T.int64(4096), T.int64(w_y)), \"float16\")\n",
    "        var_matmul_intermediate = T.alloc_buffer(\n",
    "            (T.int64(1), T.int64(1), T.int64(w_y)), \"float16\"\n",
    "        )\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(w_y)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv1611[v_i // T.int64(8), v_j], lv1612[v_i // T.int64(32), v_j])\n",
    "                T.writes(var_decode_intermediate[v_i, v_j])\n",
    "                var_decode_intermediate[v_i, v_j] = (\n",
    "                    T.Cast(\n",
    "                        \"float16\",\n",
    "                        T.bitwise_and(\n",
    "                            T.shift_right(\n",
    "                                lv1611[v_i // T.int64(8), v_j],\n",
    "                                T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4),\n",
    "                            ),\n",
    "                            T.uint32(15),\n",
    "                        ),\n",
    "                    )\n",
    "                    - T.float16(7)\n",
    "                ) * lv1612[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(w_y), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv1622[v_i0, v_i1, v_k], var_decode_intermediate[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = (\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2]\n",
    "                    + lv1622[v_i0, v_i1, v_k] * var_decode_intermediate[v_k, v_i2]\n",
    "                )\n",
    "        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(w_y)):\n",
    "            with T.block(\"T_update\"):\n",
    "                v_ax0, v_ax1, v_ax2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "                T.reads(var_matmul_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                T.writes(p_output0_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                p_output0_intermediate[v_ax0, v_ax1, v_ax2] = var_matmul_intermediate[v_ax0, v_ax1, v_ax2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "// Function: main_kernel\n",
      "#ifdef cl_khr_fp16\n",
      "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
      "#elif defined(cl_amd_fp16)\n",
      "#pragma OPENCL EXTENSION cl_amd_fp16 : enable\n",
      "#else\n",
      "#error \"Half precision floating point not supported by OpenCL implementation on your device.\" \n",
      "#endif\n",
      "\n",
      "__kernel void main_kernel(__global uint* restrict lv1611, __global half* restrict lv1612, __global half* restrict lv1622, __global half* restrict p_output0_intermediate) {\n",
      "  __local half lv1622_shared[4096];\n",
      "  half8 var_matmul_intermediate_local[1];\n",
      "  half8 lv1612_local[1];\n",
      "  uint8 lv1611_local[1];\n",
      "  for (int ax2_0 = 0; ax2_0 < 8; ++ax2_0) {\n",
      "    vstore8(vload8(0, lv1622 + ((ax2_0 * 512) + ((convert_int(get_local_id(0))) * 8))), 0, lv1622_shared + ((ax2_0 * 512) + ((convert_int(get_local_id(0))) * 8)));\n",
      "  }\n",
      "  var_matmul_intermediate_local[0] = ((half8)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f));\n",
      "  barrier(CLK_LOCAL_MEM_FENCE);\n",
      "  for (int k_0 = 0; k_0 < 128; ++k_0) {\n",
      "    lv1612_local[0] = vload8(0, lv1612 + (((k_0 * 22016) + ((convert_int(get_group_id(0))) * 512)) + ((convert_int(get_local_id(0))) * 8)));\n",
      "    for (int k_1 = 0; k_1 < 4; ++k_1) {\n",
      "      lv1611_local[0] = vload8(0, lv1611 + ((((k_0 * 88064) + (k_1 * 22016)) + ((convert_int(get_group_id(0))) * 512)) + ((convert_int(get_local_id(0))) * 8)));\n",
      "      for (int k_2 = 0; k_2 < 8; ++k_2) {\n",
      "        var_matmul_intermediate_local[0] = (var_matmul_intermediate_local[0] + (((half8)(lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)])) * (((convert_half8(((lv1611_local[0]  >>  ((uint8)(((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4))))  &  ((uint8)((uint)15, (uint)15, (uint)15, (uint)15, (uint)15, (uint)15, (uint)15, (uint)15))))) - ((half8)((half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f))) * lv1612_local[0])));\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  vstore8(var_matmul_intermediate_local[0], 0, p_output0_intermediate + (((convert_int(get_group_id(0))) * 512) + ((convert_int(get_local_id(0))) * 8)));\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ref to mlc-llm/dispatch/dispatch_tir_operator_adreno.py\n",
    "def sch_fused_decode5_fused_matmul6_silu1(func):\n",
    "    \"\"\"\n",
    "    Summarize\n",
    "    --------------------------------\n",
    "    gemv整体分析优化内容:\n",
    "    1. 内存方面\n",
    "        \n",
    "        1.1 将参与多次计算的 `v` 搬运到shared_memory中\n",
    "        \n",
    "        1.2 将参与单次运算, 且较小的 quantized_weight_scale搬运到local memory中, 这一步感觉作用不大，可以做实验看下去掉的效果\n",
    "\n",
    "        1.3 通过tiling, 使计算满足cache line, 提高cache命中\n",
    "    \n",
    "    2. 计算方面\n",
    "        \n",
    "        2.1 计算合并(内联), 如将dequantize操作与gemv合并, 将中间结果从local_memory -> register, 降低中间缓存及其内存搬运导致的耗时, 大幅度提高计算效率\n",
    "\n",
    "        2.2 通过split和reorder操作实现tiling, 提高cache命中, 减少内存搬运\n",
    "\n",
    "        2.3 绑定硬件线程(block & thread)\n",
    "\n",
    "        2.4 使用vectorize, 利用SIMD指令集加速 load/store & ALU计算\n",
    "\n",
    "        2.5 实际上还将内存搬运(global memory -> local/shared memory)与计算独立开来，提高内存搬运效率(需要看生成的源码如何实现)\n",
    "\n",
    "        2.6 `v` 的shared memory搬移时, 使用cooperative_fetch, 暂时没搞明白如何实现, 需要生成的cuda代码\n",
    "\n",
    "    3. 改进点?\n",
    "        没有看到用unroll操作, 其余的好像都用到了 -- 用了 vectorize, 不需要unroll了\n",
    "        \n",
    "    opencl source\n",
    "    ----------------------------------------------------------------\n",
    "    // Function: main_kernel\n",
    "    #ifdef cl_khr_fp16\n",
    "    #pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
    "    #elif defined(cl_amd_fp16)\n",
    "    #pragma OPENCL EXTENSION cl_amd_fp16 : enable\n",
    "    #else\n",
    "    #error \"Half precision floating point not supported by OpenCL implementation on your device.\" \n",
    "    #endif\n",
    "    // lv1611 - w_int4\n",
    "    // lv1612 - scale (group size = 32)\n",
    "    // lv1622 - input x\n",
    "    // kernel:  输入 [512, 11008], [128, 11008], [1,1,4096] -> [1,1 11008]\n",
    "    //          工作组设置: blockIdx = 43, threadIdx = 64\n",
    "    //          计算: output[4] = input[4096] x 4列W [1,1,4096] => [4]\n",
    "    __kernel void main_kernel(__global uint* restrict lv1611, __global half* restrict lv1612, __global half* restrict lv1622, __global half* restrict p_output0_intermediate) {\n",
    "    // 这里标注的是local内存, 即\n",
    "    __local half lv1622_shared[4096];\n",
    "    // 以下三个half4都是private memory, 如果resgiter足够的话, 会放到resgiter中, 如果不够会放到 local memory -> global memory\n",
    "    half4 var_matmul_intermediate_local[1];\n",
    "    half4 lv1612_local[1];\n",
    "    uint4 lv1611_local[1];\n",
    "    ## local_id范围[0,63], 64*8*8=4096, 即一个workgroup把 [1,1,4096]的输入都读到了shared_memory中\n",
    "    for (int ax2_0 = 0; ax2_0 < 8; ++ax2_0) {\n",
    "        vstore8(vload8(0, lv1622 + ((ax2_0 * 512) + ((convert_int(get_local_id(0))) * 8))), 0, lv1622_shared + ((ax2_0 * 512) + ((convert_int(get_local_id(0))) * 8)));\n",
    "    }\n",
    "    // 中间缓存初始化\n",
    "    var_matmul_intermediate_local[0] = ((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f));\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    # W的对应关系, 按行存储: lv1622_shared[v_i0, v_i1, v_k], lv1611_local[v_k // T.int64(8), v_i2], lv1612_local[v_k // T.int64(32), v_i2]\n",
    "    for (int k_0 = 0; k_0 < 128; ++k_0) {\n",
    "        // load scale(group size = 32), shape[128, 11008]\n",
    "        // 一个group_id处理256个数据, 一个local_id处理(k0 * 4)个数据(reudce) --> blockIdx = 43, threadIdx = 64, 即每次读取 [1, 4] 个Scale数据 -> [1, 4*32]个Weight的scale\n",
    "        lv1612_local[0] = vload4(0, lv1612 + (((k_0 * 11008) + ((convert_int(get_group_id(0))) * 256)) + ((convert_int(get_local_id(0))) * 4)));\n",
    "        for (int k_1 = 0; k_1 < 4; ++k_1) {\n",
    "        // load w_int4, shape[512, 11008]\n",
    "        // 一个group_id处理256个数据, 一个local_id处理(k1 * k0 * 4)个数据 --> blockIdx = 43, threadIdx = 64, 即每次读取 [1, 4] 个 Weight数据 -> [1, 4*8]个Wieght的量化数据\n",
    "        lv1611_local[0] = vload4(0, lv1611 + ((((k_0 * 44032) + (k_1 * 11008)) + ((convert_int(get_group_id(0))) * 256)) + ((convert_int(get_local_id(0))) * 4)));\n",
    "        // weight: u32 X 4 -> 8 X int4 X 4\n",
    "        for (int k_2 = 0; k_2 < 8; ++k_2) {\n",
    "            // half4 ouput = output + (half4 lv1622_shared[k_0*32 + k_1*8 + k_2]) * half4\n",
    "            var_matmul_intermediate_local[0] = (var_matmul_intermediate_local[0] + (((half4)(lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)])) * (((convert_half4(((lv1611_local[0]  >>  ((uint4)(((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4))))  &  ((uint4)((uint)15, (uint)15, (uint)15, (uint)15))))) - ((half4)((half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f))) * lv1612_local[0])));\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    vstore4((var_matmul_intermediate_local[0] * (((half4)((half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f)) / (((half4)((half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f)) + exp((((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f)) - var_matmul_intermediate_local[0]))))), 0, p_output0_intermediate + (((convert_int(get_group_id(0))) * 256) + ((convert_int(get_local_id(0))) * 4)));\n",
    "    }\n",
    "    \"\"\" \n",
    "    sch = tvm.tir.Schedule(func)\n",
    "    b0 = sch.get_block(name=\"decode\", func_name=\"main\") # decode([512,11008], [128, 11008]) -> [4096, 11008]\n",
    "    b1 = sch.get_block(name=\"matmul\", func_name=\"main\") # matmul([4096], [4096, 11008], ) -> [11008]\n",
    "    # matmul: 1, 1, 11008, 4096\n",
    "    l2, l3, l4, l5 = sch.get_loops(block=b1)\n",
    "    # l6 = 11008\n",
    "    l6 = sch.fuse(l2, l3, l4, preserve_unit_iters=True)\n",
    "    # v7 = 43, v8 = 64, v9 = 4\n",
    "    # 搜索记录:\n",
    "    #   None, 512, 2   2.039464ms\n",
    "    #   43, 256, 2     1.007784ms\n",
    "    #   None, 128, 2   1.154256ms\n",
    "    #   None, 64, 2    1.558120ms\n",
    "\n",
    "    #   None, 256, 4   1.893800ms\n",
    "    #   None, 200, 4   0.906336ms\n",
    "    #   None, 180, 4   2.019104ms\n",
    "    #   None, 160, 4   1.849120ms\n",
    "    #   None, 140, 4   2.168464ms\n",
    "    #   None, 129, 4   2.527432ms\n",
    "    #   43, 128, 4     0.917504ms  best\n",
    "    #   None, 100, 4   2.207560ms\n",
    "\n",
    "    #   86, 64, 4      1.106272ms\n",
    "    #   172, 32, 4     1.738040ms\n",
    "    #   None, 128, 8   37.157400ms\n",
    "    #   None, 80, 8    38.082024ms\n",
    "    #   43, 64, 8      1.292280ms\n",
    "    #   None, 50, 8    35.535704ms\n",
    "    #   86, 32, 8      2.030792ms\n",
    "    ### 注: None会不关心是否会整除, 会引入判断语句来防止越界\n",
    "    blockIdxX = None\n",
    "    threadidxX = 64\n",
    "    vectorize_factor = 8\n",
    "    # v7, v8, v9 = sch.sample_perfect_tile(\n",
    "    #     loop=l6, n=3, max_innermost_factor=4, decision=[blockIdxX, threadidxX, vectorize_factor]\n",
    "    # )\n",
    "    # print(sch.get(v7)) #可以打印值\n",
    "    # spatial axes l6 = 11008 -> l10 = 43, l11 = 64, l12 = 4\n",
    "    # l10, l11, l12 = sch.split(loop=l6, factors=[v7, v8, v9], preserve_unit_iters=True)\n",
    "    l10, l11, l12 = sch.split(loop=l6, factors=[blockIdxX, threadidxX, vectorize_factor], preserve_unit_iters=True)\n",
    "\n",
    "    # l5 = 4096 -> v13 = 128, v14 = 4, v15 = 8\n",
    "    v13, v14, v15 = sch.sample_perfect_tile(\n",
    "        loop=l5, n=3, max_innermost_factor=8, decision=[128, 4, 8]\n",
    "    )\n",
    "    # reduce l5 = 4096 -> l16 = 128, l17 = 4, l18 = 8\n",
    "    l16, l17, l18 = sch.split(\n",
    "        loop=l5, factors=[v13, v14, v15], preserve_unit_iters=True\n",
    "    )\n",
    "    # spatial loopRV: l10, l11, l12\n",
    "    # reduce loopRV: l16, l17, l18\n",
    "    ## 从这里来看规律：\n",
    "    ###     spatial可以拆分，内层factor l12可以放到最内层循环\n",
    "    ###     reduce可以做拆分，并移动到spatial外层循环和内层循环之间，效果上就是每个reduce position可以执行多次spatial操作(即spatial内层)\n",
    "    ####        优化效果: 1. 在spatial内层操作时，输出的element不变，减少了输出内存的读取；但是对于W来说，需要隔行去读取element, 因此需要多计算几个reduce(列)以利用cache\n",
    "    ####        优化效果: 2. 由于W shape = [4096, 11008], x[4096] * shape的操作，相当于x与每列W做运算，这会导致cache命中极低，导致cache thrashing(缓存抖动)\n",
    "    ####            调整后, 对于W的读取，每一个k_inner(l17 * l18 = 4 * 8, fp16数据类型，正好是64bytes)块中会做s_inner(l12 = 4)次运算, \n",
    "    ####            ** 相当于4个cache line同时计算, 实际上4行这个数字取决于cache的大小，这里应该可以更大\n",
    "    ####            ** 相当于，做了tiling操作, 为什么要将k_inner拆分，目前理解是可以用l18(8)来做vectorize的load/store/alu 指令(即SIMD)\n",
    "    ####        优化效果: 3. 对于外层spatial可以绑定到硬件线程了\n",
    "    sch.reorder(l10, l11, l16, l17, l18, l12)\n",
    "    # l10 = 43, 绑定到blockIdx\n",
    "    sch.bind(loop=l10, thread_axis=\"blockIdx.x\")\n",
    "    # l11 = 64, 绑定到threadIdx,  64对于adereno gpu来说有意义，因为cache line = 64bytes\n",
    "    # l10*l11 = 2752, AD4X后的GPU最大的workgroup size(threadIdx.x * threadIdx.y * threadIdx.z) 一般为1024， \n",
    "    sch.bind(loop=l11, thread_axis=\"threadIdx.x\")\n",
    "    # 将decode过程内联到使用decode结果的block中去，这里就是将decode放到matmul中\n",
    "    ## 从最终优化上来看：将 w = dequant(scale); o = x * w 合并为 => o = x * dequant(scale), 即将语句合并了\n",
    "    ##      由于gemv中每个w值参与一次计算, 所以这一步可以节省中间缓存的内存搬运消耗，并可以节省memory, 理论上直接计算的中间结果会存在register，访存更快\n",
    "    ##      @TODO 但是这个计算合并的操作是不是`compute_inline`做的，还不清楚 -- 看`compute_inline`的example是这样的\n",
    "    ### 在compute_inline之前, block(b1).reads = [lv1622, var_decode_intermediate], writes = [var_matmul_intermediate]\n",
    "    sch.compute_inline(block=b0)\n",
    "    ## block(b1).reads = [lv1622, lv1611, lv1612], writes = [var_matmul_intermediate]\n",
    "    ## 从reads的顺序来看, compute_inline会把b0.reads插到b1.reads后面,  同理writes应该也是这样, 但是这里没有writes, 所以没体现\n",
    "    ### 也就是compute_inline把var_decode_intermediate中间变量给省去了，印证了之前的猜测\n",
    "    ## b19指向的是 var_matmul_intermediate的赋值过程，即从global memory -> local memroy\n",
    "    ## 也就是b19是一个block, 并且包含 loops, b19.loops(未经调整)=b19.shape\n",
    "    b19 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")\n",
    "    # reverse_compute_at 将一个消费者block(b19)移动到指定的loop(l11)中，并且重新生成该block包含的loops(暂时理解为绑定到该block的loops)\n",
    "    ##   这样该block(b19)消费的buffer(write index 0, 即var_matmul_intermediate)区域就可以覆盖\n",
    "    ##    指定loop(l11)下生产者blocks(一个或多个，这里指decode)生成的buffer区域(暂时理解为var_decode_intermediate)\n",
    "    # b19: [1, 1, 11008]\n",
    "    # b19插入后: [l10, l11, 1, 1, 1*1*11008/(l10*l11)] -> [43, 64, 1, 1, 4] 这个只是纯赋值的loop grid，在update时还要将reduce插入\n",
    "    sch.reverse_compute_at(block=b19, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    # 执行完reverse_compute_at, b1.reads和writes的内容没变\n",
    "    # 将lv1611[512,11008]设置为local内存\n",
    "    ## 这个内存设置奇怪, 只用一次, 不需要线放到local中\n",
    "    b20 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"local\")\n",
    "    # 将lv1612[128,11008]设置为local内存\n",
    "    ## 这个内存设置奇怪, 只用一次, 不需要线放到local中\n",
    "    b21 = sch.cache_read(block=b1, read_buffer_index=2, storage_scope=\"local\")\n",
    "    # 将lv1622[1,1,4096]设置为shared内存\n",
    "    b22 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")\n",
    "    # 将一个生产者block(b22)移动到指定loop(l11)中，并且重新生成该block(b22)包含的loops, 这样该block(b22)生产的buffer区域就可以覆盖指定loop(l11)下消费者blocks消费的buffer区域\n",
    "    ## preserve_unit_loops的作用: 如将(128, 129) compute_at (128)下，若为true则会有 for 128 (for(1, 129)), 即保留shape中value为1的loop, 若为False, 则有for 128 (for(129))\n",
    "    # b22: lv1622[1,1,4096]\n",
    "    ## 猜测shape: [l10, l11, 1, 1, 4096/(l10 * l11)] @TODO 无法整除咋整.. 无法整除就没整除 -> [l10, l11, 1, 1, 4096], 后面重新划分4096, 重新绑定threadIdx.x, 再者shared_memory不能绑定到blockIdx中, 所以必须重新绑定\n",
    "    ## 观察到shape: [[l10, l11], [1,1,4096]] 即b22的初始化还在[l10,l11]的loop内，但是压根没按照这个计算, 这种情况得看下最终的cuda代码是如何处理的\n",
    "    ##### for i0_i1_i2_fused_1 in T.thread_binding(T.int64(64), thread=\"threadIdx.x\"):\n",
    "    #####     lv1622_shared = T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\", scope=\"shared\")\n",
    "    #####     for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):\n",
    "    #####         with T.block(\"lv1622_shared\"):\n",
    "    #####             v0, v1, v2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "    #####             lv1622 = T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\")\n",
    "    #####             T.reads(lv1622[v0, v1, v2])\n",
    "    #####             T.writes(lv1622_shared[v0, v1, v2])\n",
    "    #####             lv1622_shared[v0, v1, v2] = lv1622[v0, v1, v2]\n",
    "    sch.compute_at(block=b22, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    # print(sch.get(l11))\n",
    "    v23 = sch.sample_categorical(\n",
    "        candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=3\n",
    "    )\n",
    "    # 做标注，实际形式会在b22中生成 T.block_attr({\"ann_key\", \"ann_value\"})\n",
    "    ## 标注了 lv1622[4096] cooperative_fetch:8\n",
    "    ## 在opencl kernel中实现了每个block中,不同ThreadIdx, 通过vload8加载数据到shared中\n",
    "    sch.annotate(\n",
    "        block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v23\n",
    "    )\n",
    "    # 这里有点奇怪，b20和b21是decode需要用到的buffer, 而l16 l17是reduce loop拆分的前两个subloop, 为什么会绑定到不同\n",
    "    sch.compute_at(block=b20, loop=l17, preserve_unit_loops=True, index=-1)\n",
    "    sch.compute_at(block=b21, loop=l16, preserve_unit_loops=True, index=-1)\n",
    "    # @TODO 猜测b20_0: b20的原始loop: [512, 11008], 插入到l17后变为: [l10, l11, l16, l17, 512, 11008]\n",
    "    # l10 = 43, l11 = 64, l16 = 128, l17 = 4 => [l10, l11, l16, l17, 1, 512*11008/(43*64*128*4)]\n",
    "    ## 实际情况: 插入l17后变为 [43, 64, 128, 4, 1, 4]\n",
    "    l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)\n",
    "    # print(sch.get(l28))#, sch.get(l25),sch.get(l26), sch.get(l27), sch.get(l28),sch.get(l29))\n",
    "    # vectorize 4\n",
    "    sch.vectorize(loop=l29)\n",
    "    # b21: lv1612[128,11008]\n",
    "    # b21 插入后: [l10, l11, l16, 1, 128*11008/(l10*l11*l16)] => [43, 64, 128, 1, 4 ]\n",
    "    l30, l31, l32, l33, l34 = sch.get_loops(block=b21)\n",
    "    sch.vectorize(loop=l34)\n",
    "    # b19: [1, 1, 11008]\n",
    "    # b19插入后: [l10, l11, 1, 1, 11008/(l10*l11)] -> [43, 64, 1, 1, 4]\n",
    "    l35, l36, l37, l38, l39 = sch.get_loops(block=b19)\n",
    "    # l39 = 4\n",
    "    sch.vectorize(loop=l39)\n",
    "    # l12 = 4\n",
    "    sch.vectorize(loop=l12)\n",
    "    # 包含reduction计算的即为reduction block\n",
    "    # 将reduction block中的 Init block和update block拆分， 即将matmul中var_matmul_intermediate = float16(0)的操作拆分出来\n",
    "    # 注意现在b1通过之前的compute_at包含了很多初始化的计算(@TODO 实际看只有b22的赋初始值的过程)\n",
    "    # b40是b22的赋初始值过程 @TODO 没找到b20 b21 b22的内存转移操作在哪儿\n",
    "    b40 = sch.decompose_reduction(block=b1, loop=l16)\n",
    "    b41 = sch.get_block(name=\"T_update\", func_name=\"main\")\n",
    "    sch.reverse_compute_inline(block=b41)\n",
    "    # 标注scheudle的后处理开始，@TODO 不知道作用 \n",
    "    sch.enter_postproc()\n",
    "    # 去掉 annotate, 从这里可以猜测`enter_postproc`应该对annotate信息做了处理\n",
    "    sch.unannotate(block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\")\n",
    "    # 此时b22只包含 global memory -> shared memory的过程\n",
    "    # 察到shape: [[l10, l11], [1,1,4096]] 即b22的初始化还在[l10,l11]的loop内\n",
    "    l43, l44, l45, l46, l47 = sch.get_loops(block=b22)\n",
    "    # preserve_unit_iters作用不大，仅是否考虑计算 value为1 的loop\n",
    "    # 用于vectorize构建shared_memory\n",
    "    # l48, l49, l50 = sch.split(loop=l47, factors=[None, 64, 8], preserve_unit_iters=True)\n",
    "    l48, l49, l50 = sch.split(loop=l47, factors=[None, threadidxX, 8], preserve_unit_iters=True)\n",
    "    # l49 = 64, l50 = 8\n",
    "    sch.vectorize(loop=l50)\n",
    "    sch.bind(loop=l49, thread_axis=\"threadIdx.x\")\n",
    "    return sch.mod[\"main\"].with_attr(\"tir.is_scheduled\", 1)\n",
    "\n",
    "\n",
    "sch_manual = tvm.tir.Schedule(ModuleToManual)\n",
    "# sch_fused_decode5_fused_matmul6_silu1(sch_manual.mod[func_name])\n",
    "sch_manual.mod['main'] = sch_fused_decode5_fused_matmul6_silu1(sch_manual.mod[func_name])\n",
    "# print(sch_manual.mod.script())\n",
    "print(\"================================================\")\n",
    "rt_mod = tvm.build(sch_manual.mod, target=\"opencl\")\n",
    "print(rt_mod.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda -keys=cuda,gpu -arch=sm_61 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n",
      "[[[-6644. -7192. -6284. -6688. -6772. -6120. -6876. -6168. -6472. -6424.]]]\n",
      "[[[-6644. -7192. -6284. -6688. -6772. -6120. -6876. -6168. -6472. -6424.]]]\n"
     ]
    }
   ],
   "source": [
    "# run and compare with cuda\n",
    "import numpy as np\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_w_np = np.random.uniform(size=(w_w_x, w_y)).astype(\"uint32\")\n",
    "W_s_np = np.random.uniform(size=(w_s_x, w_y)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, x_shape)).astype(\"float16\")\n",
    "# W_w_np = np.ones((w_w_x, w_y), np.uint32) * 1#.astype(\"uint32\")\n",
    "# W_s_np = np.ones((w_s_x, w_y), np.float16) * 1#.astype(\"float16\") * 2\n",
    "# Input_np = np.ones((1, 1, x_shape), np.float16)#.astype(\"float16\")\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "def numpy_caculate():\n",
    "    test_cols = 10\n",
    "    output = np.zeros((1, 1, test_cols), dtype = np.float16)\n",
    "    W_w_inv_np = np.transpose(W_w_np)\n",
    "    W_s_inv_np = np.transpose(W_s_np)\n",
    "    for i in range(test_cols):\n",
    "        for r in range(x_shape):\n",
    "            temp = Input_np[0][0][r] * np.float16((W_w_inv_np[i][r // 8] >> ((r % 8) * 4) & (15)) - np.float16(7.0)) * W_s_inv_np[i][r // 32]\n",
    "            output[0][0][i] = output[0][0][i] + temp\n",
    "    print(output)\n",
    "    output = np.zeros((1, 1, test_cols), dtype = np.float16)\n",
    "    for i in range(test_cols):\n",
    "        for r in range(x_shape):\n",
    "            temp = Input_np[0][0][r] * np.float16((W_w_np[r // 8][i] >> ((r % 8) * 4) & (15)) - np.float16(7.0)) * W_s_np[r // 32][i]\n",
    "            temp_output = output[0][0][i]\n",
    "            output[0][0][i] = temp_output + temp\n",
    "            # print(f\"{temp_output} + {temp} = {output[0][0][i]}\")\n",
    "    print(output)\n",
    "numpy_caculate()\n",
    "def print_npdata(np_data: np.ndarray) :\n",
    "    d = np_data.flatten()\n",
    "    p_size = 10 if d.size > 10 else d.size\n",
    "    print(d[:p_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_evaluator GEMV-Blocking: 65.352494 GFLOPS\n",
      "[-5976. -6344. -7012. -5560. -6352. -5812. -7388. -6224. -6852. -6048.]\n"
     ]
    }
   ],
   "source": [
    "# cuda未优化版本测试\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "rt_mod = tvm.build(src_gpu_mod, target=\"cuda\")\n",
    "W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "print(\"manual_evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / evaluator(W_w_nd, W_s_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "# print(Output_nd.numpy())\n",
    "print_npdata(Output_nd.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TVM_NDK_CC\"]=\"/home/sensetime/Android/Sdk/ndk/25.2.9519653/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android33-clang++\"\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.4.236.32\"\n",
    "rpc_port = 9190\n",
    "comp_target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-android\")  # TODO: Only support arm64 for now\n",
    "\n",
    "def test_opencl(mod: tvm.IRModule, name_hint: str):\n",
    "    # mod = tvm.lower(sch_manual.mod)\n",
    "    print(\"Build ...\")\n",
    "    android_rt_mod = tvm.build(mod, target=\"opencl\", target_host=\"llvm -mtriple=aarch64-linux-android\")\n",
    "    # print(android_rt_mod.imported_modules[0].get_source())\n",
    "    temp = utils.tempdir()\n",
    "    path_dso_cl = temp.relpath(\"dev_lib_cl.so\")\n",
    "    android_rt_mod.export_library(path_dso_cl, ndk.create_shared)\n",
    "\n",
    "    print(\"Run GPU(OpenCL Flavor) test ...\")\n",
    "    # Establish remote connection with target hardware\n",
    "\n",
    "    tracker = rpc.connect_tracker(rpc_host, rpc_port)\n",
    "    remote = tracker.request(device_key, priority=0, session_timeout=60)\n",
    "    print(\"Connect to device done.\")\n",
    "    dev = remote.cl(0)\n",
    "    remote.upload(path_dso_cl)\n",
    "    f1 = remote.load_module(\"dev_lib_cl.so\")\n",
    "\n",
    "    W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "    W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "    Input_nd = tvm.nd.array(Input_np, dev)\n",
    "    Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "    test_number=32\n",
    "    time_f = f1.time_evaluator(f1.entry_name, dev, number=test_number)\n",
    "    cost = time_f(W_w_nd, W_s_nd, Input_nd, Output_nd).mean\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %fms with loop %d\" % (name_hint, cost * 1000, test_number))\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %fGFLOPS\" % (name_hint, num_flop / cost / 1e9))\n",
    "\n",
    "    return Output_nd.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\"\"\"A rule for GEMV and DecodeGEMV.\"\"\"\n",
    "import re\n",
    "from functools import reduce\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "from tvm import DataType, arith, ir, tir\n",
    "from tvm.target import Target\n",
    "\n",
    "from tvm.dlight.base import (\n",
    "    BlockInfo,\n",
    "    ScheduleRule,\n",
    "    collect_vars_used_in_access_region,\n",
    "    detect_dominant_read,\n",
    "    is_broadcast_epilogue,\n",
    "    normalize_prim_func,\n",
    "    try_inline_contiguous_spatial,\n",
    ")\n",
    "\n",
    "\n",
    "def _get_reduction_expr(block: tir.Block) -> Optional[tir.PrimExpr]:\n",
    "    # Detect and return `Y` in `X[...] = X[...] + Y`\n",
    "    buffer_store = block.body\n",
    "    if not isinstance(buffer_store, tir.BufferStore):\n",
    "        return None\n",
    "    if not isinstance(buffer_store.value, tir.Add):\n",
    "        return None\n",
    "    if not ir.structural_equal(\n",
    "        buffer_store.value.a,\n",
    "        tir.BufferLoad(buffer_store.buffer, block.body.indices),\n",
    "        map_free_vars=True,\n",
    "    ):\n",
    "        return None\n",
    "    return buffer_store.value.b\n",
    "\n",
    "\n",
    "def get_bytes(dtype: Union[DataType, str]) -> int:\n",
    "    num = re.findall(r\"\\d+\", dtype)\n",
    "    if len(num) != 1:\n",
    "        raise ValueError(f\"Cannot get bytes from {dtype}\")\n",
    "    return int(num[0]) // 8\n",
    "\n",
    "\n",
    "def is_gemv(sch: tir.Schedule, block_info: BlockInfo) -> Optional[List[tir.Buffer]]:\n",
    "    \"\"\"Check if the block is a GEMV.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    sch : tir.Schedule\n",
    "        The schedule\n",
    "\n",
    "    block_info : BlockInfo\n",
    "        The block info to be checked\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ret : Optional[List[tir.Buffer]]\n",
    "        The vector buffers used in the GEMV if it is a GEMV, otherwise None.\n",
    "    \"\"\"\n",
    "    block = block_info.block_rv\n",
    "    block_stmt = sch.get(block)\n",
    "    conditions = []\n",
    "    conditions.append(block_info.is_reduction())\n",
    "    conditions.append(len(block_stmt.reads) >= 2)\n",
    "    conditions.append(len(block_stmt.writes) == 1)\n",
    "    conditions.append(_get_reduction_expr(block_stmt) is not None)\n",
    "    conditions.append(len(collect_vars_used_in_access_region(block_stmt.writes[0].region)) > 0)\n",
    "    if not all(conditions):\n",
    "        return None\n",
    "\n",
    "    iter_num = len(block_stmt.iter_vars)\n",
    "    ret = [\n",
    "        read.buffer\n",
    "        for read in block_stmt.reads\n",
    "        if len(collect_vars_used_in_access_region(read.region)) < iter_num\n",
    "    ]\n",
    "    return ret if 0 < len(ret) < len(block_stmt.reads) else None\n",
    "\n",
    "\n",
    "def normalize(\n",
    "    sch: tir.Schedule,\n",
    "    block_info: BlockInfo,\n",
    ") -> Optional[bool]:\n",
    "    \"\"\"Normalize the main block.\"\"\"\n",
    "    block_stmt: tir.Block = sch.get(block_info.block_rv)\n",
    "    access = arith.normalize_to_iter_sum(\n",
    "        detect_dominant_read(block_stmt),\n",
    "        input_iters={i.var: i.dom for i in block_stmt.iter_vars},\n",
    "    )\n",
    "\n",
    "    buffers_use_vars = [collect_vars_used_in_access_region(buf.region) for buf in block_stmt.writes]\n",
    "    buffers_use_vars.extend(\n",
    "        [collect_vars_used_in_access_region(buf.region) for buf in block_stmt.reads]\n",
    "    )\n",
    "    if access.base != 0:\n",
    "        print(\"access.base == 1\")\n",
    "        return None\n",
    "    iter_to_info = {i.var: i for i in block_info.iters}\n",
    "    batch_loops, s_loops, r_loops, c_loops = [], [], [], []\n",
    "    inner_axis = access.args[-1].source.source\n",
    "    print(f\"inner_axis = {inner_axis}\")\n",
    "    is_inner_reduction = iter_to_info[inner_axis].kind == \"R\"\n",
    "\n",
    "    for split_expr in access.args:\n",
    "        var = split_expr.source.source\n",
    "        info = iter_to_info.get(var)\n",
    "        loop = info.loop_rv\n",
    "        is_reduction = info.kind == \"R\"\n",
    "        if split_expr.lower_factor > 1:\n",
    "            if c_loops:\n",
    "                print(f\"c_loops = {c_loops}\")\n",
    "                return None\n",
    "            loop, c_loop = sch.split(loop, factors=[None, split_expr.lower_factor])\n",
    "            # we only support the inner most dim being grouped atm\n",
    "            if is_reduction ^ is_inner_reduction:\n",
    "                print(f\"is_reduction = {is_reduction}, is_inner_reduction = {is_inner_reduction}\")\n",
    "                return None\n",
    "            c_loops.append(c_loop)\n",
    "        if is_reduction:\n",
    "            r_loops.append(loop)\n",
    "        elif all([var in buf_vars for buf_vars in buffers_use_vars]):\n",
    "            batch_loops.append(loop)\n",
    "        else:\n",
    "            s_loops.append(loop)\n",
    "\n",
    "    assert s_loops\n",
    "    assert r_loops\n",
    "    if not c_loops:\n",
    "        c_loops = [sch.add_unit_loop(block_info.block_rv)]\n",
    "    if not batch_loops:\n",
    "        batch_loops = [sch.add_unit_loop(block_info.block_rv)]\n",
    "    sch.reorder(*batch_loops, *s_loops, *r_loops, *c_loops)\n",
    "    sch.fuse(*batch_loops)\n",
    "    sch.fuse(*s_loops)\n",
    "    sch.fuse(*r_loops)\n",
    "    return is_inner_reduction\n",
    "\n",
    "\n",
    "class GEMV(ScheduleRule):\n",
    "    \"\"\"A rule for GEMV and DecodeGEMV.\"\"\"\n",
    "\n",
    "    def apply(  # pylint: disable=too-many-locals,too-many-branches,too-many-return-statements\n",
    "        self,\n",
    "        func: tir.PrimFunc,\n",
    "        target: Target,\n",
    "        _: bool,\n",
    "    ) -> Union[None, tir.Schedule, List[tir.Schedule]]:\n",
    "        print(\"GEMV in!\")\n",
    "        if not isinstance(func, tir.PrimFunc):\n",
    "            return None\n",
    "        sch = tir.Schedule(func)\n",
    "        block_infos = normalize_prim_func(sch)\n",
    "        block_infos = try_inline_contiguous_spatial(sch, block_infos)\n",
    "        if len(block_infos) == 1:\n",
    "            epilogue = None\n",
    "        elif len(block_infos) == 2:\n",
    "            epilogue = block_infos[1]\n",
    "            if not epilogue.is_injective():\n",
    "                return None\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        block_info = block_infos[0]\n",
    "        if len(block_info.iters) not in [2, 3]:\n",
    "            # either [B, S, R] = [B, S, R] * [B, R]\n",
    "            # or [S, R] = [S, R] * [R]\n",
    "            return None\n",
    "        block = block_info.block_rv\n",
    "        vector_input_buffers = is_gemv(sch, block_info)\n",
    "        if vector_input_buffers is None:\n",
    "            return None\n",
    "        print(f\"block_infos len: {len(block_infos)}\")\n",
    "        for b in block_infos:\n",
    "            print(f\"\\t{b}\")\n",
    "        print(\"================================================\")\n",
    "        # Step 1. Normalize the block, merge spatial and reduction iters\n",
    "        is_inner_reduction = normalize(sch, block_info)\n",
    "        print(is_inner_reduction)\n",
    "\n",
    "        # Step 2. Do the scheduling\n",
    "        if is_inner_reduction:\n",
    "            self.sch_inner_reduction(sch, target, block, vector_input_buffers, epilogue)\n",
    "            return sch\n",
    "        else:\n",
    "            # TODO: Need to handle GEMV with KN layout\n",
    "            return None\n",
    "\n",
    "    def sch_inner_reduction(  # pylint: disable=too-many-arguments, invalid-name, unused-argument\n",
    "        self,\n",
    "        sch: tir.Schedule,\n",
    "        target: Target,\n",
    "        block: tir.schedule.BlockRV,\n",
    "        vector_input_buffers: List[tir.Buffer],\n",
    "        epilogue_info: Optional[BlockInfo],\n",
    "    ):\n",
    "        \"\"\"Schedule the inner reduction block.\"\"\"\n",
    "\n",
    "        def get_max_factor(n, factors):\n",
    "            factors = sorted(factors, reverse=True)\n",
    "            for factor in factors:\n",
    "                if n % factor == 0:\n",
    "                    return factor\n",
    "            return 1\n",
    "\n",
    "        def apply(\n",
    "            sch: tir.Schedule,\n",
    "            gemv,\n",
    "            TAG_S,\n",
    "            TAG_R,\n",
    "            TS,\n",
    "            TR,\n",
    "            TILE_S,\n",
    "            TILE_R,\n",
    "            VEC_LOAD,\n",
    "            VEC_C,\n",
    "            LOAD_V_SHARED,\n",
    "            LOAD_V_VEC,\n",
    "            UNROLL,\n",
    "        ):\n",
    "            # rfactor: reduce to tx * vec_c\n",
    "            _, s, r, c = sch.get_loops(block=gemv)\n",
    "            s = sch.fuse(_, s)\n",
    "            r = sch.fuse(r, c)\n",
    "            bx, ts, tile_s = sch.split(s, factors=[None, TS, TILE_S], preserve_unit_iters=True)\n",
    "            r, tr, tile_r_vec_n, vec_c = sch.split(\n",
    "                r, factors=[None, TR, TILE_R // VEC_C, VEC_C], preserve_unit_iters=True\n",
    "            )\n",
    "            sch.reorder(r, tile_r_vec_n, tr, vec_c)\n",
    "            tr_vec_c = sch.fuse(tr, vec_c)\n",
    "            rf = sch.rfactor(tr_vec_c, 0)\n",
    "\n",
    "            # rfactor: reduce to tx\n",
    "            bx, ts, tile_s, tr_vec_c = sch.get_loops(block=gemv)\n",
    "            tr, vec_c = sch.split(tr_vec_c, factors=[TR, None], preserve_unit_iters=True)\n",
    "            rf2 = sch.rfactor(tr, 0)\n",
    "\n",
    "            # bind, vectorize compute\n",
    "            bx, ts, tile_s, r, tile_r_vec_n, tr_vec_c = sch.get_loops(block=rf)\n",
    "            tr, vec_c = sch.split(tr_vec_c, factors=[TR, None], preserve_unit_iters=True)\n",
    "            sch.reorder(bx, ts, tr, r, tile_s, tile_r_vec_n, vec_c)\n",
    "            sch.bind(bx, \"blockIdx.x\")\n",
    "            sch.bind(ts, TAG_S)\n",
    "            sch.bind(tr, TAG_R)\n",
    "            sch.vectorize(vec_c)\n",
    "\n",
    "            shared_mem_usage = 0\n",
    "            for buf in vector_input_buffers:\n",
    "                buf_size = reduce(\n",
    "                    lambda x, y: x * y, buf.shape, tir.IntImm(buf.shape[0].dtype, 1)\n",
    "                ) * get_bytes(buf.dtype)\n",
    "                shared_mem_usage += buf_size\n",
    "            LOAD_V_SHARED = (\n",
    "                LOAD_V_SHARED\n",
    "                and isinstance(shared_mem_usage, tir.IntImm)\n",
    "                and shared_mem_usage.value <= target.max_shared_memory_per_block\n",
    "            )\n",
    "\n",
    "            # vectorize load A\n",
    "            # (TODO) this is now actually problematic since the number of loops is dependent on the\n",
    "            # number of dimensions of A_q\n",
    "            Aq_local = sch.cache_read(rf, read_buffer_index=1, storage_scope=\"local\")\n",
    "            sch.compute_at(Aq_local, r, preserve_unit_loops=True)\n",
    "            s_local, r_local = sch.get_loops(block=Aq_local)[-2:]\n",
    "            s_local, vec_load = sch.split(\n",
    "                s_local, factors=[None, VEC_LOAD], preserve_unit_iters=True\n",
    "            )\n",
    "            sch.reorder(s_local, r_local, vec_load)  # either s_local or r_local should be 1\n",
    "            sch.vectorize(vec_load)\n",
    "\n",
    "            # load vector into shared memory, shape should be the whole vector\n",
    "            if LOAD_V_SHARED:\n",
    "                assert len(vector_input_buffers) == 1\n",
    "                V_shared = sch.cache_read(rf, read_buffer_index=0, storage_scope=\"shared\")\n",
    "                sch.compute_at(V_shared, tr, preserve_unit_loops=True)\n",
    "                l = sch.get_loops(block=V_shared)[-1]\n",
    "                loop: tir.For = sch.get(l)\n",
    "                if isinstance(loop.extent, tir.IntImm):\n",
    "                    # avoid introducing predicates when vector length is too large\n",
    "                    vec_length = max(\n",
    "                        min(\n",
    "                            get_max_factor(\n",
    "                                (int)(loop.extent),\n",
    "                                [TS * TR * 1, TS * TR * 2, TS * TR * 4, TS * TR * 8],\n",
    "                            )\n",
    "                            // TS\n",
    "                            // TR,\n",
    "                            LOAD_V_VEC,\n",
    "                        ),\n",
    "                        1,\n",
    "                    )\n",
    "                else:\n",
    "                    vec_length = LOAD_V_VEC\n",
    "                if TAG_R == \"threadIdx.x\":\n",
    "                    _, ty, tx, vec = sch.split(\n",
    "                        l, factors=[None, TS, TR, vec_length], preserve_unit_iters=True\n",
    "                    )\n",
    "                else:\n",
    "                    _, ty, tx, vec = sch.split(\n",
    "                        l, factors=[None, TR, TS, vec_length], preserve_unit_iters=True\n",
    "                    )\n",
    "                sch.bind(ty, \"threadIdx.y\")\n",
    "                sch.bind(tx, \"threadIdx.x\")\n",
    "                sch.vectorize(vec)\n",
    "\n",
    "            # reduce tile_s * tr * vec to tile_s * tr\n",
    "            sch.reverse_compute_at(rf2, loop=bx, preserve_unit_loops=True)\n",
    "            tr, vec_c, *ts_tile_s = sch.get_loops(block=rf2)[1:]\n",
    "            ts_tile_s = sch.fuse(*ts_tile_s)\n",
    "            ts, tile_s = sch.split(ts_tile_s, factors=[TS, None], preserve_unit_iters=True)\n",
    "            tile_s, vec_s = sch.split(\n",
    "                tile_s,\n",
    "                factors=[None, get_max_factor(TILE_S, [1, 2, 4, 8])],\n",
    "                preserve_unit_iters=True,\n",
    "            )\n",
    "            sch.reorder(ts, tr, tile_s, vec_s, vec_c)\n",
    "            sch.bind(ts, TAG_S)\n",
    "            sch.bind(tr, TAG_R)\n",
    "            sch.vectorize(vec_s)\n",
    "\n",
    "            # reduce tile_s * tr to tile_s\n",
    "            sch.reverse_compute_at(gemv, loop=bx, preserve_unit_loops=True)\n",
    "            tr, *ts_tile_s = sch.get_loops(block=gemv)[1:]\n",
    "            ts_tile_s = sch.fuse(*ts_tile_s)\n",
    "            ts, tile_s = sch.split(ts_tile_s, factors=[TS, None], preserve_unit_iters=True)\n",
    "            sch.reorder(tile_s, ts, tr)\n",
    "            sch.bind(ts, TAG_S)\n",
    "            sch.bind(tr, TAG_R)\n",
    "\n",
    "            sch.decompose_reduction(rf, loop=sch.get_loops(block=rf)[3])\n",
    "            sch.decompose_reduction(rf2, loop=sch.get_loops(block=rf2)[-1])\n",
    "\n",
    "            sch.set_scope(rf, buffer_index=0, storage_scope=\"local\")\n",
    "            sch.set_scope(rf2, buffer_index=0, storage_scope=\"local\")\n",
    "\n",
    "            unroll_factor = UNROLL\n",
    "\n",
    "            sch.annotate(\n",
    "                block_or_loop=sch.get_loops(rf)[3],\n",
    "                ann_key=\"pragma_auto_unroll_max_step\",\n",
    "                ann_val=unroll_factor,\n",
    "            )\n",
    "            sch.annotate(\n",
    "                block_or_loop=sch.get_loops(rf)[3], ann_key=\"pragma_unroll_explicit\", ann_val=1\n",
    "            )\n",
    "\n",
    "            sch.annotate(\n",
    "                block_or_loop=sch.get_loops(rf2)[3],\n",
    "                ann_key=\"pragma_auto_unroll_max_step\",\n",
    "                ann_val=unroll_factor,\n",
    "            )\n",
    "            sch.annotate(\n",
    "                block_or_loop=sch.get_loops(rf2)[3], ann_key=\"pragma_unroll_explicit\", ann_val=1\n",
    "            )\n",
    "\n",
    "            if LOAD_V_SHARED:\n",
    "                sch.annotate(\n",
    "                    block_or_loop=sch.get_loops(V_shared)[-4],\n",
    "                    ann_key=\"pragma_unroll_explicit\",\n",
    "                    ann_val=unroll_factor,\n",
    "                )\n",
    "                sch.annotate(\n",
    "                    block_or_loop=sch.get_loops(V_shared)[-4], ann_key=\"pragma_vectorize\", ann_val=1\n",
    "                )\n",
    "\n",
    "            # Schedule epilogue\n",
    "            if epilogue_info is not None:\n",
    "                epilogue = epilogue_info.block_rv\n",
    "                if is_broadcast_epilogue(sch, block, epilogue):\n",
    "                    sch.reverse_compute_at(epilogue, bx)\n",
    "                    sch.set_scope(block, 0, \"shared\")\n",
    "                    _, _, *s = sch.get_loops(epilogue)  # pylint: disable=invalid-name\n",
    "                    _, tx = sch.split(sch.fuse(*s), factors=[None, TX])\n",
    "                    sch.bind(tx, \"threadIdx.x\")\n",
    "                else:\n",
    "                    sch.reverse_compute_at(epilogue, bx, preserve_unit_loops=True)\n",
    "                    ts_tile_s = sch.fuse(*sch.get_loops(epilogue)[1:])\n",
    "                    ts_tile_s = sch.get_loops(epilogue)[-1]\n",
    "                    ts, tile_s = sch.split(ts_tile_s, factors=[TS, None], preserve_unit_iters=True)\n",
    "                    sch.bind(ts, TAG_S)\n",
    "                    sch.set_scope(block, 0, \"local\")\n",
    "            # pylint: enable=invalid-name\n",
    "            return sch\n",
    "\n",
    "        def get_extent(loop_rv: tir.schedule.LoopRV):\n",
    "            loop: tir.For = sch.get(loop_rv)\n",
    "            return loop.extent.value if isinstance(loop.extent, tir.IntImm) else loop.extent\n",
    "\n",
    "        # Specify the `len_tx` and `len_ty` according to the loop extent\n",
    "        batch, s, r, c = sch.get_loops(block=block)\n",
    "        len_batch, len_s, len_r, len_c = (\n",
    "            get_extent(batch),\n",
    "            get_extent(s),\n",
    "            get_extent(r),\n",
    "            get_extent(c),\n",
    "        )\n",
    "        len_S = len_batch * len_s\n",
    "        len_R = len_r * len_c\n",
    "\n",
    "        TAG_S, TAG_R = \"threadIdx.y\", \"threadIdx.x\"\n",
    "        print(f\"kind.name = {target.kind.name}, host = {target.host}\")\n",
    "        if target.kind.name == \"cuda\":\n",
    "            VEC_C = 4\n",
    "            LOAD_V_SHARED = True\n",
    "            LOAD_V_VEC = 8\n",
    "            UNROLL = 256\n",
    "            if isinstance(len_S, int):\n",
    "                if len_S > len_R:\n",
    "                    TS, TR = 4, 64\n",
    "                else:\n",
    "                    TS, TR = 16, 32\n",
    "        elif target.kind.name == \"metal\":\n",
    "            VEC_C = 2\n",
    "            LOAD_V_SHARED = True\n",
    "            LOAD_V_VEC = 4\n",
    "            UNROLL = 256\n",
    "            TS, TR = 64, 8\n",
    "        elif target.kind.name == \"rocm\":\n",
    "            VEC_C = 4\n",
    "            LOAD_V_SHARED = True\n",
    "            LOAD_V_VEC = 8\n",
    "            UNROLL = 256\n",
    "            if isinstance(len_S, int):\n",
    "                if len_S > len_R:\n",
    "                    TS, TR = 1, 128\n",
    "                else:\n",
    "                    TS, TR = 8, 64\n",
    "        elif target.kind.name == \"opencl\" and \"android\" in str(target.host):\n",
    "            print(\"dlight GEMV: opencl\")\n",
    "            TAG_S, TAG_R = \"threadIdx.x\", \"threadIdx.y\"\n",
    "            VEC_C = 8\n",
    "            LOAD_V_SHARED = True\n",
    "            LOAD_V_VEC = 4\n",
    "            UNROLL = 8\n",
    "            TS, TR = 4, 32\n",
    "        elif target.kind.name == \"vulkan\":\n",
    "            VEC_C = 4\n",
    "            LOAD_V_SHARED = True\n",
    "            LOAD_V_VEC = 4\n",
    "            UNROLL = 256\n",
    "            if isinstance(len_S, int):\n",
    "                if len_S > len_R:\n",
    "                    TS, TR = 4, 32\n",
    "                else:\n",
    "                    TS, TR = 16, 32\n",
    "        else:\n",
    "            print(\"dlight GEMV: default\")\n",
    "            VEC_C = 1\n",
    "            LOAD_V_SHARED = False\n",
    "            LOAD_V_VEC = -1\n",
    "            UNROLL = 64\n",
    "            TS, TR = 1, 64\n",
    "\n",
    "        if not isinstance(len_S, int):\n",
    "            TS, TR = 1, 64\n",
    "        TILE_S, TILE_R = (\n",
    "            1,\n",
    "            len_c\n",
    "            if len_c > 1\n",
    "            else max(get_max_factor(len_r, [TR * 1, TR * 2, TR * 4, TR * 8]) // TR, 1),\n",
    "        )\n",
    "        VEC_C = min(get_max_factor(TILE_R, [1, 2, 4, 8]), VEC_C)\n",
    "        VEC_LOAD = 1\n",
    "\n",
    "        return apply(\n",
    "            sch,\n",
    "            gemv=block,\n",
    "            TAG_S=TAG_S,\n",
    "            TAG_R=TAG_R,\n",
    "            TS=TS,\n",
    "            TR=TR,\n",
    "            TILE_S=TILE_S,\n",
    "            TILE_R=TILE_R,\n",
    "            VEC_LOAD=VEC_LOAD,\n",
    "            VEC_C=VEC_C,\n",
    "            LOAD_V_SHARED=LOAD_V_SHARED,\n",
    "            LOAD_V_VEC=LOAD_V_VEC,\n",
    "            UNROLL=UNROLL,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/driver/build_module.py:264: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to device done.\n",
      "evaluator[source] GEMV-Blocking: 4.269120 ms with loop 32\n",
      "evaluator[source] GEMV-Blocking: 287.742388 GFLOPS\n",
      "[-6600. -6816. -7668. -6056. -6908. -6464. -7976. -6960. -7600. -6672.]\n"
     ]
    }
   ],
   "source": [
    "# 未优化版本opencl测试\n",
    "from tvm import dlight as dl\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    # src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "    mod_deploy = dl.ApplyDefaultSchedule(  # pylint: disable=not-callable\n",
    "        dl.gpu.Matmul(),\n",
    "        dl.gpu.GEMV(),\n",
    "        dl.gpu.Reduction(),\n",
    "        dl.gpu.GeneralReduction(),\n",
    "        dl.gpu.Fallback(),\n",
    "    )(sch.mod)\n",
    "src_output = test_opencl(mod_deploy, \"source\")\n",
    "print_npdata(src_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[opted] GEMV-Blocking: 1.295184ms with loop 32\n",
      "evaluator[opted] GEMV-Blocking: 948.441908GFLOPS\n",
      "[-6644. -7192. -6284. -6688. -6772. -6116. -6876. -6164. -6472. -6416.]\n"
     ]
    }
   ],
   "source": [
    "#优化版本opencl测试\n",
    "# print(sch_manual.mod)\n",
    "opt_output = test_opencl(sch_manual.mod, \"opted\")\n",
    "print_npdata(opt_output)\n",
    "# np.testing.assert_equal(opt_output, src_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.158.176.30\"\n",
    "rpc_port = 5001\n",
    "# remote = autotvm.measure.request_remote(device_key, \"10.158.176.30\", 5001, timeout=10000)\n",
    "# dev = remote.device(str(target), 0)\n",
    "\n",
    "# num_flop = 1228406784\n",
    "# W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "# S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "# Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# # Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# W_nd = tvm.nd.array(W_np, dev)\n",
    "# S_nd = tvm.nd.array(S_np, dev)\n",
    "# Input_nd = tvm.nd.array(Input_np, dev)\n",
    "# Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc_config = ms.runner.RPCConfig(tracker_host=rpc_host, tracker_port=rpc_port, tracker_key = device_key)\n",
    "runner= ms.runner.RPCRunner(rpc_config)\n",
    "# ms.builder.LocalBuilder()\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "database = ms.tune_tir(\n",
    "    mod=ModuleSrc,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_first\",\n",
    "    cost_model=\"xgb\",\n",
    "    runner = runner\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, sch.mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.script import relax as R\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def main(A: R.Tensor((3, 4), dtype=\"float32\"), B: R.Tensor((4, 5), dtype=\"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((3, 5), dtype=\"float32\") = R.matmul(A, B)\n",
    "            gv: R.Tensor((3, 5), dtype=\"float32\") = lv\n",
    "            R.output(gv)\n",
    "        return gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto_scheduler test\n",
    "from tvm import auto_scheduler\n",
    "import numpy as np\n",
    "a_np = np.random.rand(3, 4).astype(\"float32\")\n",
    "b_np = np.random.rand(4, 5).astype(\"float32\")\n",
    "a_nd = tvm.runtime.NDArray(a_np)\n",
    "b_nd = tvm.runtime.NDArray(b_np)\n",
    "sch = tvm.tir.Schedule(Module)\n",
    "\n",
    "params = {\"A\": a_np, \"B\": b_np}\n",
    "## 报错，这里只支持relay\n",
    "# tasks = auto_scheduler.extract_tasks(sch.mod, params, target=target)\n",
    "tasks = ms.relax_integration.extract_tasks(sch.mod, target=target, params=params)\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mod_deploy import Module as ModuleAll\n",
    "params_all = {}\n",
    "tasks_all = auto_scheduler.extract_tasks(ModuleAll, params_all, target=target)\n",
    "print(len(tasks_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_file = \"tune.json\"\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "W_nd = tvm.nd.array(W_np, dev)\n",
    "S_nd = tvm.nd.array(S_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "new_mod = sch.mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = auto_scheduler.SearchTask(func=sch.mod['fused_fused_decode11_fused_matmul5_cast2'], args=sch.mod['fused_fused_decode11_fused_matmul5_cast2'].params, target=target)\n",
    "\n",
    "# tune_option = auto_scheduler.TuningOptions(\n",
    "#     num_measure_trials=10,\n",
    "#     measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "\n",
    "database = ms.tune_tir(\n",
    "    mod=new_mod,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_45593_1\",\n",
    "    cost_model=\"xgb\"\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, new_mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sch1.trace)\n",
    "# print(sch1.mod.script())\n",
    "rt_mod = tvm.build(sch1.mod, target=\"cuda\")\n",
    "\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (1228406784 / evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_database = ms.Database.create(kind='json', work_dir='./tune_45593_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sch = ms.tir_integration.compile_tir(record_database, new_mod, target)\n",
    "\n",
    "record_rt_mod = tvm.build(record_sch.mod, target=\"cuda\")\n",
    "\n",
    "record_evaluator = record_rt_mod.time_evaluator(\"main\", dev, number=20)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / record_evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(record_sch.trace)\n",
    "print(record_sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Dict, List, Optional, Union, Callable\n",
    "from tvm import runtime\n",
    "if TYPE_CHECKING:\n",
    "    import numpy as np  # type: ignore\n",
    "    from tvm.ir import IRModule\n",
    "    from tvm.meta_schedule.runner import EvaluatorConfig, RPCConfig\n",
    "    from tvm.runtime import Device, Module, NDArray\n",
    "    from tvm.target import Target\n",
    "    from tvm.runtime.vm import Executable\n",
    "\n",
    "\n",
    "def f_measurement(\n",
    "    rt_mod: runtime.Module, device: runtime.ndarray.Device, input_data: Dict[str, runtime.NDArray]\n",
    "):\n",
    "    vm = relax.VirtualMachine(rt_mod, device=device)\n",
    "    vm.save_function(\"main\", \"measure_func\", **input_data, include_return=False)\n",
    "    evaluator = vm.time_evaluator(\n",
    "        func_name=\"measure_func\",\n",
    "        dev=device,\n",
    "        repeat=100,\n",
    "        number=1,\n",
    "        min_repeat_ms=500,\n",
    "    )\n",
    "    return evaluator()\n",
    "\n",
    "def run_module_via_rpc(\n",
    "    rpc_config: \"RPCConfig\",\n",
    "    lib: Union[\"Module\", \"Executable\"],\n",
    "    dev_type: str,\n",
    "    args: Union[Dict[int, \"np.ndarray\"], Dict[str, \"np.ndarray\"]],\n",
    "    continuation: Callable,\n",
    "    backend: Optional[str] = \"graph\",\n",
    "):\n",
    "    \"\"\"Execute a tvm.runtime.Module on RPC remote\"\"\"\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    from tvm.contrib.tar import tar\n",
    "    from tvm.runtime import ndarray\n",
    "\n",
    "    # pylint: enable=import-outside-toplevel\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + tar.output_format)\n",
    "        filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + \"so\")\n",
    "        if backend == \"vm\":\n",
    "            code, lib = lib.save(filename, fmt=\"so\")\n",
    "        from tvm.contrib import ndk\n",
    "        lib.export_library(filename, ndk.create_shared)\n",
    "        session = rpc_config.connect_server()\n",
    "        print(type(session._sess))\n",
    "        session.upload(filename)\n",
    "        _, filename = os.path.split(filename)\n",
    "        rt_mod = session.load_module(filename)\n",
    "        \n",
    "        if backend == \"vm\":\n",
    "            rt_mod = session.get_function(\"runtime.Load_Executable\")(code, rt_mod)\n",
    "            # rt_mod = session.get_function(\"runtime.module.loadfile_relax.Executable\")(filename)\n",
    "        dev = session.device(dev_type=dev_type, dev_id=0)\n",
    "        # print(dev)\n",
    "        # create the remote runtime module\n",
    "        print(rt_mod)\n",
    "        print(rt_mod['main'])\n",
    "        from tvm.contrib import graph_executor as runtime\n",
    "        module = runtime.GraphModule(rt_mod[\"main\"](dev))\n",
    "        print(module)\n",
    "        for k, v in args.items():\n",
    "            module.set_input(k, tvm.nd.array(v))\n",
    "        return module.run()\n",
    "        # nd_args = {k: ndarray.array(v, dev) for k, v in args.items()}\n",
    "        nd_args = {k: ndarray.empty(v.shape, v.dtype, dev) for k, v in args.items()}\n",
    "        return continuation(rt_mod, dev, nd_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
