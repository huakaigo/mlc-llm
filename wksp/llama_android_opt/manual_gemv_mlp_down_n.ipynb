{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import ir as I\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@I.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def main(lv50: T.Buffer((T.int64(1376), T.int64(4096)), \"uint32\"), lv51: T.Buffer((T.int64(344), T.int64(4096)), \"float16\"), p_lv5: T.handle, p_lv3: T.handle, p_output0: T.handle):\n",
      "        T.func_attr({\"tir.is_scheduled\": 1, \"tir.noalias\": T.bool(True)})\n",
      "        n = T.int64()\n",
      "        lv5 = T.match_buffer(p_lv5, (T.int64(1), n, T.int64(11008)), \"float16\")\n",
      "        lv3 = T.match_buffer(p_lv3, (T.int64(1), n, T.int64(4096)), \"float16\")\n",
      "        p_output0_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(4096)), \"float16\")\n",
      "        # with T.block(\"root\"):\n",
      "        decode_local = T.alloc_buffer((T.int64(11008), T.int64(4096)), \"float16\", scope=\"local\")\n",
      "        lv50_local = T.alloc_buffer((T.int64(1376), T.int64(4096)), \"uint32\", scope=\"local\")\n",
      "        lv51_local = T.alloc_buffer((T.int64(344), T.int64(4096)), \"float16\", scope=\"local\")\n",
      "        lv5_pad_local = T.alloc_buffer((T.int64(1), (n + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(11008)), \"float16\", scope=\"local\")\n",
      "        var_NT_matmul_intermediate_pad_local = T.alloc_buffer((T.int64(1), (n + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(4096)), \"float16\", scope=\"local\")\n",
      "        BlockIdx_x: T.int32 = 32\n",
      "        ThreadIdx_x: T.int32 = 32\n",
      "        ThreadIdx_y: T.int32 = 16\n",
      "        vectorize_factor: T.int32 = 4\n",
      "        processed_rows_per_thread: T.int32 = 2\n",
      "        for i0_i1_fused_0_i0_i1_fused_1_0_fused in T.thread_binding((n + T.int64(31)) // T.int64(32), thread=\"blockIdx.y\"):\n",
      "            for i2_0 in T.thread_binding(T.Cast(\"int64\", BlockIdx_x), thread=\"blockIdx.x\"):\n",
      "                for i0_i1_fused_1_1 in T.thread_binding(T.Cast(\"int64\", ThreadIdx_y), thread=\"threadIdx.y\"):\n",
      "                    for i2_1 in T.thread_binding(T.Cast(\"int64\", ThreadIdx_x), thread=\"threadIdx.x\"):\n",
      "                        for i0_i1_fused_1_2_init in range(T.Cast(\"int64\", processed_rows_per_thread)):\n",
      "                            for i2_2_init in T.vectorized(T.Cast(\"int64\", vectorize_factor)):\n",
      "                                with T.block(\"NT_matmul_init\"):\n",
      "                                    v_i0 = T.axis.spatial(T.int64(1), T.int64(0))\n",
      "                                    v_i1 = T.axis.spatial((n + T.int64(31)) // T.int64(32) * T.int64(32), i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32) + i0_i1_fused_1_1 * T.Cast(\"int64\", processed_rows_per_thread) + i0_i1_fused_1_2_init)\n",
      "                                    v_i2 = T.axis.spatial(T.int64(4096), i2_0 * T.Cast(\"int64\", ThreadIdx_x * vectorize_factor) + i2_1 * T.Cast(\"int64\", vectorize_factor) + i2_2_init)\n",
      "                                    T.reads()\n",
      "                                    T.writes(var_NT_matmul_intermediate_pad_local[v_i0, v_i1, v_i2])\n",
      "                                    var_NT_matmul_intermediate_pad_local[v_i0, v_i1, v_i2] = T.float16(0)\n",
      "                        for k_0 in range(T.int64(344)):\n",
      "                            for ax0 in range(T.int64(1)):\n",
      "                                for ax1 in T.vectorized(T.Cast(\"int64\", vectorize_factor)):\n",
      "                                    with T.block(\"lv51_local\"):\n",
      "                                        v0 = T.axis.spatial(T.int64(344), k_0 + ax0)\n",
      "                                        v1 = T.axis.spatial(T.int64(4096), i2_0 * T.Cast(\"int64\", ThreadIdx_x * vectorize_factor) + i2_1 * T.Cast(\"int64\", vectorize_factor) + ax1)\n",
      "                                        T.reads(lv51[v0, v1])\n",
      "                                        T.writes(lv51_local[v0, v1])\n",
      "                                        lv51_local[v0, v1] = lv51[v0, v1]\n",
      "                            for k_1 in range(T.int64(4)):\n",
      "                                for ax0 in range(T.int64(1)):\n",
      "                                    for ax1 in T.vectorized(vectorize_factor):\n",
      "                                        with T.block(\"lv50_local\"):\n",
      "                                            v0 = T.axis.spatial(T.int64(1376), k_0 * T.int64(4) + k_1 + ax0)\n",
      "                                            v1 = T.axis.spatial(T.int64(4096), i2_0 * T.Cast(\"int64\", ThreadIdx_x * vectorize_factor) + i2_1 * T.Cast(\"int64\", vectorize_factor) + T.Cast(\"int64\", ax1))\n",
      "                                            T.reads(lv50[v0, v1])\n",
      "                                            T.writes(lv50_local[v0, v1])\n",
      "                                            lv50_local[v0, v1] = lv50[v0, v1]\n",
      "                                for k_2 in range(T.int64(8)):\n",
      "                                    for ax0 in range(T.int64(1)):\n",
      "                                        for ax1 in T.vectorized(vectorize_factor):\n",
      "                                            with T.block(\"decode\"):\n",
      "                                                v_i = T.axis.spatial(T.int64(11008), k_0 * T.int64(32) + k_1 * T.int64(8) + k_2 + ax0)\n",
      "                                                v_j = T.axis.spatial(T.int64(4096), i2_0 * T.Cast(\"int64\", ThreadIdx_x * vectorize_factor) + i2_1 * T.Cast(\"int64\", vectorize_factor) + T.Cast(\"int64\", ax1))\n",
      "                                                T.reads(lv50_local[v_i // T.int64(8), v_j], lv51_local[v_i // T.int64(32), v_j])\n",
      "                                                T.writes(decode_local[v_i, v_j])\n",
      "                                                decode_local[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv50_local[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv51_local[v_i // T.int64(32), v_j]\n",
      "                                    for ax0, ax1 in T.grid(T.int64(1), processed_rows_per_thread):\n",
      "                                        for ax2 in T.vectorized(T.int64(1)):\n",
      "                                            with T.block(\"lv5_pad_local\"):\n",
      "                                                v0 = T.axis.spatial(T.int64(1), ax0)\n",
      "                                                v1 = T.axis.spatial((n + T.int64(31)) // T.int64(32) * T.int64(32), i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32) + i0_i1_fused_1_1 * T.Cast(\"int64\", processed_rows_per_thread) + T.Cast(\"int64\", ax1))\n",
      "                                                v2 = T.axis.spatial(T.int64(11008), k_0 * T.int64(32) + k_1 * T.int64(8) + k_2 + ax2)\n",
      "                                                T.reads(lv5[v0, v1, v2])\n",
      "                                                T.writes(lv5_pad_local[v0, v1, v2])\n",
      "                                                lv5_pad_local[v0, v1, v2] = T.if_then_else(v1 < n, lv5[v0, v1, v2], T.float16(0))\n",
      "                                    for i0_i1_fused_1_2 in range(processed_rows_per_thread):\n",
      "                                        for i2_2 in T.vectorized(vectorize_factor):\n",
      "                                            with T.block(\"NT_matmul_update\"):\n",
      "                                                v_i0 = T.axis.spatial(T.int64(1), T.int64(0))\n",
      "                                                v_i1 = T.axis.spatial((n + T.int64(31)) // T.int64(32) * T.int64(32), i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32) + i0_i1_fused_1_1 * T.Cast(\"int64\", processed_rows_per_thread) + T.Cast(\"int64\", i0_i1_fused_1_2))\n",
      "                                                v_i2 = T.axis.spatial(T.int64(4096), i2_0 * T.Cast(\"int64\", ThreadIdx_x * vectorize_factor) + i2_1 * T.Cast(\"int64\", vectorize_factor) + T.Cast(\"int64\", i2_2))\n",
      "                                                v_k = T.axis.reduce(T.int64(11008), k_0 * T.int64(32) + k_1 * T.int64(8) + k_2)\n",
      "                                                T.reads(var_NT_matmul_intermediate_pad_local[v_i0, v_i1, v_i2], lv5_pad_local[v_i0, v_i1, v_k], decode_local[v_k, v_i2])\n",
      "                                                T.writes(var_NT_matmul_intermediate_pad_local[v_i0, v_i1, v_i2])\n",
      "                                                var_NT_matmul_intermediate_pad_local[v_i0, v_i1, v_i2] = var_NT_matmul_intermediate_pad_local[v_i0, v_i1, v_i2] + lv5_pad_local[v_i0, v_i1, v_k] * decode_local[v_k, v_i2]\n",
      "                        for ax0, ax1 in T.grid(T.int64(1), processed_rows_per_thread):\n",
      "                            for ax2 in T.vectorized(vectorize_factor):\n",
      "                                with T.block(\"var_NT_matmul_intermediate_pad_local\"):\n",
      "                                    v0 = T.axis.spatial(T.int64(1), ax0)\n",
      "                                    v1 = T.axis.spatial((n + T.int64(31)) // T.int64(32) * T.int64(32), i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32) + i0_i1_fused_1_1 * T.Cast(\"int64\", processed_rows_per_thread) + T.Cast(\"int64\", ax1))\n",
      "                                    v2 = T.axis.spatial(T.int64(4096), i2_0 * T.Cast(\"int64\", ThreadIdx_x * vectorize_factor) + i2_1 * T.Cast(\"int64\", vectorize_factor) + T.Cast(\"int64\", ax2))\n",
      "                                    T.reads(lv3[v0, v1, v2], var_NT_matmul_intermediate_pad_local[v0, v1, v2])\n",
      "                                    T.writes(p_output0_intermediate[v0, v1, v2])\n",
      "                                    if v1 < n:\n",
      "                                        p_output0_intermediate[v0, v1, v2] = lv3[v0, v1, v2] + var_NT_matmul_intermediate_pad_local[v0, v1, v2]\n",
      "================================================\n",
      "// Function: main_kernel\n",
      "#ifdef cl_khr_fp16\n",
      "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
      "#elif defined(cl_amd_fp16)\n",
      "#pragma OPENCL EXTENSION cl_amd_fp16 : enable\n",
      "#else\n",
      "#error \"Half precision floating point not supported by OpenCL implementation on your device.\" \n",
      "#endif\n",
      "\n",
      "__kernel void main_kernel(__global half* restrict lv3, __global half* restrict lv5, __global uint* restrict lv50, __global half* restrict lv51, __global half* restrict p_output0_intermediate, long n) {\n",
      "  half4 var_NT_matmul_intermediate_pad_local[2];\n",
      "  half4 lv51_local[1];\n",
      "  uint4 lv50_local[1];\n",
      "  half4 decode_local[1];\n",
      "  half lv5_pad_local[2];\n",
      "  for (int i0_i1_fused_1_2_init = 0; i0_i1_fused_1_2_init < 2; ++i0_i1_fused_1_2_init) {\n",
      "    var_NT_matmul_intermediate_pad_local[i0_i1_fused_1_2_init] = ((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f));\n",
      "  }\n",
      "  for (long k_0 = 0; k_0 < (long)344; ++k_0) {\n",
      "    lv51_local[0] = vload4(0, lv51 + (((k_0 * (long)4096) + ((convert_long(get_group_id(0))) * (long)128)) + ((convert_long(get_local_id(0))) * (long)4)));\n",
      "    for (long k_1 = 0; k_1 < (long)4; ++k_1) {\n",
      "      lv50_local[0] = vload4(0, lv50 + ((((k_0 * (long)16384) + (k_1 * (long)4096)) + ((convert_long(get_group_id(0))) * (long)128)) + ((convert_long(get_local_id(0))) * (long)4)));\n",
      "      for (long k_2 = 0; k_2 < (long)8; ++k_2) {\n",
      "        decode_local[0] = (((convert_half4(((lv50_local[0]  >>  ((uint4)(((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4))))  &  ((uint4)((uint)15, (uint)15, (uint)15, (uint)15))))) - ((half4)((half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f))) * lv51_local[0]);\n",
      "        for (int ax1 = 0; ax1 < 2; ++ax1) {\n",
      "          lv5_pad_local[ax1] = ((((((convert_long(get_group_id(1))) * (long)32) + ((convert_long(get_local_id(1))) * (long)2)) + (convert_long(ax1))) < n) ? lv5[(((((((convert_long(get_group_id(1))) * (long)352256) + ((convert_long(get_local_id(1))) * (long)22016)) + ((convert_long(ax1)) * (long)11008)) + (k_0 * (long)32)) + (k_1 * (long)8)) + k_2)] : (half)0.000000e+00f);\n",
      "        }\n",
      "        for (int i0_i1_fused_1_2 = 0; i0_i1_fused_1_2 < 2; ++i0_i1_fused_1_2) {\n",
      "          var_NT_matmul_intermediate_pad_local[i0_i1_fused_1_2] = (var_NT_matmul_intermediate_pad_local[i0_i1_fused_1_2] + (((half4)(lv5_pad_local[i0_i1_fused_1_2], lv5_pad_local[i0_i1_fused_1_2], lv5_pad_local[i0_i1_fused_1_2], lv5_pad_local[i0_i1_fused_1_2])) * decode_local[0]));\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  for (int ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n",
      "    if (((((convert_long(get_group_id(1))) * (long)32) + ((convert_long(get_local_id(1))) * (long)2)) + (convert_long(ax1_1))) < n) {\n",
      "      vstore4((vload4(0, lv3 + ((((((convert_long(get_group_id(1))) * (long)131072) + ((convert_long(get_local_id(1))) * (long)8192)) + ((convert_long(ax1_1)) * (long)4096)) + ((convert_long(get_group_id(0))) * (long)128)) + ((convert_long(get_local_id(0))) * (long)4))) + var_NT_matmul_intermediate_pad_local[ax1_1]), 0, p_output0_intermediate + ((((((convert_long(get_group_id(1))) * (long)131072) + ((convert_long(get_local_id(1))) * (long)8192)) + ((convert_long(ax1_1)) * (long)4096)) + ((convert_long(get_group_id(0))) * (long)128)) + ((convert_long(get_local_id(0))) * (long)4)));\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tvm\n",
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm import autotvm, auto_scheduler\n",
    "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
    "from tvm import meta_schedule as ms\n",
    "from tvm.ir import IRModule\n",
    "from tvm import relax\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils, ndk\n",
    "x_shape = 11008\n",
    "w_w_x = 1376\n",
    "w_s_x = 344\n",
    "w_y = 4096\n",
    "func_name = \"main\"\n",
    "@I.ir_module\n",
    "class ModuleSrc:\n",
    "    @T.prim_func(private=False)\n",
    "    # fused_fused_decode5_fused_NT_matmul4_add\n",
    "    def main(lv17: T.Buffer((T.int64(1376), T.int64(4096)), \"uint32\"), lv18: T.Buffer((T.int64(344), T.int64(4096)), \"float16\"), p_lv16: T.handle, p_lv12: T.handle, p_output0: T.handle):\n",
    "        T.func_attr({\"tir.noalias\": T.bool(True)})\n",
    "        n = T.int64()\n",
    "        lv16 = T.match_buffer(p_lv16, (T.int64(1), n, T.int64(11008)), \"float16\")\n",
    "        lv12 = T.match_buffer(p_lv12, (T.int64(1), n, T.int64(4096)), \"float16\")\n",
    "        p_output0_intermediate = T.match_buffer(p_output0, (T.int64(1), n, T.int64(4096)), \"float16\")\n",
    "        # with T.block(\"root\"):\n",
    "        decode = T.alloc_buffer((T.int64(11008), T.int64(4096)), \"float16\")\n",
    "        p_output0_intermediate_1 = T.alloc_buffer((T.int64(4096), T.int64(11008)), \"float16\")\n",
    "        var_NT_matmul_intermediate = T.alloc_buffer((T.int64(1), n, T.int64(4096)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(11008), T.int64(4096)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv17[v_i // T.int64(8), v_j], lv18[v_i // T.int64(32), v_j])\n",
    "                T.writes(decode[v_i, v_j])\n",
    "                decode[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv17[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv18[v_i // T.int64(32), v_j]\n",
    "        for ax0, ax1 in T.grid(T.int64(4096), T.int64(11008)):\n",
    "            with T.block(\"T_transpose\"):\n",
    "                v_ax0, v_ax1 = T.axis.remap(\"SS\", [ax0, ax1])\n",
    "                T.reads(decode[v_ax1, v_ax0])\n",
    "                T.writes(p_output0_intermediate_1[v_ax0, v_ax1])\n",
    "                p_output0_intermediate_1[v_ax0, v_ax1] = decode[v_ax1, v_ax0]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), n, T.int64(4096), T.int64(11008)):\n",
    "            with T.block(\"NT_matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv16[v_i0, v_i1, v_k], p_output0_intermediate_1[v_i2, v_k])\n",
    "                T.writes(var_NT_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_NT_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_NT_matmul_intermediate[v_i0, v_i1, v_i2] = var_NT_matmul_intermediate[v_i0, v_i1, v_i2] + lv16[v_i0, v_i1, v_k] * p_output0_intermediate_1[v_i2, v_k]\n",
    "        for ax0, ax1, ax2 in T.grid(T.int64(1), n, T.int64(4096)):\n",
    "            with T.block(\"T_add\"):\n",
    "                v_ax0, v_ax1, v_ax2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "                T.reads(lv12[v_ax0, v_ax1, v_ax2], var_NT_matmul_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                T.writes(p_output0_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                p_output0_intermediate[v_ax0, v_ax1, v_ax2] = lv12[v_ax0, v_ax1, v_ax2] + var_NT_matmul_intermediate[v_ax0, v_ax1, v_ax2]\n",
    "\n",
    "@I.ir_module\n",
    "class ModuleToManual:\n",
    "    @T.prim_func(private=False)\n",
    "    #fused_decode2_fused_NT_matmul3_add_after\n",
    "    def main(\n",
    "        lv50: T.Buffer((T.int64(1376), T.int64(4096)), \"uint32\"),\n",
    "        lv51: T.Buffer((T.int64(344), T.int64(4096)), \"float16\"),\n",
    "        p_lv5: T.handle,\n",
    "        p_lv3: T.handle,\n",
    "        p_output0: T.handle,\n",
    "    ):\n",
    "        T.func_attr({\"tir.noalias\": T.bool(True), \"tir.is_scheduled\": 1})\n",
    "        n = T.int64()\n",
    "        lv5 = T.match_buffer(p_lv5, (T.int64(1), n, T.int64(11008)), \"float16\")\n",
    "        lv3 = T.match_buffer(p_lv3, (T.int64(1), n, T.int64(4096)), \"float16\")\n",
    "        p_output0_intermediate = T.match_buffer(\n",
    "            p_output0, (T.int64(1), n, T.int64(4096)), \"float16\"\n",
    "        )\n",
    "        # with T.block(\"root\"):\n",
    "        decode_local = T.alloc_buffer(\n",
    "            (T.int64(11008), T.int64(4096)), \"float16\", scope=\"local\"\n",
    "        )\n",
    "        lv50_local = T.alloc_buffer((T.int64(1376), T.int64(4096)), \"uint32\", scope=\"local\")\n",
    "        lv51_local = T.alloc_buffer((T.int64(344), T.int64(4096)), \"float16\", scope=\"local\")\n",
    "        lv5_pad_local = T.alloc_buffer(\n",
    "            (T.int64(1), (n + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(11008)),\n",
    "            \"float16\",\n",
    "            scope=\"local\",\n",
    "        )\n",
    "        var_NT_matmul_intermediate_pad_local = T.alloc_buffer(\n",
    "            (T.int64(1), (n + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(4096)),\n",
    "            \"float16\",\n",
    "            scope=\"local\",\n",
    "        )\n",
    "        # 任务划分:\n",
    "        ### 一个thread处理 `processed_rows_per_thread`行 `vectorize_factor` 列(输出角度)\n",
    "        ### 完整处理 `processed_rows_per_thread` 行输入需要: blockIdx.x * threadIdx.x 配合\n",
    "        ### 完整处理 `n` 行输入需要: blockIdx.y * threadIdx.y 配合\n",
    "        #### 分析: 根据`n`变化的只有 blockIdx.y, 说明 blockIdx.x * threadIdx.x * threadIdx.y 可以完整处理32行输入\n",
    "        #  4 16 24 128 2\n",
    "        BlockIdx_x = 32\n",
    "        # n = 32\n",
    "        # BlockIdx_y = (n+31)//32 * 32 # 这里32是假设输入为32的倍数, //32的32 = thready * \n",
    "        ThreadIdx_x = 32#16\n",
    "        ThreadIdx_y = 16#8\n",
    "        vectorize_factor = 4#8\n",
    "        # processed_columns_per_thread = vectorize_factor# w_y / (BlockIdx_x * ThreadIdx_x) == vectorize_factor\n",
    "        processed_rows_per_thread = 2# == 32 / threadIdx.y\n",
    "        ### reduce拆分\n",
    "        ## 当前各个threadIndex的含义：\n",
    "        #   blockIdx.y  : Bb = (n + 31) // 32 * 32, 一次kernel调用用于处理 Bb 个32行输入\n",
    "        #   blockIdx.x  : 32, 和 threadIdx.x, vectorize_factor 配合, 一个线程用于处理 4096 个元素(一行)\n",
    "        #                   (= blockIdx.x * threadIdx.x * vectorize_factor), 此外单线程还会同时处理 processed_rows_per_thread 行\n",
    "        #   threadIdx.x : 16, 和 blockIdx.x, vectorize_factor 配合, 一个线程用于处理 4096 个元素(一行)\n",
    "        #                   (= blockIdx.x * threadIdx.x * vectorize_factor), 此外单线程还会同时处理 processed_rows_per_thread 行\n",
    "        #   threadIdx.y : 8, 和 processed_rows_per_thread 配合用于处理 1个 32行输入\n",
    "        ## 加一个threadIdx.z, 来处理拆分reduce的情况\n",
    "        #   当前一列11008拆分情况(参数固定, 其中4和8受限于group_size和weight bits不可修改, 仅可拆344): 344 * 4 * 8\n",
    "        #   rsplit_factor: [8, 4, 2] -> outter_loop: [43, 86, 172]\n",
    "        # rsplit_factor = 8\n",
    "        # r_outter_loop = 344 // rsplit_factor\n",
    "        # assert 344 % rsplit_factor == 0, \"344 is not divisible by rsplit_factor\"\n",
    "\n",
    "        for i0_i1_fused_0_i0_i1_fused_1_0_fused in T.thread_binding(\n",
    "            (n + T.int64(31)) // T.int64(32), thread=\"blockIdx.y\"\n",
    "        ):\n",
    "            for i2_0 in T.thread_binding(T.int64(BlockIdx_x), thread=\"blockIdx.x\"):\n",
    "                for i0_i1_fused_1_1 in T.thread_binding(T.int64(ThreadIdx_y), thread=\"threadIdx.y\"):\n",
    "                    for i2_1 in T.thread_binding(T.int64(ThreadIdx_x), thread=\"threadIdx.x\"):\n",
    "                        for i0_i1_fused_1_2_init in range(T.int64(processed_rows_per_thread)):\n",
    "                            for i2_2_init in T.vectorized(T.int64(vectorize_factor)):\n",
    "                                with T.block(\"NT_matmul_init\"):\n",
    "                                    v_i0 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "                                    v_i1 = T.axis.spatial(\n",
    "                                        (n + T.int64(31)) // T.int64(32) * T.int64(32),\n",
    "                                        i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32)\n",
    "                                        + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                        + i0_i1_fused_1_2_init,\n",
    "                                    )\n",
    "                                    v_i2 = T.axis.spatial(\n",
    "                                        T.int64(4096),\n",
    "                                        i2_0 * (ThreadIdx_x * vectorize_factor) + i2_1 * vectorize_factor + i2_2_init,\n",
    "                                    )\n",
    "                                    T.reads()\n",
    "                                    T.writes(\n",
    "                                        var_NT_matmul_intermediate_pad_local[\n",
    "                                            v_i0, v_i1, v_i2\n",
    "                                        ]\n",
    "                                    )\n",
    "                                    var_NT_matmul_intermediate_pad_local[\n",
    "                                        v_i0, v_i1, v_i2\n",
    "                                    ] = T.float16(0)\n",
    "                        for k_0 in range(T.int64(344)):\n",
    "                            for ax0 in range(T.int64(1)):\n",
    "                                for ax1 in T.vectorized(T.int64(vectorize_factor)):\n",
    "                                    with T.block(\"lv51_local\"):\n",
    "                                        v0 = T.axis.spatial(T.int64(344), k_0 + ax0)\n",
    "                                        v1 = T.axis.spatial(\n",
    "                                            T.int64(4096),\n",
    "                                            i2_0 * (ThreadIdx_x * vectorize_factor) + i2_1 * vectorize_factor + ax1,\n",
    "                                        )\n",
    "                                        T.reads(lv51[v0, v1])\n",
    "                                        T.writes(lv51_local[v0, v1])\n",
    "                                        lv51_local[v0, v1] = lv51[v0, v1]\n",
    "                            for k_1 in range(T.int64(4)):\n",
    "                                for ax0 in range(T.int64(1)):\n",
    "                                    for ax1 in T.vectorized(vectorize_factor):\n",
    "                                        with T.block(\"lv50_local\"):\n",
    "                                            v0 = T.axis.spatial(\n",
    "                                                T.int64(1376), k_0 * T.int64(4) + k_1 + ax0\n",
    "                                            )\n",
    "                                            v1 = T.axis.spatial(\n",
    "                                                T.int64(4096),\n",
    "                                                i2_0 * (ThreadIdx_x * vectorize_factor)\n",
    "                                                + i2_1 * vectorize_factor\n",
    "                                                + ax1,\n",
    "                                            )\n",
    "                                            T.reads(lv50[v0, v1])\n",
    "                                            T.writes(lv50_local[v0, v1])\n",
    "                                            lv50_local[v0, v1] = lv50[v0, v1]\n",
    "                                for k_2 in range(T.int64(8)):\n",
    "                                    for ax0 in range(T.int64(1)):\n",
    "                                        for ax1 in T.vectorized(vectorize_factor):\n",
    "                                            with T.block(\"decode\"):\n",
    "                                                v_i = T.axis.spatial(\n",
    "                                                    T.int64(11008),\n",
    "                                                    k_0 * T.int64(32)\n",
    "                                                    + k_1 * T.int64(8)\n",
    "                                                    + k_2\n",
    "                                                    + ax0,\n",
    "                                                )\n",
    "                                                v_j = T.axis.spatial(\n",
    "                                                    T.int64(4096),\n",
    "                                                    i2_0 * (ThreadIdx_x * vectorize_factor)\n",
    "                                                    + i2_1 * vectorize_factor\n",
    "                                                    + ax1,\n",
    "                                                )\n",
    "                                                T.reads(\n",
    "                                                    lv50_local[v_i // T.int64(8), v_j],\n",
    "                                                    lv51_local[v_i // T.int64(32), v_j],\n",
    "                                                )\n",
    "                                                T.writes(decode_local[v_i, v_j])\n",
    "                                                decode_local[v_i, v_j] = (\n",
    "                                                    T.Cast(\n",
    "                                                        \"float16\",\n",
    "                                                        T.bitwise_and(\n",
    "                                                            T.shift_right(\n",
    "                                                                lv50_local[\n",
    "                                                                    v_i // T.int64(8), v_j\n",
    "                                                                ],\n",
    "                                                                T.Cast(\n",
    "                                                                    \"uint32\",\n",
    "                                                                    v_i % T.int64(8),\n",
    "                                                                )\n",
    "                                                                * T.uint32(4),\n",
    "                                                            ),\n",
    "                                                            T.uint32(15),\n",
    "                                                        ),\n",
    "                                                    )\n",
    "                                                    - T.float16(7)\n",
    "                                                ) * lv51_local[v_i // T.int64(32), v_j]\n",
    "                                    for ax0, ax1 in T.grid(T.int64(1), processed_rows_per_thread):\n",
    "                                        for ax2 in T.vectorized(T.int64(1)):\n",
    "                                            with T.block(\"lv5_pad_local\"):\n",
    "                                                v0 = T.axis.spatial(T.int64(1), ax0)\n",
    "                                                v1 = T.axis.spatial(\n",
    "                                                    (n + T.int64(31))\n",
    "                                                    // T.int64(32)\n",
    "                                                    * T.int64(32),\n",
    "                                                    i0_i1_fused_0_i0_i1_fused_1_0_fused\n",
    "                                                    * T.int64(32)\n",
    "                                                    + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                                    + ax1,\n",
    "                                                )\n",
    "                                                v2 = T.axis.spatial(\n",
    "                                                    T.int64(11008),\n",
    "                                                    k_0 * T.int64(32)\n",
    "                                                    + k_1 * T.int64(8)\n",
    "                                                    + k_2\n",
    "                                                    + ax2,\n",
    "                                                )\n",
    "                                                T.reads(lv5[v0, v1, v2])\n",
    "                                                T.writes(lv5_pad_local[v0, v1, v2])\n",
    "                                                lv5_pad_local[v0, v1, v2] = T.if_then_else(\n",
    "                                                    v1 < n, lv5[v0, v1, v2], T.float16(0)\n",
    "                                                )\n",
    "                                    for i0_i1_fused_1_2 in range(processed_rows_per_thread):\n",
    "                                        for i2_2 in T.vectorized(vectorize_factor):\n",
    "                                            with T.block(\"NT_matmul_update\"):\n",
    "                                                v_i0 = T.axis.spatial(\n",
    "                                                    T.int64(1), T.int64(0)\n",
    "                                                )\n",
    "                                                v_i1 = T.axis.spatial(\n",
    "                                                    (n + T.int64(31))\n",
    "                                                    // T.int64(32)\n",
    "                                                    * T.int64(32),\n",
    "                                                    i0_i1_fused_0_i0_i1_fused_1_0_fused\n",
    "                                                    * T.int64(32)\n",
    "                                                    + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                                    + i0_i1_fused_1_2,\n",
    "                                                )\n",
    "                                                v_i2 = T.axis.spatial(\n",
    "                                                    T.int64(4096),\n",
    "                                                    i2_0 * (ThreadIdx_x * vectorize_factor)\n",
    "                                                    + i2_1 * vectorize_factor\n",
    "                                                    + i2_2,\n",
    "                                                )\n",
    "                                                v_k = T.axis.reduce(\n",
    "                                                    T.int64(11008),\n",
    "                                                    k_0 * T.int64(32)\n",
    "                                                    + k_1 * T.int64(8)\n",
    "                                                    + k_2,\n",
    "                                                )\n",
    "                                                T.reads(\n",
    "                                                    var_NT_matmul_intermediate_pad_local[\n",
    "                                                        v_i0, v_i1, v_i2\n",
    "                                                    ],\n",
    "                                                    lv5_pad_local[v_i0, v_i1, v_k],\n",
    "                                                    decode_local[v_k, v_i2],\n",
    "                                                )\n",
    "                                                T.writes(\n",
    "                                                    var_NT_matmul_intermediate_pad_local[\n",
    "                                                        v_i0, v_i1, v_i2\n",
    "                                                    ]\n",
    "                                                )\n",
    "                                                var_NT_matmul_intermediate_pad_local[\n",
    "                                                    v_i0, v_i1, v_i2\n",
    "                                                ] = (\n",
    "                                                    var_NT_matmul_intermediate_pad_local[\n",
    "                                                        v_i0, v_i1, v_i2\n",
    "                                                    ]\n",
    "                                                    + lv5_pad_local[v_i0, v_i1, v_k]\n",
    "                                                    * decode_local[v_k, v_i2]\n",
    "                                                )\n",
    "                        for ax0, ax1 in T.grid(T.int64(1), processed_rows_per_thread):\n",
    "                            for ax2 in T.vectorized(vectorize_factor):\n",
    "                                with T.block(\"var_NT_matmul_intermediate_pad_local\"):\n",
    "                                    v0 = T.axis.spatial(T.int64(1), ax0)\n",
    "                                    v1 = T.axis.spatial(\n",
    "                                        (n + T.int64(31)) // T.int64(32) * T.int64(32),\n",
    "                                        i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32)\n",
    "                                        + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                        + ax1,\n",
    "                                    )\n",
    "                                    v2 = T.axis.spatial(\n",
    "                                        T.int64(4096),\n",
    "                                        i2_0 * (ThreadIdx_x * vectorize_factor) + i2_1 * vectorize_factor + ax2,\n",
    "                                    )\n",
    "                                    T.reads(\n",
    "                                        lv3[v0, v1, v2],\n",
    "                                        var_NT_matmul_intermediate_pad_local[v0, v1, v2],\n",
    "                                    )\n",
    "                                    T.writes(p_output0_intermediate[v0, v1, v2])\n",
    "                                    if v1 < n:\n",
    "                                        p_output0_intermediate[v0, v1, v2] = (\n",
    "                                            lv3[v0, v1, v2]\n",
    "                                            + var_NT_matmul_intermediate_pad_local[\n",
    "                                                v0, v1, v2\n",
    "                                            ]\n",
    "                                        )\n",
    "sch_manual = tvm.tir.Schedule(ModuleToManual)\n",
    "print(sch_manual.mod.script())\n",
    "print(\"================================================\")\n",
    "rt_mod = tvm.build(sch_manual.mod, target=\"opencl\")\n",
    "print(rt_mod.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda -keys=cuda,gpu -arch=sm_61 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-13696. -13152. -13264. -13496. -13016. -13104. -12216. -13536.\n",
      "   -13608. -13192.]\n",
      "  [-13720. -13192. -13448. -13288. -13016. -13432. -12512. -13832.\n",
      "   -13864. -13000.]]]\n",
      "[[[-13696. -13152. -13264. -13496. -13016. -13104. -12216. -13536.\n",
      "   -13608. -13192.]\n",
      "  [-13720. -13192. -13448. -13288. -13016. -13432. -12512. -13832.\n",
      "   -13864. -13000.]]]\n"
     ]
    }
   ],
   "source": [
    "# run and compare with cuda\n",
    "import numpy as np\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "seq_len = 32\n",
    "W_w_np = np.random.uniform(size=(w_w_x, w_y)).astype(\"uint32\")\n",
    "W_s_np = np.random.uniform(size=(w_s_x, w_y)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, seq_len, x_shape)).astype(\"float16\")\n",
    "Add_np = np.random.uniform(size=(1, seq_len, w_y)).astype(\"float16\")\n",
    "# W_w_np = np.ones((w_w_x, w_y), np.uint32) * 1#.astype(\"uint32\")\n",
    "# W_s_np = np.ones((w_s_x, w_y), np.float16) * 1#.astype(\"float16\") * 2\n",
    "# Input_np = np.ones((1, 1, x_shape), np.float16)#.astype(\"float16\")\n",
    "Output_nd = tvm.nd.array(np.zeros((1, seq_len, w_y), dtype=\"float16\"), dev)\n",
    "def numpy_caculate():\n",
    "    test_rows = 2\n",
    "    test_cols = 10\n",
    "    output = np.zeros((1, test_rows, test_cols), dtype = np.float16)\n",
    "    W_w_inv_np = np.transpose(W_w_np)\n",
    "    W_s_inv_np = np.transpose(W_s_np)\n",
    "    for row in range(test_rows):\n",
    "        for i in range(test_cols):\n",
    "            for r in range(x_shape):\n",
    "                temp = Input_np[0][row][r] * np.float16((W_w_inv_np[i][r // 8] >> ((r % 8) * 4) & (15)) - np.float16(7.0)) * W_s_inv_np[i][r // 32]\n",
    "                output[0][row][i] = output[0][row][i] + temp\n",
    "            output[0][row][i] = output[0][row][i] + Add_np[0][row][i]\n",
    "    print(output)\n",
    "    output = np.zeros((1, test_rows, test_cols), dtype = np.float16)\n",
    "    for row in range(test_rows):\n",
    "        for i in range(test_cols):\n",
    "            for r in range(x_shape):\n",
    "                temp = Input_np[0][row][r] * np.float16((W_w_np[r // 8][i] >> ((r % 8) * 4) & (15)) - np.float16(7.0)) * W_s_np[r // 32][i]\n",
    "                temp_output = output[0][row][i]\n",
    "                output[0][row][i] = temp_output + temp\n",
    "                # print(f\"{temp_output} + {temp} = {output[0][0][i]}\")\n",
    "            output[0][row][i] = output[0][row][i] + Add_np[0][row][i]\n",
    "    print(output)\n",
    "numpy_caculate()\n",
    "def print_npdata(np_data: np.ndarray) :\n",
    "    print(np_data)\n",
    "    print_num = 20\n",
    "    d = np_data.flatten()\n",
    "    p_size = print_num if d.size > print_num else d.size\n",
    "    print(d[:p_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_evaluator GEMV-Blocking: 1.541656 GFLOPS\n",
      "[[[-13704. -13152. -13264. ... -12968. -12312. -13288.]\n",
      "  [-13720. -13192. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13376. ... -13136. -12424. -13384.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12880. -12552. -13552.]\n",
      "  [-13800. -13144. -13328. ... -12864. -12464. -13640.]\n",
      "  [-13664. -13232. -13360. ... -12952. -12376. -13360.]]]\n",
      "[-13704. -13152. -13264. -13504. -13016. -13128. -12216. -13544. -13600.\n",
      " -13192. -13160. -12784. -13136. -13392. -13624. -13736. -12800. -11776.\n",
      " -12904. -12656.]\n"
     ]
    }
   ],
   "source": [
    "# cuda未优化版本测试\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "rt_mod = tvm.build(src_gpu_mod, target=\"cuda\")\n",
    "W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Add_nd = tvm.nd.array(Add_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, seq_len, w_y), dtype=\"float16\"), dev)\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "print(\"manual_evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / evaluator(W_w_nd, W_s_nd, Input_nd, Add_nd, Output_nd).mean / 1e9))\n",
    "# print(Output_nd.numpy())\n",
    "print_npdata(Output_nd.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TVM_NDK_CC\"]=\"/home/sensetime/Android/Sdk/ndk/25.2.9519653/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android33-clang++\"\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.4.236.32\"\n",
    "rpc_port = 9190\n",
    "comp_target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-android\")  # TODO: Only support arm64 for now\n",
    "\n",
    "def test_opencl(mod: tvm.IRModule, name_hint: str):\n",
    "    # mod = tvm.lower(sch_manual.mod)\n",
    "    print(\"Build ...\")\n",
    "    android_rt_mod = tvm.build(mod, target=\"opencl\", target_host=\"llvm -mtriple=aarch64-linux-android\")\n",
    "    # print(android_rt_mod.imported_modules[0].get_source())\n",
    "    temp = utils.tempdir()\n",
    "    path_dso_cl = temp.relpath(\"dev_lib_cl.so\")\n",
    "    android_rt_mod.export_library(path_dso_cl, ndk.create_shared)\n",
    "\n",
    "    print(\"Run GPU(OpenCL Flavor) test ...\")\n",
    "    # Establish remote connection with target hardware\n",
    "\n",
    "    tracker = rpc.connect_tracker(rpc_host, rpc_port)\n",
    "    remote = tracker.request(device_key, priority=0, session_timeout=60)\n",
    "    print(\"Connect to device done.\")\n",
    "    dev = remote.cl(0)\n",
    "    remote.upload(path_dso_cl)\n",
    "    f1 = remote.load_module(\"dev_lib_cl.so\")\n",
    "\n",
    "    W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "    W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "    Input_nd = tvm.nd.array(Input_np, dev)\n",
    "    Add_nd = tvm.nd.array(Add_np, dev)\n",
    "    Output_nd = tvm.nd.array(np.zeros((1, seq_len, w_y), dtype=\"float16\"), dev)\n",
    "    test_number=32\n",
    "    time_f = f1.time_evaluator(f1.entry_name, dev, number=test_number)\n",
    "    cost = time_f(W_w_nd, W_s_nd, Input_nd, Add_nd, Output_nd).mean\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %fms with loop %d\" % (name_hint, cost * 1000, test_number))\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %fGFLOPS\" % (name_hint, num_flop / cost / 1e9))\n",
    "\n",
    "    print_npdata(Output_nd.numpy())\n",
    "    # return Output_nd.numpy()\n",
    "    return cost*1000 # unit: ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[source] GEMV-Blocking: 191.681360ms with loop 32\n",
      "evaluator[source] GEMV-Blocking: 6.408588GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n"
     ]
    }
   ],
   "source": [
    "# 未优化版本opencl测试\n",
    "from tvm import dlight as dl\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    # src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "    mod_deploy = dl.ApplyDefaultSchedule(  # pylint: disable=not-callable\n",
    "        dl.gpu.Matmul(),\n",
    "        dl.gpu.GEMV(),\n",
    "        dl.gpu.Reduction(),\n",
    "        dl.gpu.GeneralReduction(),\n",
    "        dl.gpu.Fallback(),\n",
    "    )(sch.mod)\n",
    "src_output = test_opencl(mod_deploy, \"source\")\n",
    "# print_npdata(src_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[opted] GEMV-Blocking: 11.407832ms with loop 32\n",
      "evaluator[opted] GEMV-Blocking: 107.681002GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n"
     ]
    }
   ],
   "source": [
    "#优化版本opencl测试\n",
    "# print(sch_manual.mod)\n",
    "opt_output = test_opencl(sch_manual.mod, \"opted\")\n",
    "# print_npdata(opt_output)\n",
    "np.testing.assert_equal(opt_output, src_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks: 150\n",
      "search record [1/150]: skip 8 16 1 512 2\n",
      "search record [2/150]: skip because bx isn't divisible 8 16 2 384 2\n",
      "search record [3/150]: start run 8 16 2 256 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 184.540640ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 6.656565GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [4/150]: skip because bx isn't divisible 8 16 3 192 2\n",
      "search record [5/150]: start run 8 16 4 128 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 92.357032ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 13.300631GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [6/150]: skip because bx isn't divisible 8 16 6 96 2\n",
      "search record [7/150]: start run 8 16 8 64 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 81.497384ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 15.072960GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [8/150]: skip because bx isn't divisible 8 16 11 48 2\n",
      "search record [9/150]: start run 8 16 16 32 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 167.341064ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 7.340737GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [10/150]: start run 8 16 32 16 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 222.593856ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 5.518601GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [11/150]: skip 8 8 1 512 4\n",
      "search record [12/150]: skip 8 8 2 384 4\n",
      "search record [13/150]: start run 8 8 2 256 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 141.576920ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 8.676603GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [14/150]: skip because bx isn't divisible 8 8 3 192 4\n",
      "search record [15/150]: start run 8 8 4 128 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 72.473696ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 16.949691GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [16/150]: skip because bx isn't divisible 8 8 6 96 4\n",
      "search record [17/150]: start run 8 8 8 64 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 50.963800ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 24.103516GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [18/150]: skip because bx isn't divisible 8 8 11 48 4\n",
      "search record [19/150]: start run 8 8 16 32 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 52.931632ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 23.207423GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [20/150]: start run 8 8 32 16 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 95.591048ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 12.850647GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [21/150]: skip 8 4 1 512 8\n",
      "search record [22/150]: skip 8 4 2 384 8\n",
      "search record [23/150]: skip 8 4 2 256 8\n",
      "search record [24/150]: skip 8 4 3 192 8\n",
      "search record [25/150]: start run 8 4 4 128 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 18.198240ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 67.501406GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [26/150]: skip because bx isn't divisible 8 4 6 96 8\n",
      "search record [27/150]: start run 8 4 8 64 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 9.581368ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 128.207870GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [28/150]: skip because bx isn't divisible 8 4 11 48 8\n",
      "search record [29/150]: start run 8 4 16 32 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 9.625520ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 127.619784GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [30/150]: start run 8 4 32 16 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 8.411888ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 146.032232GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [31/150]: skip 8 2 1 512 16\n",
      "search record [32/150]: skip 8 2 2 384 16\n",
      "search record [33/150]: skip 8 2 2 256 16\n",
      "search record [34/150]: skip 8 2 3 192 16\n",
      "search record [35/150]: skip 8 2 4 128 16\n",
      "search record [36/150]: skip 8 2 6 96 16\n",
      "search record [37/150]: start run 8 2 8 64 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 12.821152ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 95.810952GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [38/150]: skip because bx isn't divisible 8 2 11 48 16\n",
      "search record [39/150]: start run 8 2 16 32 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 12.827128ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 95.766315GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [40/150]: start run 8 2 32 16 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 9.724992ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 126.314426GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [41/150]: skip 8 1 1 512 32\n",
      "search record [42/150]: skip 8 1 2 384 32\n",
      "search record [43/150]: skip 8 1 2 256 32\n",
      "search record [44/150]: skip 8 1 3 192 32\n",
      "search record [45/150]: skip 8 1 4 128 32\n",
      "search record [46/150]: skip 8 1 6 96 32\n",
      "search record [47/150]: skip 8 1 8 64 32\n",
      "search record [48/150]: skip 8 1 11 48 32\n",
      "search record [49/150]: start run 8 1 16 32 32\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.345656ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 80.049154GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [50/150]: start run 8 1 32 16 32\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 12.205928ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 100.640179GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [51/150]: start run 4 16 2 512 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 571.066640ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 2.151074GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [52/150]: skip because bx isn't divisible 4 16 3 384 2\n",
      "search record [53/150]: start run 4 16 4 256 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 307.225976ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 3.998382GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [54/150]: skip because bx isn't divisible 4 16 6 192 2\n",
      "search record [55/150]: start run 4 16 8 128 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 171.037672ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 7.182083GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [56/150]: skip because bx isn't divisible 4 16 11 96 2\n",
      "search record [57/150]: start run 4 16 16 64 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 251.708456ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 4.880276GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [58/150]: skip because bx isn't divisible 4 16 22 48 2\n",
      "search record [59/150]: start run 4 16 32 32 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 384.803088ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 3.192300GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [60/150]: start run 4 16 64 16 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 714.522672ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1.719199GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [61/150]: skip 4 8 2 512 4\n",
      "search record [62/150]: skip 4 8 3 384 4\n",
      "search record [63/150]: start run 4 8 4 256 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 24.294936ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 50.562256GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [64/150]: skip because bx isn't divisible 4 8 6 192 4\n",
      "search record [65/150]: start run 4 8 8 128 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 12.207048ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 100.630946GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [66/150]: skip because bx isn't divisible 4 8 11 96 4\n",
      "search record [67/150]: start run 4 8 16 64 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 12.165800ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 100.972134GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [68/150]: skip because bx isn't divisible 4 8 22 48 4\n",
      "search record [69/150]: start run 4 8 32 32 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 10.812632ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 113.608489GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [70/150]: start run 4 8 64 16 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 10.797472ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 113.767999GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [71/150]: skip 4 4 2 512 8\n",
      "search record [72/150]: skip 4 4 3 384 8\n",
      "search record [73/150]: skip 4 4 4 256 8\n",
      "search record [74/150]: skip 4 4 6 192 8\n",
      "search record [75/150]: start run 4 4 8 128 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 10.654808ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 115.291311GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [76/150]: skip because bx isn't divisible 4 4 11 96 8\n",
      "search record [77/150]: start run 4 4 16 64 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 10.609816ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 115.780216GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [78/150]: skip because bx isn't divisible 4 4 22 48 8\n",
      "search record [79/150]: start run 4 4 32 32 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 8.865360ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 138.562538GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [80/150]: start run 4 4 64 16 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 8.993896ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 136.582276GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [81/150]: skip 4 2 2 512 16\n",
      "search record [82/150]: skip 4 2 3 384 16\n",
      "search record [83/150]: skip 4 2 4 256 16\n",
      "search record [84/150]: skip 4 2 6 192 16\n",
      "search record [85/150]: skip 4 2 8 128 16\n",
      "search record [86/150]: skip 4 2 11 96 16\n",
      "search record [87/150]: start run 4 2 16 64 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 13.554392ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 90.627952GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [88/150]: skip because bx isn't divisible 4 2 22 48 16\n",
      "search record [89/150]: start run 4 2 32 32 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 11.406216ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 107.696258GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [90/150]: start run 4 2 64 16 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 11.825120ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 103.881126GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [91/150]: skip 4 1 2 512 32\n",
      "search record [92/150]: skip 4 1 3 384 32\n",
      "search record [93/150]: skip 4 1 4 256 32\n",
      "search record [94/150]: skip 4 1 6 192 32\n",
      "search record [95/150]: skip 4 1 8 128 32\n",
      "search record [96/150]: skip 4 1 11 96 32\n",
      "search record [97/150]: skip 4 1 16 64 32\n",
      "search record [98/150]: skip 4 1 22 48 32\n",
      "search record [99/150]: start run 4 1 32 32 32\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 11.803096ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 104.074963GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [100/150]: start run 4 1 64 16 32\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 12.326024ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 99.659613GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [101/150]: start run 2 16 4 512 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 290.219552ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 4.232681GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [102/150]: skip because bx isn't divisible 2 16 6 384 2\n",
      "search record [103/150]: start run 2 16 8 256 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 160.892512ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 7.634953GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [104/150]: skip because bx isn't divisible 2 16 11 192 2\n",
      "search record [105/150]: start run 2 16 16 128 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 166.959072ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 7.357532GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [106/150]: skip because bx isn't divisible 2 16 22 96 2\n",
      "search record [107/150]: start run 2 16 32 64 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 226.126128ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 5.432396GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [108/150]: skip because bx isn't divisible 2 16 43 48 2\n",
      "search record [109/150]: start run 2 16 64 32 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 448.751632ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 2.737387GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [110/150]: start run 2 16 128 16 2\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 721.599344ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1.702339GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [111/150]: skip 2 8 4 512 4\n",
      "search record [112/150]: skip 2 8 6 384 4\n",
      "search record [113/150]: start run 2 8 8 256 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 21.629576ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 56.792920GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [114/150]: skip because bx isn't divisible 2 8 11 192 4\n",
      "search record [115/150]: start run 2 8 16 128 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 21.347216ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 57.544121GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [116/150]: skip because bx isn't divisible 2 8 22 96 4\n",
      "search record [117/150]: start run 2 8 32 64 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.327592ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 80.143494GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [118/150]: skip because bx isn't divisible 2 8 43 48 4\n",
      "search record [119/150]: start run 2 8 64 32 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.387568ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 79.831120GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [120/150]: start run 2 8 128 16 4\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.043120ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 81.659043GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [121/150]: skip 2 4 4 512 8\n",
      "search record [122/150]: skip 2 4 6 384 8\n",
      "search record [123/150]: skip 2 4 8 256 8\n",
      "search record [124/150]: skip 2 4 11 192 8\n",
      "search record [125/150]: start run 2 4 16 128 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 18.657456ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 65.839994GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [126/150]: skip because bx isn't divisible 2 4 22 96 8\n",
      "search record [127/150]: start run 2 4 32 64 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.687168ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 78.306472GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [128/150]: skip because bx isn't divisible 2 4 43 48 8\n",
      "search record [129/150]: start run 2 4 64 32 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.832264ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 77.588826GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [130/150]: start run 2 4 128 16 8\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.773008ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 77.880312GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [131/150]: skip 2 2 4 512 16\n",
      "search record [132/150]: skip 2 2 6 384 16\n",
      "search record [133/150]: skip 2 2 8 256 16\n",
      "search record [134/150]: skip 2 2 11 192 16\n",
      "search record [135/150]: skip 2 2 16 128 16\n",
      "search record [136/150]: skip 2 2 22 96 16\n",
      "search record [137/150]: start run 2 2 32 64 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 16.440840ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 74.716790GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [138/150]: skip because bx isn't divisible 2 2 43 48 16\n",
      "search record [139/150]: start run 2 2 64 32 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 16.493512ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 74.478182GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [140/150]: start run 2 2 128 16 16\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 15.568472ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 78.903491GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [141/150]: skip 2 1 4 512 32\n",
      "search record [142/150]: skip 2 1 6 384 32\n",
      "search record [143/150]: skip 2 1 8 256 32\n",
      "search record [144/150]: skip 2 1 11 192 32\n",
      "search record [145/150]: skip 2 1 16 128 32\n",
      "search record [146/150]: skip 2 1 22 96 32\n",
      "search record [147/150]: skip 2 1 32 64 32\n",
      "search record [148/150]: skip 2 1 43 48 32\n",
      "search record [149/150]: start run 2 1 64 32 32\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 17.216568ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 71.350271GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "search record [150/150]: start run 2 1 128 16 32\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 16.512224ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 74.393781GFLOPS\n",
      "[[[-13696. -13152. -13264. ... -12952. -12328. -13280.]\n",
      "  [-13720. -13200. -13448. ... -13112. -12416. -13640.]\n",
      "  [-13576. -13032. -13360. ... -13136. -12424. -13400.]\n",
      "  ...\n",
      "  [-13520. -13096. -13384. ... -12864. -12544. -13552.]\n",
      "  [-13800. -13152. -13328. ... -12864. -12464. -13632.]\n",
      "  [-13672. -13232. -13360. ... -12968. -12384. -13360.]]]\n",
      "[-13696. -13152. -13264. -13496. -13016. -13104. -12224. -13544. -13608.\n",
      " -13192. -13160. -12784. -13128. -13400. -13616. -13728. -12816. -11776.\n",
      " -12896. -12640.]\n",
      "=====\n",
      "================================\n",
      "+------------------+---------------------------+------------+-------------+-------------+--------------------+\n",
      "| vectorize_factor | processed_rows_per_thread | blockIdx.x | threadIdx.x | threadIdx.y |      cost(ms)      |\n",
      "+------------------+---------------------------+------------+-------------+-------------+--------------------+\n",
      "|        8         |             16            |     2      |     256     |      2      |     184.54064      |\n",
      "|        8         |             16            |     4      |     128     |      2      |     92.357032      |\n",
      "|        8         |             16            |     8      |      64     |      2      |     81.497384      |\n",
      "|        8         |             16            |     16     |      32     |      2      |     167.341064     |\n",
      "|        8         |             16            |     32     |      16     |      2      |     222.593856     |\n",
      "|        8         |             8             |     2      |     256     |      4      |     141.57692      |\n",
      "|        8         |             8             |     4      |     128     |      4      |     72.473696      |\n",
      "|        8         |             8             |     8      |      64     |      4      |      50.9638       |\n",
      "|        8         |             8             |     16     |      32     |      4      |     52.931632      |\n",
      "|        8         |             8             |     32     |      16     |      4      |     95.591048      |\n",
      "|        8         |             4             |     4      |     128     |      8      |      18.19824      |\n",
      "|        8         |             4             |     8      |      64     |      8      |      9.581368      |\n",
      "|        8         |             4             |     16     |      32     |      8      |      9.62552       |\n",
      "|        8         |             4             |     32     |      16     |      8      |      8.411888      |\n",
      "|        8         |             2             |     8      |      64     |      16     |     12.821152      |\n",
      "|        8         |             2             |     16     |      32     |      16     |     12.827128      |\n",
      "|        8         |             2             |     32     |      16     |      16     |      9.724992      |\n",
      "|        8         |             1             |     16     |      32     |      32     |     15.345656      |\n",
      "|        8         |             1             |     32     |      16     |      32     |     12.205928      |\n",
      "|        4         |             16            |     2      |     512     |      2      |     571.06664      |\n",
      "|        4         |             16            |     4      |     256     |      2      |     307.225976     |\n",
      "|        4         |             16            |     8      |     128     |      2      |     171.037672     |\n",
      "|        4         |             16            |     16     |      64     |      2      |     251.708456     |\n",
      "|        4         |             16            |     32     |      32     |      2      |     384.803088     |\n",
      "|        4         |             16            |     64     |      16     |      2      |     714.522672     |\n",
      "|        4         |             8             |     4      |     256     |      4      |     24.294936      |\n",
      "|        4         |             8             |     8      |     128     |      4      |     12.207048      |\n",
      "|        4         |             8             |     16     |      64     |      4      |      12.1658       |\n",
      "|        4         |             8             |     32     |      32     |      4      |     10.812632      |\n",
      "|        4         |             8             |     64     |      16     |      4      |     10.797472      |\n",
      "|        4         |             4             |     8      |     128     |      8      |     10.654808      |\n",
      "|        4         |             4             |     16     |      64     |      8      |     10.609816      |\n",
      "|        4         |             4             |     32     |      32     |      8      |      8.86536       |\n",
      "|        4         |             4             |     64     |      16     |      8      |      8.993896      |\n",
      "|        4         |             2             |     16     |      64     |      16     |     13.554392      |\n",
      "|        4         |             2             |     32     |      32     |      16     |     11.406216      |\n",
      "|        4         |             2             |     64     |      16     |      16     |      11.82512      |\n",
      "|        4         |             1             |     32     |      32     |      32     |     11.803096      |\n",
      "|        4         |             1             |     64     |      16     |      32     |     12.326024      |\n",
      "|        2         |             16            |     4      |     512     |      2      |     290.219552     |\n",
      "|        2         |             16            |     8      |     256     |      2      |     160.892512     |\n",
      "|        2         |             16            |     16     |     128     |      2      |     166.959072     |\n",
      "|        2         |             16            |     32     |      64     |      2      |     226.126128     |\n",
      "|        2         |             16            |     64     |      32     |      2      |     448.751632     |\n",
      "|        2         |             16            |    128     |      16     |      2      |     721.599344     |\n",
      "|        2         |             8             |     8      |     256     |      4      |     21.629576      |\n",
      "|        2         |             8             |     16     |     128     |      4      |     21.347216      |\n",
      "|        2         |             8             |     32     |      64     |      4      |     15.327592      |\n",
      "|        2         |             8             |     64     |      32     |      4      |     15.387568      |\n",
      "|        2         |             8             |    128     |      16     |      4      |      15.04312      |\n",
      "|        2         |             4             |     16     |     128     |      8      |     18.657456      |\n",
      "|        2         |             4             |     32     |      64     |      8      | 15.687168000000002 |\n",
      "|        2         |             4             |     64     |      32     |      8      | 15.832263999999999 |\n",
      "|        2         |             4             |    128     |      16     |      8      |     15.773008      |\n",
      "|        2         |             2             |     32     |      64     |      16     |      16.44084      |\n",
      "|        2         |             2             |     64     |      32     |      16     |     16.493512      |\n",
      "|        2         |             2             |    128     |      16     |      16     |     15.568472      |\n",
      "|        2         |             1             |     64     |      32     |      32     |     17.216568      |\n",
      "|        2         |             1             |    128     |      16     |      32     |     16.512224      |\n",
      "+------------------+---------------------------+------------+-------------+-------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# 自动搜索\n",
    "# 以32为倍数先搜一波\n",
    "# @TODO: 探索更低的倍数，以降低padding的额外性能损耗\n",
    "def auto_tune(record_file: str):\n",
    "    from typing import Union\n",
    "    def search(vf: int, pr: int, bx: int, tx: int, ty: int):\n",
    "        \"\"\"search by workgroup\n",
    "\n",
    "        Args:\n",
    "            blockIdxX (_type_): blockIdx.x\n",
    "            threadIdxX (_type_): threadIdx.x\n",
    "            vectorize_output (_type_): 输出的vectorize参数, 决定单线程输出多少个结果\n",
    "            vectorize_input (list, optional): 输入X拷贝到shared_memory时的vectorize参数, 一般为4或8\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        @I.ir_module\n",
    "        class ModuleToManual:\n",
    "            @T.prim_func(private=False)\n",
    "            # fused_decode1_fused_NT_matmul2_silu_after\n",
    "            def main(\n",
    "                lv50: T.Buffer((T.int64(1376), T.int64(4096)), \"uint32\"),\n",
    "                lv51: T.Buffer((T.int64(344), T.int64(4096)), \"float16\"),\n",
    "                p_lv5: T.handle,\n",
    "                p_lv3: T.handle,\n",
    "                p_output0: T.handle,\n",
    "            ):\n",
    "                T.func_attr({\"tir.noalias\": T.bool(True), \"tir.is_scheduled\": 1})\n",
    "                n = T.int64()\n",
    "                lv5 = T.match_buffer(p_lv5, (T.int64(1), n, T.int64(11008)), \"float16\")\n",
    "                lv3 = T.match_buffer(p_lv3, (T.int64(1), n, T.int64(4096)), \"float16\")\n",
    "                p_output0_intermediate = T.match_buffer(\n",
    "                    p_output0, (T.int64(1), n, T.int64(4096)), \"float16\"\n",
    "                )\n",
    "                # with T.block(\"root\"):\n",
    "                decode_local = T.alloc_buffer(\n",
    "                    (T.int64(11008), T.int64(4096)), \"float16\", scope=\"local\"\n",
    "                )\n",
    "                lv50_local = T.alloc_buffer((T.int64(1376), T.int64(4096)), \"uint32\", scope=\"local\")\n",
    "                lv51_local = T.alloc_buffer((T.int64(344), T.int64(4096)), \"float16\", scope=\"local\")\n",
    "                lv5_pad_local = T.alloc_buffer(\n",
    "                    (T.int64(1), (n + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(11008)),\n",
    "                    \"float16\",\n",
    "                    scope=\"local\",\n",
    "                )\n",
    "                var_NT_matmul_intermediate_pad_local = T.alloc_buffer(\n",
    "                    (T.int64(1), (n + T.int64(31)) // T.int64(32) * T.int64(32), T.int64(4096)),\n",
    "                    \"float16\",\n",
    "                    scope=\"local\",\n",
    "                )\n",
    "\n",
    "                # 任务划分:\n",
    "                ### 一个thread处理 `processed_rows_per_thread`行 `vectorize_factor` 列(输出角度)\n",
    "                ### 完整处理 `processed_rows_per_thread` 行输入需要: blockIdx.x * threadIdx.x 配合\n",
    "                ### 完整处理 `n` 行输入需要: blockIdx.y * threadIdx.y 配合\n",
    "                #### 分析: 根据`n`变化的只有 blockIdx.y, 说明 blockIdx.x * threadIdx.x * threadIdx.y 可以完整处理32行输入\n",
    "                BlockIdx_x = bx#32\n",
    "                # n = 32\n",
    "                # BlockIdx_y = (n+31)//32 * 32 # 这里32是假设输入为32的倍数, //32的32 = thready * \n",
    "                ThreadIdx_x = tx#16 * 3\n",
    "                ThreadIdx_y = ty#8\n",
    "                vectorize_factor = vf#8\n",
    "                # processed_columns_per_thread = vectorize_factor# w_y / (BlockIdx_x * ThreadIdx_x) == vectorize_factor\n",
    "                processed_rows_per_thread = pr#4\n",
    "\n",
    "                ## BlockIdx.y == [BlockIdx.x, ThreadIdx.x, ThraedIdx.y] 解决 seq_length为32的处理\n",
    "                for i0_i1_fused_0_i0_i1_fused_1_0_fused in T.thread_binding(\n",
    "                    (n + T.int64(31)) // T.int64(32), thread=\"blockIdx.y\"\n",
    "                ):\n",
    "                    for i2_0 in T.thread_binding(T.int64(BlockIdx_x), thread=\"blockIdx.x\"):\n",
    "                        for i0_i1_fused_1_1 in T.thread_binding(T.int64(ThreadIdx_y), thread=\"threadIdx.y\"):\n",
    "                            for i2_1 in T.thread_binding(T.int64(ThreadIdx_x), thread=\"threadIdx.x\"):\n",
    "                                for i0_i1_fused_1_2_init in range(T.int64(processed_rows_per_thread)):\n",
    "                                    for i2_2_init in T.vectorized(T.int64(vectorize_factor)):\n",
    "                                        with T.block(\"NT_matmul_init\"):\n",
    "                                            v_i0 = T.axis.spatial(T.int64(1), T.int64(0))\n",
    "                                            v_i1 = T.axis.spatial(\n",
    "                                                (n + T.int64(31)) // T.int64(32) * T.int64(32),\n",
    "                                                i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32)\n",
    "                                                + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                                + i0_i1_fused_1_2_init,\n",
    "                                            )\n",
    "                                            v_i2 = T.axis.spatial(\n",
    "                                                T.int64(4096),\n",
    "                                                i2_0 * (ThreadIdx_x * vectorize_factor) + i2_1 * vectorize_factor + i2_2_init,\n",
    "                                            )\n",
    "                                            T.reads()\n",
    "                                            T.writes(\n",
    "                                                var_NT_matmul_intermediate_pad_local[\n",
    "                                                    v_i0, v_i1, v_i2\n",
    "                                                ]\n",
    "                                            )\n",
    "                                            var_NT_matmul_intermediate_pad_local[\n",
    "                                                v_i0, v_i1, v_i2\n",
    "                                            ] = T.float16(0)\n",
    "                                for k_0 in range(T.int64(344)):\n",
    "                                    for ax0 in range(T.int64(1)):\n",
    "                                        for ax1 in T.vectorized(T.int64(vectorize_factor)):\n",
    "                                            with T.block(\"lv51_local\"):\n",
    "                                                v0 = T.axis.spatial(T.int64(344), k_0 + ax0)\n",
    "                                                v1 = T.axis.spatial(\n",
    "                                                    T.int64(4096),\n",
    "                                                    i2_0 * (ThreadIdx_x * vectorize_factor) + i2_1 * vectorize_factor + ax1,\n",
    "                                                )\n",
    "                                                T.reads(lv51[v0, v1])\n",
    "                                                T.writes(lv51_local[v0, v1])\n",
    "                                                lv51_local[v0, v1] = lv51[v0, v1]\n",
    "                                    for k_1 in range(T.int64(4)):\n",
    "                                        for ax0 in range(T.int64(1)):\n",
    "                                            for ax1 in T.vectorized(vectorize_factor):\n",
    "                                                with T.block(\"lv50_local\"):\n",
    "                                                    v0 = T.axis.spatial(\n",
    "                                                        T.int64(1376), k_0 * T.int64(4) + k_1 + ax0\n",
    "                                                    )\n",
    "                                                    v1 = T.axis.spatial(\n",
    "                                                        T.int64(4096),\n",
    "                                                        i2_0 * (ThreadIdx_x * vectorize_factor)\n",
    "                                                        + i2_1 * vectorize_factor\n",
    "                                                        + ax1,\n",
    "                                                    )\n",
    "                                                    T.reads(lv50[v0, v1])\n",
    "                                                    T.writes(lv50_local[v0, v1])\n",
    "                                                    lv50_local[v0, v1] = lv50[v0, v1]\n",
    "                                        for k_2 in range(T.int64(8)):\n",
    "                                            for ax0 in range(T.int64(1)):\n",
    "                                                for ax1 in T.vectorized(vectorize_factor):\n",
    "                                                    with T.block(\"decode\"):\n",
    "                                                        v_i = T.axis.spatial(\n",
    "                                                            T.int64(11008),\n",
    "                                                            k_0 * T.int64(32)\n",
    "                                                            + k_1 * T.int64(8)\n",
    "                                                            + k_2\n",
    "                                                            + ax0,\n",
    "                                                        )\n",
    "                                                        v_j = T.axis.spatial(\n",
    "                                                            T.int64(4096),\n",
    "                                                            i2_0 * (ThreadIdx_x * vectorize_factor)\n",
    "                                                            + i2_1 * vectorize_factor\n",
    "                                                            + ax1,\n",
    "                                                        )\n",
    "                                                        T.reads(\n",
    "                                                            lv50_local[v_i // T.int64(8), v_j],\n",
    "                                                            lv51_local[v_i // T.int64(32), v_j],\n",
    "                                                        )\n",
    "                                                        T.writes(decode_local[v_i, v_j])\n",
    "                                                        decode_local[v_i, v_j] = (\n",
    "                                                            T.Cast(\n",
    "                                                                \"float16\",\n",
    "                                                                T.bitwise_and(\n",
    "                                                                    T.shift_right(\n",
    "                                                                        lv50_local[\n",
    "                                                                            v_i // T.int64(8), v_j\n",
    "                                                                        ],\n",
    "                                                                        T.Cast(\n",
    "                                                                            \"uint32\",\n",
    "                                                                            v_i % T.int64(8),\n",
    "                                                                        )\n",
    "                                                                        * T.uint32(4),\n",
    "                                                                    ),\n",
    "                                                                    T.uint32(15),\n",
    "                                                                ),\n",
    "                                                            )\n",
    "                                                            - T.float16(7)\n",
    "                                                        ) * lv51_local[v_i // T.int64(32), v_j]\n",
    "                                            for ax0, ax1 in T.grid(T.int64(1), processed_rows_per_thread):\n",
    "                                                for ax2 in T.vectorized(T.int64(1)):\n",
    "                                                    with T.block(\"lv5_pad_local\"):\n",
    "                                                        v0 = T.axis.spatial(T.int64(1), ax0)\n",
    "                                                        v1 = T.axis.spatial(\n",
    "                                                            (n + T.int64(31))\n",
    "                                                            // T.int64(32)\n",
    "                                                            * T.int64(32),\n",
    "                                                            i0_i1_fused_0_i0_i1_fused_1_0_fused\n",
    "                                                            * T.int64(32)\n",
    "                                                            + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                                            + ax1,\n",
    "                                                        )\n",
    "                                                        v2 = T.axis.spatial(\n",
    "                                                            T.int64(11008),\n",
    "                                                            k_0 * T.int64(32)\n",
    "                                                            + k_1 * T.int64(8)\n",
    "                                                            + k_2\n",
    "                                                            + ax2,\n",
    "                                                        )\n",
    "                                                        T.reads(lv5[v0, v1, v2])\n",
    "                                                        T.writes(lv5_pad_local[v0, v1, v2])\n",
    "                                                        lv5_pad_local[v0, v1, v2] = T.if_then_else(\n",
    "                                                            v1 < n, lv5[v0, v1, v2], T.float16(0)\n",
    "                                                        )\n",
    "                                            for i0_i1_fused_1_2 in range(processed_rows_per_thread):\n",
    "                                                for i2_2 in T.vectorized(vectorize_factor):\n",
    "                                                    with T.block(\"NT_matmul_update\"):\n",
    "                                                        v_i0 = T.axis.spatial(\n",
    "                                                            T.int64(1), T.int64(0)\n",
    "                                                        )\n",
    "                                                        v_i1 = T.axis.spatial(\n",
    "                                                            (n + T.int64(31))\n",
    "                                                            // T.int64(32)\n",
    "                                                            * T.int64(32),\n",
    "                                                            i0_i1_fused_0_i0_i1_fused_1_0_fused\n",
    "                                                            * T.int64(32)\n",
    "                                                            + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                                            + i0_i1_fused_1_2,\n",
    "                                                        )\n",
    "                                                        v_i2 = T.axis.spatial(\n",
    "                                                            T.int64(4096),\n",
    "                                                            i2_0 * (ThreadIdx_x * vectorize_factor)\n",
    "                                                            + i2_1 * vectorize_factor\n",
    "                                                            + i2_2,\n",
    "                                                        )\n",
    "                                                        v_k = T.axis.reduce(\n",
    "                                                            T.int64(11008),\n",
    "                                                            k_0 * T.int64(32)\n",
    "                                                            + k_1 * T.int64(8)\n",
    "                                                            + k_2,\n",
    "                                                        )\n",
    "                                                        T.reads(\n",
    "                                                            var_NT_matmul_intermediate_pad_local[\n",
    "                                                                v_i0, v_i1, v_i2\n",
    "                                                            ],\n",
    "                                                            lv5_pad_local[v_i0, v_i1, v_k],\n",
    "                                                            decode_local[v_k, v_i2],\n",
    "                                                        )\n",
    "                                                        T.writes(\n",
    "                                                            var_NT_matmul_intermediate_pad_local[\n",
    "                                                                v_i0, v_i1, v_i2\n",
    "                                                            ]\n",
    "                                                        )\n",
    "                                                        var_NT_matmul_intermediate_pad_local[\n",
    "                                                            v_i0, v_i1, v_i2\n",
    "                                                        ] = (\n",
    "                                                            var_NT_matmul_intermediate_pad_local[\n",
    "                                                                v_i0, v_i1, v_i2\n",
    "                                                            ]\n",
    "                                                            + lv5_pad_local[v_i0, v_i1, v_k]\n",
    "                                                            * decode_local[v_k, v_i2]\n",
    "                                                        )\n",
    "                                for ax0, ax1 in T.grid(T.int64(1), processed_rows_per_thread):\n",
    "                                    for ax2 in T.vectorized(vectorize_factor):\n",
    "                                        with T.block(\"var_NT_matmul_intermediate_pad_local\"):\n",
    "                                            v0 = T.axis.spatial(T.int64(1), ax0)\n",
    "                                            v1 = T.axis.spatial(\n",
    "                                                (n + T.int64(31)) // T.int64(32) * T.int64(32),\n",
    "                                                i0_i1_fused_0_i0_i1_fused_1_0_fused * T.int64(32)\n",
    "                                                + i0_i1_fused_1_1 * processed_rows_per_thread\n",
    "                                                + ax1,\n",
    "                                            )\n",
    "                                            v2 = T.axis.spatial(\n",
    "                                                T.int64(4096),\n",
    "                                                i2_0 * (ThreadIdx_x * vectorize_factor) + i2_1 * vectorize_factor + ax2,\n",
    "                                            )\n",
    "                                            T.reads(\n",
    "                                                lv3[v0, v1, v2],\n",
    "                                                var_NT_matmul_intermediate_pad_local[v0, v1, v2],\n",
    "                                            )\n",
    "                                            T.writes(p_output0_intermediate[v0, v1, v2])\n",
    "                                            if v1 < n:\n",
    "                                                p_output0_intermediate[v0, v1, v2] = (\n",
    "                                                    lv3[v0, v1, v2]\n",
    "                                                    + var_NT_matmul_intermediate_pad_local[\n",
    "                                                        v0, v1, v2\n",
    "                                                    ]\n",
    "                                                )\n",
    "        return tvm.tir.Schedule(ModuleToManual).mod\n",
    "\n",
    "    BlockIdx_x = [None] # 32\n",
    "    # n = 32\n",
    "    # BlockIdx_y = (n+31)//32 * 32 # 这里32是假设输入为32的倍数, //32的32 = thready * \n",
    "    ThreadIdx_x = [16, 32, 48, 64, 96, 128, 192, 256, 384, 512]#16 * 3\n",
    "    # ThreadIdx_y = 8 # = 32 / processed_rows_per_thread\n",
    "    vectorize_factor = [2, 4, 8]# 8\n",
    "    # processed_columns_per_thread = vectorize_factor# w_y / (BlockIdx_x * ThreadIdx_x) == vectorize_factor\n",
    "    processed_rows_per_thread = [1, 2, 4, 8, 16]#4\n",
    "    task_index = 0\n",
    "    total_task_num = len(vectorize_factor)*len(BlockIdx_x)*len(ThreadIdx_x)*len(processed_rows_per_thread)\n",
    "    records = {}\n",
    "    print(f\"Total tasks: {total_task_num}\")\n",
    "    # try:\n",
    "    vectorize_factor_r = vectorize_factor[::-1]\n",
    "    processed_rows_per_thread_r = processed_rows_per_thread[::-1]\n",
    "    ThreadIdx_x_r = ThreadIdx_x[::-1]\n",
    "    # table\n",
    "    write_interval = 5\n",
    "    from prettytable import PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"vectorize_factor\", \"processed_rows_per_thread\", \"blockIdx.x\", \"threadIdx.x\", \"threadIdx.y\", \"cost(ms)\"]\n",
    "    for vf in vectorize_factor_r:\n",
    "        for pr in processed_rows_per_thread_r:\n",
    "            for tx in ThreadIdx_x_r:\n",
    "                task_index = task_index + 1\n",
    "                import math\n",
    "                bx = math.ceil(w_y /(vf * tx))\n",
    "                ty = math.ceil(32//pr)\n",
    "                if tx * vf >= w_y or tx*ty > 1024 or 32 % pr != 0: # w_y为输出列数, 工作组和vectorize相乘不能大于该数字\n",
    "                    print(f\"search record [{task_index}/{total_task_num}]: skip {vf} {pr} {bx} {tx} {ty}\")\n",
    "                    continue\n",
    "                if w_y % (vf * tx) != 0:\n",
    "                    print(f\"search record [{task_index}/{total_task_num}]: skip because bx isn't divisible {vf} {pr} {bx} {tx} {ty}\")\n",
    "                    continue\n",
    "                print(f\"search record [{task_index}/{total_task_num}]: start run {vf} {pr} {bx} {tx} {ty}\")\n",
    "                # vf: int, pr: int, bx: int, tx: int, ty: int):\n",
    "                mod_deploy = search(vf, pr, bx, tx, ty)\n",
    "                cost = test_opencl(mod_deploy, \"search\")\n",
    "                print(\"=====\")\n",
    "                records[(vf, pr, bx, tx, ty)] = cost\n",
    "                table.add_row([vf, pr, bx, tx, ty, cost])\n",
    "                if task_index % write_interval == 0:\n",
    "                    with open(record_file, 'wt') as f:\n",
    "                        f.write(table.get_csv_string())\n",
    "    # except Exception as e:\n",
    "    #     print(f\"error occured: {e}\")\n",
    "    ### write file\n",
    "    # from prettytable import PrettyTable\n",
    "    # table = PrettyTable()\n",
    "    # table.field_names = [\"vectorize_factor\", \"processed_rows_per_thread\", \"blockIdx.x\", \"threadIdx.x\", \"threadIdx.y\", \"cost(ms)\"]\n",
    "    # for config, cost in records.items():\n",
    "    #     table.add_row([config[0], config[1], config[2], config[3], config[4], cost])\n",
    "    #     print(f\"{config}: {cost}ms\")\n",
    "    print(\"================================\")\n",
    "    print(table)\n",
    "    with open(record_file, 'wt') as f:\n",
    "        f.write(table.get_csv_string())\n",
    "    \n",
    "    # record_sorted = sorted(record.items(), key=lambda x: x[1][0], reverse=True)\n",
    "auto_tune(\"./manual_tune/down_n_tune_record_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.158.176.30\"\n",
    "rpc_port = 5001\n",
    "# remote = autotvm.measure.request_remote(device_key, \"10.158.176.30\", 5001, timeout=10000)\n",
    "# dev = remote.device(str(target), 0)\n",
    "\n",
    "# num_flop = 1228406784\n",
    "# W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "# S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "# Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# # Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# W_nd = tvm.nd.array(W_np, dev)\n",
    "# S_nd = tvm.nd.array(S_np, dev)\n",
    "# Input_nd = tvm.nd.array(Input_np, dev)\n",
    "# Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc_config = ms.runner.RPCConfig(tracker_host=rpc_host, tracker_port=rpc_port, tracker_key = device_key)\n",
    "runner= ms.runner.RPCRunner(rpc_config)\n",
    "# ms.builder.LocalBuilder()\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "database = ms.tune_tir(\n",
    "    mod=ModuleSrc,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_first\",\n",
    "    cost_model=\"xgb\",\n",
    "    runner = runner\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, sch.mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.script import relax as R\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def main(A: R.Tensor((3, 4), dtype=\"float32\"), B: R.Tensor((4, 5), dtype=\"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((3, 5), dtype=\"float32\") = R.matmul(A, B)\n",
    "            gv: R.Tensor((3, 5), dtype=\"float32\") = lv\n",
    "            R.output(gv)\n",
    "        return gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto_scheduler test\n",
    "from tvm import auto_scheduler\n",
    "import numpy as np\n",
    "a_np = np.random.rand(3, 4).astype(\"float32\")\n",
    "b_np = np.random.rand(4, 5).astype(\"float32\")\n",
    "a_nd = tvm.runtime.NDArray(a_np)\n",
    "b_nd = tvm.runtime.NDArray(b_np)\n",
    "sch = tvm.tir.Schedule(Module)\n",
    "\n",
    "params = {\"A\": a_np, \"B\": b_np}\n",
    "## 报错，这里只支持relay\n",
    "# tasks = auto_scheduler.extract_tasks(sch.mod, params, target=target)\n",
    "tasks = ms.relax_integration.extract_tasks(sch.mod, target=target, params=params)\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mod_deploy import Module as ModuleAll\n",
    "params_all = {}\n",
    "tasks_all = auto_scheduler.extract_tasks(ModuleAll, params_all, target=target)\n",
    "print(len(tasks_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_file = \"tune.json\"\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "W_nd = tvm.nd.array(W_np, dev)\n",
    "S_nd = tvm.nd.array(S_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "new_mod = sch.mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = auto_scheduler.SearchTask(func=sch.mod['fused_fused_decode11_fused_matmul5_cast2'], args=sch.mod['fused_fused_decode11_fused_matmul5_cast2'].params, target=target)\n",
    "\n",
    "# tune_option = auto_scheduler.TuningOptions(\n",
    "#     num_measure_trials=10,\n",
    "#     measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "\n",
    "database = ms.tune_tir(\n",
    "    mod=new_mod,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_45593_1\",\n",
    "    cost_model=\"xgb\"\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, new_mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sch1.trace)\n",
    "# print(sch1.mod.script())\n",
    "rt_mod = tvm.build(sch1.mod, target=\"cuda\")\n",
    "\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (1228406784 / evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_database = ms.Database.create(kind='json', work_dir='./tune_45593_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sch = ms.tir_integration.compile_tir(record_database, new_mod, target)\n",
    "\n",
    "record_rt_mod = tvm.build(record_sch.mod, target=\"cuda\")\n",
    "\n",
    "record_evaluator = record_rt_mod.time_evaluator(\"main\", dev, number=20)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / record_evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(record_sch.trace)\n",
    "print(record_sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Dict, List, Optional, Union, Callable\n",
    "from tvm import runtime\n",
    "if TYPE_CHECKING:\n",
    "    import numpy as np  # type: ignore\n",
    "    from tvm.ir import IRModule\n",
    "    from tvm.meta_schedule.runner import EvaluatorConfig, RPCConfig\n",
    "    from tvm.runtime import Device, Module, NDArray\n",
    "    from tvm.target import Target\n",
    "    from tvm.runtime.vm import Executable\n",
    "\n",
    "\n",
    "def f_measurement(\n",
    "    rt_mod: runtime.Module, device: runtime.ndarray.Device, input_data: Dict[str, runtime.NDArray]\n",
    "):\n",
    "    vm = relax.VirtualMachine(rt_mod, device=device)\n",
    "    vm.save_function(\"main\", \"measure_func\", **input_data, include_return=False)\n",
    "    evaluator = vm.time_evaluator(\n",
    "        func_name=\"measure_func\",\n",
    "        dev=device,\n",
    "        repeat=100,\n",
    "        number=1,\n",
    "        min_repeat_ms=500,\n",
    "    )\n",
    "    return evaluator()\n",
    "\n",
    "def run_module_via_rpc(\n",
    "    rpc_config: \"RPCConfig\",\n",
    "    lib: Union[\"Module\", \"Executable\"],\n",
    "    dev_type: str,\n",
    "    args: Union[Dict[int, \"np.ndarray\"], Dict[str, \"np.ndarray\"]],\n",
    "    continuation: Callable,\n",
    "    backend: Optional[str] = \"graph\",\n",
    "):\n",
    "    \"\"\"Execute a tvm.runtime.Module on RPC remote\"\"\"\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    from tvm.contrib.tar import tar\n",
    "    from tvm.runtime import ndarray\n",
    "\n",
    "    # pylint: enable=import-outside-toplevel\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + tar.output_format)\n",
    "        filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + \"so\")\n",
    "        if backend == \"vm\":\n",
    "            code, lib = lib.save(filename, fmt=\"so\")\n",
    "        from tvm.contrib import ndk\n",
    "        lib.export_library(filename, ndk.create_shared)\n",
    "        session = rpc_config.connect_server()\n",
    "        print(type(session._sess))\n",
    "        session.upload(filename)\n",
    "        _, filename = os.path.split(filename)\n",
    "        rt_mod = session.load_module(filename)\n",
    "        \n",
    "        if backend == \"vm\":\n",
    "            rt_mod = session.get_function(\"runtime.Load_Executable\")(code, rt_mod)\n",
    "            # rt_mod = session.get_function(\"runtime.module.loadfile_relax.Executable\")(filename)\n",
    "        dev = session.device(dev_type=dev_type, dev_id=0)\n",
    "        # print(dev)\n",
    "        # create the remote runtime module\n",
    "        print(rt_mod)\n",
    "        print(rt_mod['main'])\n",
    "        from tvm.contrib import graph_executor as runtime\n",
    "        module = runtime.GraphModule(rt_mod[\"main\"](dev))\n",
    "        print(module)\n",
    "        for k, v in args.items():\n",
    "            module.set_input(k, tvm.nd.array(v))\n",
    "        return module.run()\n",
    "        # nd_args = {k: ndarray.array(v, dev) for k, v in args.items()}\n",
    "        nd_args = {k: ndarray.empty(v.shape, v.dtype, dev) for k, v in args.items()}\n",
    "        return continuation(rt_mod, dev, nd_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
