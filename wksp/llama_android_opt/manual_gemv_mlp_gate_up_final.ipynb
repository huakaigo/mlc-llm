{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tvm\n",
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm import autotvm, auto_scheduler\n",
    "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
    "from tvm import meta_schedule as ms\n",
    "from tvm.ir import IRModule\n",
    "from tvm import relax\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils, ndk\n",
    "x_shape = 4096\n",
    "w_w_x = 512\n",
    "w_s_x = 128\n",
    "w_y = 11008*2\n",
    "func_name = \"main\"\n",
    "@I.ir_module\n",
    "class ModuleSrc:\n",
    "    @T.prim_func\n",
    "    def main(lv571: T.Buffer((T.int64(512), T.int64(w_y)), \"uint32\"), lv572: T.Buffer((T.int64(128), T.int64(w_y)), \"float16\"), lv1654: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"), var_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(w_y)), \"float16\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        p_output0_intermediate = T.alloc_buffer((T.int64(4096), T.int64(w_y)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(w_y)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv571[v_i // T.int64(8), v_j], lv572[v_i // T.int64(32), v_j])\n",
    "                T.writes(p_output0_intermediate[v_i, v_j])\n",
    "                p_output0_intermediate[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv571[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv572[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(w_y), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv1654[v_i0, v_i1, v_k], p_output0_intermediate[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv1654[v_i0, v_i1, v_k] * p_output0_intermediate[v_k, v_i2]\n",
    "\n",
    "@I.ir_module\n",
    "class ModuleToManual:\n",
    "    @T.prim_func\n",
    "    def main(lv571: T.Buffer((T.int64(512), T.int64(w_y)), \"uint32\"), lv572: T.Buffer((T.int64(128), T.int64(w_y)), \"float16\"), lv1654: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"), var_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(w_y)), \"float16\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        p_output0_intermediate = T.alloc_buffer((T.int64(4096), T.int64(w_y)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(w_y)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv571[v_i // T.int64(8), v_j], lv572[v_i // T.int64(32), v_j])\n",
    "                T.writes(p_output0_intermediate[v_i, v_j])\n",
    "                p_output0_intermediate[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv571[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv572[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(w_y), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv1654[v_i0, v_i1, v_k], p_output0_intermediate[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv1654[v_i0, v_i1, v_k] * p_output0_intermediate[v_k, v_i2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "// Function: main_kernel\n",
      "#ifdef cl_khr_fp16\n",
      "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
      "#elif defined(cl_amd_fp16)\n",
      "#pragma OPENCL EXTENSION cl_amd_fp16 : enable\n",
      "#else\n",
      "#error \"Half precision floating point not supported by OpenCL implementation on your device.\" \n",
      "#endif\n",
      "\n",
      "__kernel void main_kernel(__global half* restrict lv1654, __global uint* restrict lv571, __global half* restrict lv572, __global half* restrict var_matmul_intermediate) {\n",
      "  __local half lv1654_shared[4096];\n",
      "  half4 var_matmul_intermediate_local[1];\n",
      "  half4 lv572_local[1];\n",
      "  uint4 lv571_local[1];\n",
      "  for (int ax2_0 = 0; ax2_0 < 4; ++ax2_0) {\n",
      "    vstore8(vload8(0, lv1654 + ((ax2_0 * 1024) + ((convert_int(get_local_id(0))) * 8))), 0, lv1654_shared + ((ax2_0 * 1024) + ((convert_int(get_local_id(0))) * 8)));\n",
      "  }\n",
      "  var_matmul_intermediate_local[0] = ((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f));\n",
      "  barrier(CLK_LOCAL_MEM_FENCE);\n",
      "  for (int k_0 = 0; k_0 < 128; ++k_0) {\n",
      "    lv572_local[0] = vload4(0, lv572 + (((k_0 * 22016) + ((convert_int(get_group_id(0))) * 512)) + ((convert_int(get_local_id(0))) * 4)));\n",
      "    for (int k_1 = 0; k_1 < 4; ++k_1) {\n",
      "      lv571_local[0] = vload4(0, lv571 + ((((k_0 * 88064) + (k_1 * 22016)) + ((convert_int(get_group_id(0))) * 512)) + ((convert_int(get_local_id(0))) * 4)));\n",
      "      for (int k_2 = 0; k_2 < 8; ++k_2) {\n",
      "        var_matmul_intermediate_local[0] = (var_matmul_intermediate_local[0] + (((half4)(lv1654_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1654_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1654_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1654_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)])) * (((convert_half4(((lv571_local[0]  >>  ((uint4)(((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4))))  &  ((uint4)((uint)15, (uint)15, (uint)15, (uint)15))))) - ((half4)((half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f))) * lv572_local[0])));\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  vstore4(var_matmul_intermediate_local[0], 0, var_matmul_intermediate + (((convert_int(get_group_id(0))) * 512) + ((convert_int(get_local_id(0))) * 4)));\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ref to mlc-llm/dispatch/dispatch_tir_operator_adreno.py\n",
    "# 最优kernel: 0.962456ms\n",
    "# vf vi\ttx\tbx\n",
    "# 4\t 4\t128\t43\n",
    "\n",
    "def sch_fused_decode5_fused_matmul6_silu1(func):\n",
    "    sch = tvm.tir.Schedule(func)\n",
    "    b0 = sch.get_block(name=\"decode\", func_name=\"main\")\n",
    "    b1 = sch.get_block(name=\"matmul\", func_name=\"main\")\n",
    "    l2, l3, l4, l5 = sch.get_loops(block=b1)\n",
    "    l6 = sch.fuse(l2, l3, l4, preserve_unit_iters=True)\n",
    "    l10, l11, l12 = sch.split(loop=l6, factors=[43, 128, 4], preserve_unit_iters=True)\n",
    "    v13, v14, v15 = sch.sample_perfect_tile(\n",
    "        loop=l5, n=3, max_innermost_factor=8, decision=[128, 4, 8]\n",
    "    )\n",
    "    l16, l17, l18 = sch.split(\n",
    "        loop=l5, factors=[v13, v14, v15], preserve_unit_iters=True\n",
    "    )\n",
    "    sch.reorder(l10, l11, l16, l17, l18, l12)\n",
    "    sch.bind(loop=l10, thread_axis=\"blockIdx.x\")\n",
    "    sch.bind(loop=l11, thread_axis=\"threadIdx.x\")\n",
    "    sch.compute_inline(block=b0)\n",
    "    b19 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")\n",
    "    sch.reverse_compute_at(block=b19, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    b20 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"local\")\n",
    "    b21 = sch.cache_read(block=b1, read_buffer_index=2, storage_scope=\"local\")\n",
    "    b22 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")\n",
    "    sch.compute_at(block=b22, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    v23 = sch.sample_categorical(\n",
    "        candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=3\n",
    "    )\n",
    "    sch.annotate(\n",
    "        block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v23\n",
    "    )\n",
    "    sch.compute_at(block=b20, loop=l17, preserve_unit_loops=True, index=-1)\n",
    "    sch.compute_at(block=b21, loop=l16, preserve_unit_loops=True, index=-1)\n",
    "    l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)\n",
    "    sch.vectorize(loop=l29)\n",
    "    l30, l31, l32, l33, l34 = sch.get_loops(block=b21)\n",
    "    sch.vectorize(loop=l34)\n",
    "    l35, l36, l37, l38, l39 = sch.get_loops(block=b19)\n",
    "    sch.vectorize(loop=l39)\n",
    "    sch.vectorize(loop=l12)\n",
    "    b40 = sch.decompose_reduction(block=b1, loop=l16)\n",
    "    sch.enter_postproc()\n",
    "    sch.unannotate(block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\")\n",
    "    l43, l44, l45, l46, l47 = sch.get_loops(block=b22)\n",
    "    l48, l49, l50 = sch.split(loop=l47, factors=[None, 128, 4], preserve_unit_iters=True)\n",
    "    sch.vectorize(loop=l50)\n",
    "    sch.bind(loop=l49, thread_axis=\"threadIdx.x\")\n",
    "    return sch.mod[\"main\"].with_attr(\"tir.is_scheduled\", 1)\n",
    "\n",
    "\n",
    "sch_manual = tvm.tir.Schedule(ModuleToManual)\n",
    "# sch_fused_decode5_fused_matmul6_silu1(sch_manual.mod[func_name])\n",
    "sch_manual.mod['main'] = sch_fused_decode5_fused_matmul6_silu1(sch_manual.mod[func_name])\n",
    "# print(sch_manual.mod.script())\n",
    "print(\"================================================\")\n",
    "rt_mod = tvm.build(sch_manual.mod, target=\"opencl\")\n",
    "print(rt_mod.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda -keys=cuda,gpu -arch=sm_61 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n",
      "[[[-7156. -6716. -6644. -6736. -6696. -6532. -6776. -5808. -6616. -6256.]]]\n",
      "[[[-7156. -6716. -6644. -6736. -6696. -6532. -6776. -5808. -6616. -6256.]]]\n"
     ]
    }
   ],
   "source": [
    "# run and compare with cuda\n",
    "import numpy as np\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_w_np = np.random.uniform(size=(w_w_x, w_y)).astype(\"uint32\")\n",
    "W_s_np = np.random.uniform(size=(w_s_x, w_y)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, x_shape)).astype(\"float16\")\n",
    "# W_w_np = np.ones((w_w_x, w_y), np.uint32) * 1#.astype(\"uint32\")\n",
    "# W_s_np = np.ones((w_s_x, w_y), np.float16) * 1#.astype(\"float16\") * 2\n",
    "# Input_np = np.ones((1, 1, x_shape), np.float16)#.astype(\"float16\")\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "def numpy_caculate():\n",
    "    test_cols = 10\n",
    "    output = np.zeros((1, 1, test_cols), dtype = np.float16)\n",
    "    W_w_inv_np = np.transpose(W_w_np)\n",
    "    W_s_inv_np = np.transpose(W_s_np)\n",
    "    for i in range(test_cols):\n",
    "        for r in range(x_shape):\n",
    "            temp = Input_np[0][0][r] * np.float16((W_w_inv_np[i][r // 8] >> ((r % 8) * 4) & (15)) - np.float16(7.0)) * W_s_inv_np[i][r // 32]\n",
    "            output[0][0][i] = output[0][0][i] + temp\n",
    "    print(output)\n",
    "    output = np.zeros((1, 1, test_cols), dtype = np.float16)\n",
    "    for i in range(test_cols):\n",
    "        for r in range(x_shape):\n",
    "            temp = Input_np[0][0][r] * np.float16((W_w_np[r // 8][i] >> ((r % 8) * 4) & (15)) - np.float16(7.0)) * W_s_np[r // 32][i]\n",
    "            temp_output = output[0][0][i]\n",
    "            output[0][0][i] = temp_output + temp\n",
    "            # print(f\"{temp_output} + {temp} = {output[0][0][i]}\")\n",
    "    print(output)\n",
    "numpy_caculate()\n",
    "def print_npdata(np_data: np.ndarray) :\n",
    "    print(np_data)\n",
    "    print_num = 20\n",
    "    d = np_data.flatten()\n",
    "    p_size = print_num if d.size > print_num else d.size\n",
    "    print(d[:p_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_evaluator GEMV-Blocking: 65.352494 GFLOPS\n",
      "[-5976. -6344. -7012. -5560. -6352. -5812. -7388. -6224. -6852. -6048.]\n"
     ]
    }
   ],
   "source": [
    "# cuda未优化版本测试\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "rt_mod = tvm.build(src_gpu_mod, target=\"cuda\")\n",
    "W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "print(\"manual_evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / evaluator(W_w_nd, W_s_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "# print(Output_nd.numpy())\n",
    "print_npdata(Output_nd.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TVM_NDK_CC\"]=\"/home/sensetime/Android/Sdk/ndk/25.2.9519653/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android33-clang++\"\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.4.236.32\"\n",
    "rpc_port = 9190\n",
    "comp_target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-android\")  # TODO: Only support arm64 for now\n",
    "\n",
    "def test_opencl(mod: tvm.IRModule, name_hint: str):\n",
    "    # mod = tvm.lower(sch_manual.mod)\n",
    "    print(\"Build ...\")\n",
    "    android_rt_mod = tvm.build(mod, target=\"opencl\", target_host=\"llvm -mtriple=aarch64-linux-android\")\n",
    "    # print(android_rt_mod.imported_modules[0].get_source())\n",
    "    temp = utils.tempdir()\n",
    "    path_dso_cl = temp.relpath(\"dev_lib_cl.so\")\n",
    "    android_rt_mod.export_library(path_dso_cl, ndk.create_shared)\n",
    "\n",
    "    print(\"Run GPU(OpenCL Flavor) test ...\")\n",
    "    # Establish remote connection with target hardware\n",
    "\n",
    "    tracker = rpc.connect_tracker(rpc_host, rpc_port)\n",
    "    remote = tracker.request(device_key, priority=0, session_timeout=60)\n",
    "    print(\"Connect to device done.\")\n",
    "    dev = remote.cl(0)\n",
    "    remote.upload(path_dso_cl)\n",
    "    f1 = remote.load_module(\"dev_lib_cl.so\")\n",
    "\n",
    "    W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "    W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "    Input_nd = tvm.nd.array(Input_np, dev)\n",
    "    Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "    test_number=32\n",
    "    time_f = f1.time_evaluator(f1.entry_name, dev, number=test_number)\n",
    "    cost = time_f(W_w_nd, W_s_nd, Input_nd, Output_nd).mean\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %fms with loop %d\" % (name_hint, cost * 1000, test_number))\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %fGFLOPS\" % (name_hint, num_flop / cost / 1e9))\n",
    "    print(Output_nd.numpy())\n",
    "    return cost * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/driver/build_module.py:264: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connect to device done.\n",
      "evaluator[source] GEMV-Blocking: 4.269120 ms with loop 32\n",
      "evaluator[source] GEMV-Blocking: 287.742388 GFLOPS\n",
      "[-6600. -6816. -7668. -6056. -6908. -6464. -7976. -6960. -7600. -6672.]\n"
     ]
    }
   ],
   "source": [
    "# 未优化版本opencl测试\n",
    "from tvm import dlight as dl\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    # src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "    mod_deploy = dl.ApplyDefaultSchedule(  # pylint: disable=not-callable\n",
    "        dl.gpu.Matmul(),\n",
    "        dl.gpu.GEMV(),\n",
    "        dl.gpu.Reduction(),\n",
    "        dl.gpu.GeneralReduction(),\n",
    "        dl.gpu.Fallback(),\n",
    "    )(sch.mod)\n",
    "src_output = test_opencl(mod_deploy, \"source\")\n",
    "print_npdata(src_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/driver/build_module.py:264: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[opted] GEMV-Blocking: 1.050112ms with loop 32\n",
      "evaluator[opted] GEMV-Blocking: 1169.786446GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "1.050112\n",
      "[1.050112]\n"
     ]
    }
   ],
   "source": [
    "#优化版本opencl测试\n",
    "# print(sch_manual.mod)\n",
    "opt_output = test_opencl(sch_manual.mod, \"opted\")\n",
    "print_npdata(opt_output)\n",
    "# np.testing.assert_equal(opt_output, src_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tasks: 108\n",
      "search record [1/108]: start run 2 2 8 1376.0\n",
      "Build ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/llm/github/new_wksp/tvm-unity/python/tvm/driver/build_module.py:264: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 11.036448ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 111.304541GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [2/108]: start run 2 2 16 688.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 5.313416ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 231.189650GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [3/108]: start run 2 2 32 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.823272ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 435.100403GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [4/108]: start run 2 2 43 256.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.310936ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 531.562442GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [5/108]: start run 2 2 64 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.584512ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 775.258745GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [6/108]: start run 2 2 86 128.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.493392ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 822.561514GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [7/108]: start run 2 2 128 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.170536ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1049.439559GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [8/108]: start run 2 2 172 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.283008ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 957.442809GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [9/108]: start run 2 2 256 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.092064ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1124.848712GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [10/108]: start run 2 2 344 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.123840ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1093.044191GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [11/108]: start run 2 2 512 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.057152ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 597.139533GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [12/108]: start run 2 2 688 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.367848ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 898.057960GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [13/108]: start run 2 4 8 1376.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 10.136144ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 121.190739GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [14/108]: start run 2 4 16 688.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 5.051808ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 243.161811GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [15/108]: start run 2 4 32 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.745904ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 447.359698GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [16/108]: start run 2 4 43 256.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.283128ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 538.036757GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [17/108]: start run 2 4 64 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.567584ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 783.630596GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [18/108]: start run 2 4 86 128.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.476112ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 832.190771GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [19/108]: start run 2 4 128 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.155280ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1063.297888GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [20/108]: start run 2 4 172 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.282984ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 957.460720GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [21/108]: start run 2 4 256 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.089192ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1127.814732GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [22/108]: start run 2 4 344 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.130824ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1086.293520GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [23/108]: start run 2 4 512 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.014184ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 609.878136GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [24/108]: start run 2 4 688 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.369992ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 896.652524GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [25/108]: start run 2 8 8 1376.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 9.727176ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 126.286065GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [26/108]: start run 2 8 16 688.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 4.937312ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 248.800721GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [27/108]: start run 2 8 32 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.712968ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 452.790738GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [28/108]: start run 2 8 43 256.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.232616ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 550.209612GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [29/108]: start run 2 8 64 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.573552ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 780.658525GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [30/108]: start run 2 8 86 128.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.442368ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 851.659760GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [31/108]: start run 2 8 128 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.211760ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1013.737691GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [32/108]: start run 2 8 172 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.268064ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 968.726172GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [33/108]: start run 2 8 256 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.095784ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1121.030042GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [34/108]: start run 2 8 344 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.250776ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 982.115730GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [35/108]: start run 2 8 512 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.039624ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 602.271195GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [36/108]: start run 2 8 688 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.567008ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 783.918642GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [37/108]: start run 4 2 8 688.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 6.687960ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 183.674362GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [38/108]: start run 4 2 16 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 3.354768ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 366.167432GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [39/108]: start run 4 2 32 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.815472ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 676.632184GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [40/108]: start run 4 2 43 128.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.506040ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 815.653491GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [41/108]: start run 4 2 64 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.108888ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1107.782557GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [42/108]: start run 4 2 86 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.204704ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1019.675193GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [43/108]: start run 4 2 128 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 0.965120ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1272.802122GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [44/108]: start run 4 2 172 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.070088ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1147.949313GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [45/108]: start run 4 2 256 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.879272ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 653.660984GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [46/108]: start run 4 2 344 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.135504ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1081.816342GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [47/108]: start run 4 2 512 10.75\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.884896ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 651.710643GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [48/108]: start run 4 2 688 8.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.060416ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1158.419699GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [49/108]: start run 4 4 8 688.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 6.112096ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 200.979629GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [50/108]: start run 4 4 16 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 3.170480ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 387.451359GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [51/108]: start run 4 4 32 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.764176ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 696.306255GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [52/108]: start run 4 4 43 128.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.478496ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 830.848906GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [53/108]: start run 4 4 64 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.096624ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1120.171348GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [54/108]: start run 4 4 86 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.204136ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1020.156182GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [55/108]: start run 4 4 128 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 0.962456ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1276.325135GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [56/108]: start run 4 4 172 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.071272ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1146.680567GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [57/108]: start run 4 4 256 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.876968ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 654.463360GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [58/108]: start run 4 4 344 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.142064ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1075.602404GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [59/108]: start run 4 4 512 10.75\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.868360ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 657.478636GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [60/108]: start run 4 4 688 8.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.060744ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1158.061496GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [61/108]: start run 4 8 8 688.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 5.935344ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 206.964716GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [62/108]: start run 4 8 16 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 3.127480ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 392.778462GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [63/108]: start run 4 8 32 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.748496ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 702.550526GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [64/108]: start run 4 8 43 128.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.488600ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 825.209448GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [65/108]: start run 4 8 64 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.107496ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1109.174917GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [66/108]: start run 4 8 86 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.244392ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 987.154196GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [67/108]: start run 4 8 128 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 0.977712ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1256.409642GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [68/108]: start run 4 8 172 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.117256ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1099.485511GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [69/108]: start run 4 8 256 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.894448ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 648.424651GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [70/108]: start run 4 8 344 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.251160ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 981.814304GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [71/108]: start run 4 8 512 10.75\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.902080ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 645.822880GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [72/108]: start run 4 8 688 8.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.153648ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 1064.802075GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [73/108]: start run 8 2 8 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 7.256144ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 169.291952GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [74/108]: start run 8 2 16 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 3.904304ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 314.628877GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [75/108]: start run 8 2 32 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.099176ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 585.185227GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [76/108]: start run 8 2 43 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.972736ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 622.691928GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [77/108]: start run 8 2 64 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.319016ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 931.305446GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [78/108]: start run 8 2 86 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.486536ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 826.355220GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [79/108]: start run 8 2 128 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 37.107560ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 33.103949GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [80/108]: start run 8 2 172 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.619040ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 758.725408GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [81/108]: start run 8 2 256 10.75\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 36.288208ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 33.851404GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [82/108]: start run 8 2 344 8.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.644720ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 746.878973GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [83/108]: start run 8 2 512 5.375\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 40.563976ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 30.283195GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [84/108]: start run 8 2 688 4.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.691472ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 456.407046GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [85/108]: start run 8 4 8 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 6.944400ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 176.891709GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [86/108]: start run 8 4 16 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 3.718872ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 330.317038GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [87/108]: start run 8 4 32 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.044200ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 600.922994GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [88/108]: start run 8 4 43 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.915744ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 641.216563GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [89/108]: start run 8 4 64 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.302680ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 942.984297GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [90/108]: start run 8 4 86 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.483496ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 828.048599GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [91/108]: start run 8 4 128 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 37.157832ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 33.059162GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [92/108]: start run 8 4 172 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.645616ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 746.472314GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [93/108]: start run 8 4 256 10.75\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 36.271656ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 33.866851GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [94/108]: start run 8 4 344 8.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.616288ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 760.017264GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [95/108]: start run 8 4 512 5.375\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 40.398880ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 30.406951GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [96/108]: start run 8 4 688 4.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.735392ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 449.078883GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [97/108]: start run 8 8 8 344.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 6.828824ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 179.885553GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [98/108]: start run 8 8 16 172.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 3.689720ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 332.926830GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [99/108]: start run 8 8 32 86.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.031800ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 604.590405GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [100/108]: start run 8 8 43 64.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.936496ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 634.345118GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [101/108]: start run 8 8 64 43.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.302576ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 943.059587GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [102/108]: start run 8 8 86 32.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.511608ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 812.649036GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [103/108]: start run 8 8 128 21.5\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 37.153376ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 33.063127GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [104/108]: start run 8 8 172 16.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.627072ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 754.979979GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [105/108]: start run 8 8 256 10.75\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 36.400808ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 33.746690GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [106/108]: start run 8 8 344 8.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 1.619544ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 758.489293GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [107/108]: start run 8 8 512 5.375\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 40.592184ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 30.262151GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "search record [108/108]: start run 8 8 688 4.0\n",
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[search] GEMV-Blocking: 2.684696ms with loop 32\n",
      "evaluator[search] GEMV-Blocking: 457.558988GFLOPS\n",
      "[[[-7156. -6716. -6644. ... -6588. -6236. -6244.]]]\n",
      "=====\n",
      "================================\n",
      "+----+----+-----+--------+--------------------+\n",
      "| vf | vi |  tx |   bx   |      cost(ms)      |\n",
      "+----+----+-----+--------+--------------------+\n",
      "| 2  | 2  |  8  | 1376.0 |     11.036448      |\n",
      "| 2  | 2  |  16 | 688.0  |      5.313416      |\n",
      "| 2  | 2  |  32 | 344.0  |      2.823272      |\n",
      "| 2  | 2  |  43 | 256.0  |      2.310936      |\n",
      "| 2  | 2  |  64 | 172.0  |      1.584512      |\n",
      "| 2  | 2  |  86 | 128.0  |      1.493392      |\n",
      "| 2  | 2  | 128 |  86.0  |      1.170536      |\n",
      "| 2  | 2  | 172 |  64.0  |      1.283008      |\n",
      "| 2  | 2  | 256 |  43.0  |      1.092064      |\n",
      "| 2  | 2  | 344 |  32.0  |      1.12384       |\n",
      "| 2  | 2  | 512 |  21.5  |      2.057152      |\n",
      "| 2  | 2  | 688 |  16.0  |      1.367848      |\n",
      "| 2  | 4  |  8  | 1376.0 |     10.136144      |\n",
      "| 2  | 4  |  16 | 688.0  |      5.051808      |\n",
      "| 2  | 4  |  32 | 344.0  |      2.745904      |\n",
      "| 2  | 4  |  43 | 256.0  |      2.283128      |\n",
      "| 2  | 4  |  64 | 172.0  |      1.567584      |\n",
      "| 2  | 4  |  86 | 128.0  |      1.476112      |\n",
      "| 2  | 4  | 128 |  86.0  |      1.15528       |\n",
      "| 2  | 4  | 172 |  64.0  |      1.282984      |\n",
      "| 2  | 4  | 256 |  43.0  |      1.089192      |\n",
      "| 2  | 4  | 344 |  32.0  |      1.130824      |\n",
      "| 2  | 4  | 512 |  21.5  |      2.014184      |\n",
      "| 2  | 4  | 688 |  16.0  |      1.369992      |\n",
      "| 2  | 8  |  8  | 1376.0 |      9.727176      |\n",
      "| 2  | 8  |  16 | 688.0  |      4.937312      |\n",
      "| 2  | 8  |  32 | 344.0  |      2.712968      |\n",
      "| 2  | 8  |  43 | 256.0  |      2.232616      |\n",
      "| 2  | 8  |  64 | 172.0  |      1.573552      |\n",
      "| 2  | 8  |  86 | 128.0  |      1.442368      |\n",
      "| 2  | 8  | 128 |  86.0  |      1.21176       |\n",
      "| 2  | 8  | 172 |  64.0  |      1.268064      |\n",
      "| 2  | 8  | 256 |  43.0  |      1.095784      |\n",
      "| 2  | 8  | 344 |  32.0  |      1.250776      |\n",
      "| 2  | 8  | 512 |  21.5  |      2.039624      |\n",
      "| 2  | 8  | 688 |  16.0  |      1.567008      |\n",
      "| 4  | 2  |  8  | 688.0  |      6.68796       |\n",
      "| 4  | 2  |  16 | 344.0  |      3.354768      |\n",
      "| 4  | 2  |  32 | 172.0  |      1.815472      |\n",
      "| 4  | 2  |  43 | 128.0  |      1.50604       |\n",
      "| 4  | 2  |  64 |  86.0  |      1.108888      |\n",
      "| 4  | 2  |  86 |  64.0  |      1.204704      |\n",
      "| 4  | 2  | 128 |  43.0  |      0.96512       |\n",
      "| 4  | 2  | 172 |  32.0  |      1.070088      |\n",
      "| 4  | 2  | 256 |  21.5  |      1.879272      |\n",
      "| 4  | 2  | 344 |  16.0  |      1.135504      |\n",
      "| 4  | 2  | 512 | 10.75  |      1.884896      |\n",
      "| 4  | 2  | 688 |  8.0   |      1.060416      |\n",
      "| 4  | 4  |  8  | 688.0  |      6.112096      |\n",
      "| 4  | 4  |  16 | 344.0  |      3.17048       |\n",
      "| 4  | 4  |  32 | 172.0  |      1.764176      |\n",
      "| 4  | 4  |  43 | 128.0  |      1.478496      |\n",
      "| 4  | 4  |  64 |  86.0  |      1.096624      |\n",
      "| 4  | 4  |  86 |  64.0  |      1.204136      |\n",
      "| 4  | 4  | 128 |  43.0  |      0.962456      |\n",
      "| 4  | 4  | 172 |  32.0  |      1.071272      |\n",
      "| 4  | 4  | 256 |  21.5  |      1.876968      |\n",
      "| 4  | 4  | 344 |  16.0  |      1.142064      |\n",
      "| 4  | 4  | 512 | 10.75  |      1.86836       |\n",
      "| 4  | 4  | 688 |  8.0   |      1.060744      |\n",
      "| 4  | 8  |  8  | 688.0  |      5.935344      |\n",
      "| 4  | 8  |  16 | 344.0  |      3.12748       |\n",
      "| 4  | 8  |  32 | 172.0  |      1.748496      |\n",
      "| 4  | 8  |  43 | 128.0  |       1.4886       |\n",
      "| 4  | 8  |  64 |  86.0  |      1.107496      |\n",
      "| 4  | 8  |  86 |  64.0  |      1.244392      |\n",
      "| 4  | 8  | 128 |  43.0  | 0.9777119999999999 |\n",
      "| 4  | 8  | 172 |  32.0  |      1.117256      |\n",
      "| 4  | 8  | 256 |  21.5  |      1.894448      |\n",
      "| 4  | 8  | 344 |  16.0  |      1.25116       |\n",
      "| 4  | 8  | 512 | 10.75  |      1.90208       |\n",
      "| 4  | 8  | 688 |  8.0   |      1.153648      |\n",
      "| 8  | 2  |  8  | 344.0  |      7.256144      |\n",
      "| 8  | 2  |  16 | 172.0  |      3.904304      |\n",
      "| 8  | 2  |  32 |  86.0  |      2.099176      |\n",
      "| 8  | 2  |  43 |  64.0  |      1.972736      |\n",
      "| 8  | 2  |  64 |  43.0  |      1.319016      |\n",
      "| 8  | 2  |  86 |  32.0  |      1.486536      |\n",
      "| 8  | 2  | 128 |  21.5  |      37.10756      |\n",
      "| 8  | 2  | 172 |  16.0  |      1.61904       |\n",
      "| 8  | 2  | 256 | 10.75  |     36.288208      |\n",
      "| 8  | 2  | 344 |  8.0   |      1.64472       |\n",
      "| 8  | 2  | 512 | 5.375  |     40.563976      |\n",
      "| 8  | 2  | 688 |  4.0   |      2.691472      |\n",
      "| 8  | 4  |  8  | 344.0  |       6.9444       |\n",
      "| 8  | 4  |  16 | 172.0  |      3.718872      |\n",
      "| 8  | 4  |  32 |  86.0  |       2.0442       |\n",
      "| 8  | 4  |  43 |  64.0  |      1.915744      |\n",
      "| 8  | 4  |  64 |  43.0  |      1.30268       |\n",
      "| 8  | 4  |  86 |  32.0  |      1.483496      |\n",
      "| 8  | 4  | 128 |  21.5  |     37.157832      |\n",
      "| 8  | 4  | 172 |  16.0  |      1.645616      |\n",
      "| 8  | 4  | 256 | 10.75  |     36.271656      |\n",
      "| 8  | 4  | 344 |  8.0   |      1.616288      |\n",
      "| 8  | 4  | 512 | 5.375  |      40.39888      |\n",
      "| 8  | 4  | 688 |  4.0   |      2.735392      |\n",
      "| 8  | 8  |  8  | 344.0  |      6.828824      |\n",
      "| 8  | 8  |  16 | 172.0  |      3.68972       |\n",
      "| 8  | 8  |  32 |  86.0  |       2.0318       |\n",
      "| 8  | 8  |  43 |  64.0  |      1.936496      |\n",
      "| 8  | 8  |  64 |  43.0  |      1.302576      |\n",
      "| 8  | 8  |  86 |  32.0  |      1.511608      |\n",
      "| 8  | 8  | 128 |  21.5  |     37.153376      |\n",
      "| 8  | 8  | 172 |  16.0  |      1.627072      |\n",
      "| 8  | 8  | 256 | 10.75  |     36.400808      |\n",
      "| 8  | 8  | 344 |  8.0   |      1.619544      |\n",
      "| 8  | 8  | 512 | 5.375  |     40.592184      |\n",
      "| 8  | 8  | 688 |  4.0   |      2.684696      |\n",
      "+----+----+-----+--------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "# 自动搜索\n",
    "def auto_tune(record_file: str):\n",
    "    from typing import Union\n",
    "    def search(vf: int, vi: int, tx: int, bx: Union[int, None] = None):\n",
    "        \"\"\"search by workgroup\n",
    "\n",
    "        Args:\n",
    "            blockIdxX (_type_): blockIdx.x\n",
    "            threadIdxX (_type_): threadIdx.x\n",
    "            vectorize_output (_type_): 输出的vectorize参数, 决定单线程输出多少个结果\n",
    "            vectorize_input (list, optional): 输入X拷贝到shared_memory时的vectorize参数, 一般为4或8\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        @I.ir_module\n",
    "        class ModuleToManual:\n",
    "            @T.prim_func(private=False)\n",
    "            def main(lv571: T.Buffer((T.int64(512), T.int64(w_y)), \"uint32\"), lv572: T.Buffer((T.int64(128), T.int64(w_y)), \"float16\"), lv1654: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"), var_matmul_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(w_y)), \"float16\")):\n",
    "                T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "                # with T.block(\"root\"):\n",
    "                p_output0_intermediate = T.alloc_buffer((T.int64(4096), T.int64(w_y)), \"float16\")\n",
    "                for i, j in T.grid(T.int64(4096), T.int64(w_y)):\n",
    "                    with T.block(\"decode\"):\n",
    "                        v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                        T.reads(lv571[v_i // T.int64(8), v_j], lv572[v_i // T.int64(32), v_j])\n",
    "                        T.writes(p_output0_intermediate[v_i, v_j])\n",
    "                        p_output0_intermediate[v_i, v_j] = (T.Cast(\"float16\", T.bitwise_and(T.shift_right(lv571[v_i // T.int64(8), v_j], T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4)), T.uint32(15))) - T.float16(7)) * lv572[v_i // T.int64(32), v_j]\n",
    "                for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(w_y), T.int64(4096)):\n",
    "                    with T.block(\"matmul\"):\n",
    "                        v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                        T.reads(lv1654[v_i0, v_i1, v_k], p_output0_intermediate[v_k, v_i2])\n",
    "                        T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                        with T.init():\n",
    "                            var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                        var_matmul_intermediate[v_i0, v_i1, v_i2] = var_matmul_intermediate[v_i0, v_i1, v_i2] + lv1654[v_i0, v_i1, v_k] * p_output0_intermediate[v_k, v_i2]\n",
    "\n",
    "        sch = tvm.tir.Schedule(ModuleToManual)\n",
    "        b0 = sch.get_block(name=\"decode\", func_name=\"main\")\n",
    "        b1 = sch.get_block(name=\"matmul\", func_name=\"main\")\n",
    "        l2, l3, l4, l5 = sch.get_loops(block=b1)\n",
    "        l6 = sch.fuse(l2, l3, l4, preserve_unit_iters=True)\n",
    "        l10, l11, l12 = sch.split(loop=l6, factors=[bx, tx, vf], preserve_unit_iters=True)\n",
    "        v13, v14, v15 = sch.sample_perfect_tile(\n",
    "            loop=l5, n=3, max_innermost_factor=8, decision=[128, 4, 8]\n",
    "        )\n",
    "        l16, l17, l18 = sch.split(\n",
    "            loop=l5, factors=[v13, v14, v15], preserve_unit_iters=True\n",
    "        )\n",
    "        sch.reorder(l10, l11, l16, l17, l18, l12)\n",
    "        sch.bind(loop=l10, thread_axis=\"blockIdx.x\")\n",
    "        sch.bind(loop=l11, thread_axis=\"threadIdx.x\")\n",
    "        sch.compute_inline(block=b0)\n",
    "        b19 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")\n",
    "        sch.reverse_compute_at(block=b19, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "        b20 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"local\")\n",
    "        b21 = sch.cache_read(block=b1, read_buffer_index=2, storage_scope=\"local\")\n",
    "        b22 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")\n",
    "        sch.compute_at(block=b22, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "        v23 = sch.sample_categorical(\n",
    "            candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=[1, 2, 4, 8].index(vi)\n",
    "        )\n",
    "        sch.annotate(\n",
    "            block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v23\n",
    "        )\n",
    "        sch.compute_at(block=b20, loop=l17, preserve_unit_loops=True, index=-1)\n",
    "        sch.compute_at(block=b21, loop=l16, preserve_unit_loops=True, index=-1)\n",
    "        l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)\n",
    "        sch.vectorize(loop=l29)\n",
    "        l30, l31, l32, l33, l34 = sch.get_loops(block=b21)\n",
    "        sch.vectorize(loop=l34)\n",
    "        l35, l36, l37, l38, l39 = sch.get_loops(block=b19)\n",
    "        sch.vectorize(loop=l39)\n",
    "        sch.vectorize(loop=l12)\n",
    "        b40 = sch.decompose_reduction(block=b1, loop=l16)\n",
    "        sch.enter_postproc()\n",
    "        sch.unannotate(block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\")\n",
    "        l43, l44, l45, l46, l47 = sch.get_loops(block=b22)\n",
    "        l48, l49, l50 = sch.split(loop=l47, factors=[None, tx, vi], preserve_unit_iters=True)\n",
    "        sch.vectorize(loop=l50)\n",
    "        sch.bind(loop=l49, thread_axis=\"threadIdx.x\")\n",
    "        return sch.mod\n",
    "\n",
    "    vec_factor = [2, 4, 8]\n",
    "    vec_input = [2, 4, 8]\n",
    "    blockx = [None]\n",
    "    threadx = [8, 16, 32, 43, 64, 86, 128, 172, 256, 344, 512, 688]\n",
    "    task_index = 0\n",
    "    total_task_num = len(vec_factor)*len(vec_input)*len(blockx)*len(threadx)\n",
    "    records = {}\n",
    "    print(f\"Total tasks: {total_task_num}\")\n",
    "    write_interval = 5\n",
    "    from prettytable import PrettyTable\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"vf\", \"vi\", \"tx\", \"bx\", \"cost(ms)\"]\n",
    "    for vf in vec_factor:\n",
    "        for vi in vec_input:\n",
    "            for tx in threadx:\n",
    "                    task_index = task_index + 1\n",
    "                    import math\n",
    "                    bx = w_y /(vf * tx)\n",
    "                    if tx * vf > w_y or tx > 1024:\n",
    "                        print(f\"search record [{task_index}/{total_task_num}]: skip {vf} {vi} {tx} {bx}\")\n",
    "                        continue\n",
    "                    print(f\"search record [{task_index}/{total_task_num}]: start run {vf} {vi} {tx} {bx}\")\n",
    "                    bx_real = int(bx)\n",
    "                    if w_y % (vf * tx) != 0:\n",
    "                        bx_real = None\n",
    "                    mod_deploy = search(vf, vi, tx, bx_real)\n",
    "                    cost = test_opencl(mod_deploy, \"search\")\n",
    "                    print(\"=====\")\n",
    "                    records[(vf, vi, tx, bx)] = cost\n",
    "\n",
    "                    table.add_row([vf, vi, tx, bx, cost])\n",
    "                    if task_index % write_interval == 0:\n",
    "                        with open(record_file, 'wt') as f:\n",
    "                            f.write(table.get_csv_string())\n",
    "    print(\"================================\")\n",
    "    print(table)\n",
    "    \n",
    "    # record_sorted = sorted(record.items(), key=lambda x: x[1][0], reverse=True)\n",
    "auto_tune(\"./manual_tune/fused_gate_up_tune_record_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.158.176.30\"\n",
    "rpc_port = 5001\n",
    "# remote = autotvm.measure.request_remote(device_key, \"10.158.176.30\", 5001, timeout=10000)\n",
    "# dev = remote.device(str(target), 0)\n",
    "\n",
    "# num_flop = 1228406784\n",
    "# W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "# S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "# Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# # Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# W_nd = tvm.nd.array(W_np, dev)\n",
    "# S_nd = tvm.nd.array(S_np, dev)\n",
    "# Input_nd = tvm.nd.array(Input_np, dev)\n",
    "# Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc_config = ms.runner.RPCConfig(tracker_host=rpc_host, tracker_port=rpc_port, tracker_key = device_key)\n",
    "runner= ms.runner.RPCRunner(rpc_config)\n",
    "# ms.builder.LocalBuilder()\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "database = ms.tune_tir(\n",
    "    mod=ModuleSrc,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_first\",\n",
    "    cost_model=\"xgb\",\n",
    "    runner = runner\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, sch.mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.script import relax as R\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def main(A: R.Tensor((3, 4), dtype=\"float32\"), B: R.Tensor((4, 5), dtype=\"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((3, 5), dtype=\"float32\") = R.matmul(A, B)\n",
    "            gv: R.Tensor((3, 5), dtype=\"float32\") = lv\n",
    "            R.output(gv)\n",
    "        return gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto_scheduler test\n",
    "from tvm import auto_scheduler\n",
    "import numpy as np\n",
    "a_np = np.random.rand(3, 4).astype(\"float32\")\n",
    "b_np = np.random.rand(4, 5).astype(\"float32\")\n",
    "a_nd = tvm.runtime.NDArray(a_np)\n",
    "b_nd = tvm.runtime.NDArray(b_np)\n",
    "sch = tvm.tir.Schedule(Module)\n",
    "\n",
    "params = {\"A\": a_np, \"B\": b_np}\n",
    "## 报错，这里只支持relay\n",
    "# tasks = auto_scheduler.extract_tasks(sch.mod, params, target=target)\n",
    "tasks = ms.relax_integration.extract_tasks(sch.mod, target=target, params=params)\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mod_deploy import Module as ModuleAll\n",
    "params_all = {}\n",
    "tasks_all = auto_scheduler.extract_tasks(ModuleAll, params_all, target=target)\n",
    "print(len(tasks_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_file = \"tune.json\"\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "W_nd = tvm.nd.array(W_np, dev)\n",
    "S_nd = tvm.nd.array(S_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "new_mod = sch.mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = auto_scheduler.SearchTask(func=sch.mod['fused_fused_decode11_fused_matmul5_cast2'], args=sch.mod['fused_fused_decode11_fused_matmul5_cast2'].params, target=target)\n",
    "\n",
    "# tune_option = auto_scheduler.TuningOptions(\n",
    "#     num_measure_trials=10,\n",
    "#     measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "\n",
    "database = ms.tune_tir(\n",
    "    mod=new_mod,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_45593_1\",\n",
    "    cost_model=\"xgb\"\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, new_mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sch1.trace)\n",
    "# print(sch1.mod.script())\n",
    "rt_mod = tvm.build(sch1.mod, target=\"cuda\")\n",
    "\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (1228406784 / evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_database = ms.Database.create(kind='json', work_dir='./tune_45593_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sch = ms.tir_integration.compile_tir(record_database, new_mod, target)\n",
    "\n",
    "record_rt_mod = tvm.build(record_sch.mod, target=\"cuda\")\n",
    "\n",
    "record_evaluator = record_rt_mod.time_evaluator(\"main\", dev, number=20)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / record_evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(record_sch.trace)\n",
    "print(record_sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Dict, List, Optional, Union, Callable\n",
    "from tvm import runtime\n",
    "if TYPE_CHECKING:\n",
    "    import numpy as np  # type: ignore\n",
    "    from tvm.ir import IRModule\n",
    "    from tvm.meta_schedule.runner import EvaluatorConfig, RPCConfig\n",
    "    from tvm.runtime import Device, Module, NDArray\n",
    "    from tvm.target import Target\n",
    "    from tvm.runtime.vm import Executable\n",
    "\n",
    "\n",
    "def f_measurement(\n",
    "    rt_mod: runtime.Module, device: runtime.ndarray.Device, input_data: Dict[str, runtime.NDArray]\n",
    "):\n",
    "    vm = relax.VirtualMachine(rt_mod, device=device)\n",
    "    vm.save_function(\"main\", \"measure_func\", **input_data, include_return=False)\n",
    "    evaluator = vm.time_evaluator(\n",
    "        func_name=\"measure_func\",\n",
    "        dev=device,\n",
    "        repeat=100,\n",
    "        number=1,\n",
    "        min_repeat_ms=500,\n",
    "    )\n",
    "    return evaluator()\n",
    "\n",
    "def run_module_via_rpc(\n",
    "    rpc_config: \"RPCConfig\",\n",
    "    lib: Union[\"Module\", \"Executable\"],\n",
    "    dev_type: str,\n",
    "    args: Union[Dict[int, \"np.ndarray\"], Dict[str, \"np.ndarray\"]],\n",
    "    continuation: Callable,\n",
    "    backend: Optional[str] = \"graph\",\n",
    "):\n",
    "    \"\"\"Execute a tvm.runtime.Module on RPC remote\"\"\"\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    from tvm.contrib.tar import tar\n",
    "    from tvm.runtime import ndarray\n",
    "\n",
    "    # pylint: enable=import-outside-toplevel\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + tar.output_format)\n",
    "        filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + \"so\")\n",
    "        if backend == \"vm\":\n",
    "            code, lib = lib.save(filename, fmt=\"so\")\n",
    "        from tvm.contrib import ndk\n",
    "        lib.export_library(filename, ndk.create_shared)\n",
    "        session = rpc_config.connect_server()\n",
    "        print(type(session._sess))\n",
    "        session.upload(filename)\n",
    "        _, filename = os.path.split(filename)\n",
    "        rt_mod = session.load_module(filename)\n",
    "        \n",
    "        if backend == \"vm\":\n",
    "            rt_mod = session.get_function(\"runtime.Load_Executable\")(code, rt_mod)\n",
    "            # rt_mod = session.get_function(\"runtime.module.loadfile_relax.Executable\")(filename)\n",
    "        dev = session.device(dev_type=dev_type, dev_id=0)\n",
    "        # print(dev)\n",
    "        # create the remote runtime module\n",
    "        print(rt_mod)\n",
    "        print(rt_mod['main'])\n",
    "        from tvm.contrib import graph_executor as runtime\n",
    "        module = runtime.GraphModule(rt_mod[\"main\"](dev))\n",
    "        print(module)\n",
    "        for k, v in args.items():\n",
    "            module.set_input(k, tvm.nd.array(v))\n",
    "        return module.run()\n",
    "        # nd_args = {k: ndarray.array(v, dev) for k, v in args.items()}\n",
    "        nd_args = {k: ndarray.empty(v.shape, v.dtype, dev) for k, v in args.items()}\n",
    "        return continuation(rt_mod, dev, nd_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
