{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tvm\n",
    "from tvm.script import ir as I\n",
    "from tvm.script import tir as T\n",
    "from tvm import autotvm, auto_scheduler\n",
    "from tvm.autotvm.tuner import XGBTuner, GATuner, RandomTuner, GridSearchTuner\n",
    "from tvm import meta_schedule as ms\n",
    "from tvm.ir import IRModule\n",
    "from tvm import relax\n",
    "from tvm import rpc\n",
    "from tvm.contrib import utils, ndk\n",
    "x_shape = 4096\n",
    "w_w_x = 512\n",
    "w_s_x = 128\n",
    "w_y = 11008\n",
    "func_name = \"main\"\n",
    "@I.ir_module\n",
    "class ModuleSrc:\n",
    "    @T.prim_func(private=False)\n",
    "    def main(lv1611: T.Buffer((T.int64(512), T.int64(11008)), \"uint32\"),\n",
    "             lv1612: T.Buffer((T.int64(128), T.int64(11008)), \"float16\"),\n",
    "             lv1622: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"),\n",
    "             p_output0_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(11008)), \"float16\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        var_decode_intermediate = T.alloc_buffer((T.int64(4096), T.int64(11008)), \"float16\")\n",
    "        var_matmul_intermediate = T.alloc_buffer(\n",
    "            (T.int64(1), T.int64(1), T.int64(11008)), \"float16\"\n",
    "        )\n",
    "        compute = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(11008)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(11008)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv1611[v_i // T.int64(8), v_j], lv1612[v_i // T.int64(32), v_j])\n",
    "                T.writes(var_decode_intermediate[v_i, v_j])\n",
    "                var_decode_intermediate[v_i, v_j] = (\n",
    "                    T.Cast(\n",
    "                        \"float16\",\n",
    "                        T.bitwise_and(\n",
    "                            T.shift_right(\n",
    "                                lv1611[v_i // T.int64(8), v_j],\n",
    "                                T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4),\n",
    "                            ),\n",
    "                            T.uint32(15),\n",
    "                        ),\n",
    "                    )\n",
    "                    - T.float16(7)\n",
    "                ) * lv1612[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(11008), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv1622[v_i0, v_i1, v_k], var_decode_intermediate[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = (\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2]\n",
    "                    + lv1622[v_i0, v_i1, v_k] * var_decode_intermediate[v_k, v_i2]\n",
    "                )\n",
    "        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(11008)):\n",
    "            with T.block(\"compute\"):\n",
    "                v_i0, v_i1, v_i2 = T.axis.remap(\"SSS\", [i0, i1, i2])\n",
    "                T.reads(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                T.writes(compute[v_i0, v_i1, v_i2])\n",
    "                compute[v_i0, v_i1, v_i2] = T.sigmoid(\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2]\n",
    "                )\n",
    "        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(11008)):\n",
    "            with T.block(\"T_multiply\"):\n",
    "                v_ax0, v_ax1, v_ax2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "                T.reads(\n",
    "                    var_matmul_intermediate[v_ax0, v_ax1, v_ax2],\n",
    "                    compute[v_ax0, v_ax1, v_ax2],\n",
    "                )\n",
    "                T.writes(p_output0_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                p_output0_intermediate[v_ax0, v_ax1, v_ax2] = (\n",
    "                    var_matmul_intermediate[v_ax0, v_ax1, v_ax2]\n",
    "                    * compute[v_ax0, v_ax1, v_ax2]\n",
    "                )\n",
    "\n",
    "@I.ir_module\n",
    "class ModuleToManual:\n",
    "    @T.prim_func(private=False)\n",
    "    def main(lv1611: T.Buffer((T.int64(512), T.int64(11008)), \"uint32\"),\n",
    "             lv1612: T.Buffer((T.int64(128), T.int64(11008)), \"float16\"),\n",
    "             lv1622: T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\"),\n",
    "             p_output0_intermediate: T.Buffer((T.int64(1), T.int64(1), T.int64(11008)), \"float16\")):\n",
    "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
    "        # with T.block(\"root\"):\n",
    "        var_decode_intermediate = T.alloc_buffer((T.int64(4096), T.int64(11008)), \"float16\")\n",
    "        var_matmul_intermediate = T.alloc_buffer(\n",
    "            (T.int64(1), T.int64(1), T.int64(11008)), \"float16\"\n",
    "        )\n",
    "        compute = T.alloc_buffer((T.int64(1), T.int64(1), T.int64(11008)), \"float16\")\n",
    "        for i, j in T.grid(T.int64(4096), T.int64(11008)):\n",
    "            with T.block(\"decode\"):\n",
    "                v_i, v_j = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(lv1611[v_i // T.int64(8), v_j], lv1612[v_i // T.int64(32), v_j])\n",
    "                T.writes(var_decode_intermediate[v_i, v_j])\n",
    "                var_decode_intermediate[v_i, v_j] = (\n",
    "                    T.Cast(\n",
    "                        \"float16\",\n",
    "                        T.bitwise_and(\n",
    "                            T.shift_right(\n",
    "                                lv1611[v_i // T.int64(8), v_j],\n",
    "                                T.Cast(\"uint32\", v_i % T.int64(8)) * T.uint32(4),\n",
    "                            ),\n",
    "                            T.uint32(15),\n",
    "                        ),\n",
    "                    )\n",
    "                    - T.float16(7)\n",
    "                ) * lv1612[v_i // T.int64(32), v_j]\n",
    "        for i0, i1, i2, k in T.grid(T.int64(1), T.int64(1), T.int64(11008), T.int64(4096)):\n",
    "            with T.block(\"matmul\"):\n",
    "                v_i0, v_i1, v_i2, v_k = T.axis.remap(\"SSSR\", [i0, i1, i2, k])\n",
    "                T.reads(lv1622[v_i0, v_i1, v_k], var_decode_intermediate[v_k, v_i2])\n",
    "                T.writes(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                with T.init():\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2] = T.float16(0)\n",
    "                var_matmul_intermediate[v_i0, v_i1, v_i2] = (\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2]\n",
    "                    + lv1622[v_i0, v_i1, v_k] * var_decode_intermediate[v_k, v_i2]\n",
    "                )\n",
    "        for i0, i1, i2 in T.grid(T.int64(1), T.int64(1), T.int64(11008)):\n",
    "            with T.block(\"compute\"):\n",
    "                v_i0, v_i1, v_i2 = T.axis.remap(\"SSS\", [i0, i1, i2])\n",
    "                T.reads(var_matmul_intermediate[v_i0, v_i1, v_i2])\n",
    "                T.writes(compute[v_i0, v_i1, v_i2])\n",
    "                compute[v_i0, v_i1, v_i2] = T.sigmoid(\n",
    "                    var_matmul_intermediate[v_i0, v_i1, v_i2]\n",
    "                )\n",
    "        for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(11008)):\n",
    "            with T.block(\"T_multiply\"):\n",
    "                v_ax0, v_ax1, v_ax2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "                T.reads(\n",
    "                    var_matmul_intermediate[v_ax0, v_ax1, v_ax2],\n",
    "                    compute[v_ax0, v_ax1, v_ax2],\n",
    "                )\n",
    "                T.writes(p_output0_intermediate[v_ax0, v_ax1, v_ax2])\n",
    "                p_output0_intermediate[v_ax0, v_ax1, v_ax2] = (\n",
    "                    var_matmul_intermediate[v_ax0, v_ax1, v_ax2]\n",
    "                    * compute[v_ax0, v_ax1, v_ax2]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ref to mlc-llm/dispatch/dispatch_tir_operator_adreno.py\n",
    "def print_object_attr(cls):\n",
    "    print(f\"attr len = {len(cls.__dict__)}\")\n",
    "    for k, v in cls.__dict__.items():\n",
    "        # if k.startswith(\"_\") or k.startswith(\"__\"):\n",
    "        #     continue\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "def sch_fused_decode5_fused_matmul6_silu1(func):\n",
    "    \"\"\"\n",
    "    Summarize\n",
    "    --------------------------------\n",
    "    gemv整体分析优化内容:\n",
    "    1. 内存方面\n",
    "        \n",
    "        1.1 将参与多次计算的 `v` 搬运到shared_memory中\n",
    "        \n",
    "        1.2 将参与单次运算, 且较小的 quantized_weight_scale搬运到local memory中, 这一步感觉作用不大，可以做实验看下去掉的效果\n",
    "\n",
    "        1.3 通过tiling, 使计算满足cache line, 提高cache命中\n",
    "    \n",
    "    2. 计算方面\n",
    "        \n",
    "        2.1 计算合并(内联), 如将dequantize操作与gemv合并, 将中间结果从local_memory -> register, 降低中间缓存及其内存搬运导致的耗时, 大幅度提高计算效率\n",
    "\n",
    "        2.2 通过split和reorder操作实现tiling, 提高cache命中, 减少内存搬运\n",
    "\n",
    "        2.3 绑定硬件线程(block & thread)\n",
    "\n",
    "        2.4 使用vectorize, 利用SIMD指令集加速 load/store & ALU计算\n",
    "\n",
    "        2.5 实际上还将内存搬运(global memory -> local/shared memory)与计算独立开来，提高内存搬运效率(需要看生成的源码如何实现)\n",
    "\n",
    "        2.6 `v` 的shared memory搬移时, 使用cooperative_fetch, 暂时没搞明白如何实现, 需要生成的cuda代码\n",
    "\n",
    "    3. 改进点?\n",
    "        没有看到用unroll操作, 其余的好像都用到了 -- 用了 vectorize, 不需要unroll了\n",
    "        \n",
    "    opencl source\n",
    "    ----------------------------------------------------------------\n",
    "    // Function: main_kernel\n",
    "    #ifdef cl_khr_fp16\n",
    "    #pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
    "    #elif defined(cl_amd_fp16)\n",
    "    #pragma OPENCL EXTENSION cl_amd_fp16 : enable\n",
    "    #else\n",
    "    #error \"Half precision floating point not supported by OpenCL implementation on your device.\" \n",
    "    #endif\n",
    "    // lv1611 - w_int4\n",
    "    // lv1612 - scale (group size = 32)\n",
    "    // lv1622 - input x\n",
    "    // kernel:  输入 [512, 11008], [128, 11008], [1,1,4096] -> [1,1 11008]\n",
    "    //          工作组设置: blockIdx = 43, threadIdx = 64\n",
    "    //          计算: output[4] = input[4096] x 4列W [1,1,4096] => [4]\n",
    "    __kernel void main_kernel(__global uint* restrict lv1611, __global half* restrict lv1612, __global half* restrict lv1622, __global half* restrict p_output0_intermediate) {\n",
    "    // 这里标注的是local内存, 即\n",
    "    __local half lv1622_shared[4096];\n",
    "    // 以下三个half4都是private memory, 如果resgiter足够的话, 会放到resgiter中, 如果不够会放到 local memory -> global memory\n",
    "    half4 var_matmul_intermediate_local[1];\n",
    "    half4 lv1612_local[1];\n",
    "    uint4 lv1611_local[1];\n",
    "    ## local_id范围[0,63], 64*8*8=4096, 即一个workgroup把 [1,1,4096]的输入都读到了shared_memory中\n",
    "    for (int ax2_0 = 0; ax2_0 < 8; ++ax2_0) {\n",
    "        vstore8(vload8(0, lv1622 + ((ax2_0 * 512) + ((convert_int(get_local_id(0))) * 8))), 0, lv1622_shared + ((ax2_0 * 512) + ((convert_int(get_local_id(0))) * 8)));\n",
    "    }\n",
    "    // 中间缓存初始化\n",
    "    var_matmul_intermediate_local[0] = ((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f));\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    for (int k_0 = 0; k_0 < 128; ++k_0) {\n",
    "        // load scale(group size = 32), shape[128, 11008]\n",
    "        // 一个group_id处理256个数据, 一个local_id处理(k0 * 4)个数据(reudce) --> blockIdx = 43, threadIdx = 64, 即每次读取 [1, 4] 个Scale数据 -> [1, 4*32]个Weight的scale\n",
    "        lv1612_local[0] = vload4(0, lv1612 + (((k_0 * 11008) + ((convert_int(get_group_id(0))) * 256)) + ((convert_int(get_local_id(0))) * 4)));\n",
    "        for (int k_1 = 0; k_1 < 4; ++k_1) {\n",
    "        // load w_int4, shape[512, 11008]\n",
    "        // 一个group_id处理256个数据, 一个local_id处理(k1 * k0 * 4)个数据 --> blockIdx = 43, threadIdx = 64, 即每次读取 [1, 4] 个 Weight数据 -> [1, 4*8]个Wieght的量化数据\n",
    "        lv1611_local[0] = vload4(0, lv1611 + ((((k_0 * 44032) + (k_1 * 11008)) + ((convert_int(get_group_id(0))) * 256)) + ((convert_int(get_local_id(0))) * 4)));\n",
    "        // weigt: u32 X 4 -> 8 X int4 X 4\n",
    "        for (int k_2 = 0; k_2 < 8; ++k_2) {\n",
    "            // half4 ouput = output + (half4 lv1622_shared[k_0*32 + k_1*8 + k_2]) * half4\n",
    "            var_matmul_intermediate_local[0] = (var_matmul_intermediate_local[0] + (((half4)(lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)], lv1622_shared[(((k_0 * 32) + (k_1 * 8)) + k_2)])) * (((convert_half4(((lv1611_local[0]  >>  ((uint4)(((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4), ((convert_uint(k_2)) * (uint)4))))  &  ((uint4)((uint)15, (uint)15, (uint)15, (uint)15))))) - ((half4)((half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f, (half)7.000000e+00f))) * lv1612_local[0])));\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    vstore4((var_matmul_intermediate_local[0] * (((half4)((half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f)) / (((half4)((half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f, (half)1.000000e+00f)) + exp((((half4)((half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f, (half)0.000000e+00f)) - var_matmul_intermediate_local[0]))))), 0, p_output0_intermediate + (((convert_int(get_group_id(0))) * 256) + ((convert_int(get_local_id(0))) * 4)));\n",
    "    }\n",
    "    \"\"\" \n",
    "    sch = tvm.tir.Schedule(func)\n",
    "    b0 = sch.get_block(name=\"decode\", func_name=\"main\") # decode([512,11008], [128, 11008]) -> [4096, 11008]\n",
    "    b1 = sch.get_block(name=\"matmul\", func_name=\"main\") # matmul([4096], [4096, 11008], ) -> [11008]\n",
    "    # matmul: 1, 1, 11008, 4096\n",
    "    l2, l3, l4, l5 = sch.get_loops(block=b1)\n",
    "    # l6 = 11008\n",
    "    l6 = sch.fuse(l2, l3, l4, preserve_unit_iters=True)\n",
    "    # v7 = 43, v8 = 64, v9 = 4\n",
    "    v7, v8, v9 = sch.sample_perfect_tile(\n",
    "        loop=l6, n=3, max_innermost_factor=4, decision=[43, 64, 4]\n",
    "    )\n",
    "    # print(sch.get(v7)) #可以打印值\n",
    "    # spatial axes l6 = 11008 -> l10 = 43, l11 = 64, l12 = 4\n",
    "    l10, l11, l12 = sch.split(loop=l6, factors=[v7, v8, v9], preserve_unit_iters=True)\n",
    "\n",
    "    # l5 = 4096 -> v13 = 128, v14 = 4, v15 = 8\n",
    "    v13, v14, v15 = sch.sample_perfect_tile(\n",
    "        loop=l5, n=3, max_innermost_factor=8, decision=[128, 4, 8]\n",
    "    )\n",
    "    # reduce l5 = 4096 -> l16 = 128, l17 = 4, l18 = 8\n",
    "    l16, l17, l18 = sch.split(\n",
    "        loop=l5, factors=[v13, v14, v15], preserve_unit_iters=True\n",
    "    )\n",
    "    # spatial loopRV: l10, l11, l12\n",
    "    # reduce loopRV: l16, l17, l18\n",
    "    ## 从这里来看规律：\n",
    "    ###     spatial可以拆分，内层factor l12可以放到最内层循环\n",
    "    ###     reduce可以做拆分，并移动到spatial外层循环和内层循环之间，效果上就是每个reduce position可以执行多次spatial操作(即spatial内层)\n",
    "    ####        优化效果: 1. 在spatial内层操作时，输出的element不变，减少了输出内存的读取；但是对于W来说，需要隔行去读取element, 因此需要多计算几个reduce(列)以利用cache\n",
    "    ####        优化效果: 2. 由于W shape = [4096, 11008], x[4096] * shape的操作，相当于x与每列W做运算，这会导致cache命中极低，导致cache thrashing(缓存抖动)\n",
    "    ####            调整后, 对于W的读取，每一个k_inner(l17 * l18 = 4 * 8, fp16数据类型，正好是64bytes)块中会做s_inner(l12 = 4)次运算, \n",
    "    ####            ** 相当于4个cache line同时计算, 实际上4行这个数字取决于cache的大小，这里应该可以更大\n",
    "    ####            ** 相当于，做了tiling操作, 为什么要将k_inner拆分，目前理解是可以用l18(8)来做vectorize的load/store/alu 指令(即SIMD)\n",
    "    ####        优化效果: 3. 对于外层spatial可以绑定到硬件线程了\n",
    "    sch.reorder(l10, l11, l16, l17, l18, l12)\n",
    "    # l10 = 43, 绑定到blockIdx\n",
    "    sch.bind(loop=l10, thread_axis=\"blockIdx.x\")\n",
    "    # l11 = 64, 绑定到threadIdx,  64对于adereno gpu来说有意义，因为cache line = 64bytes\n",
    "    # l10*l11 = 2752, AD4X后的GPU最大的workgroup size(threadIdx.x * threadIdx.y * threadIdx.z) 一般为1024， \n",
    "    sch.bind(loop=l11, thread_axis=\"threadIdx.x\")\n",
    "    # 将decode过程内联到使用decode结果的block中去，这里就是将decode放到matmul中\n",
    "    ## 从最终优化上来看：将 w = dequant(scale); o = x * w 合并为 => o = x * dequant(scale), 即将语句合并了\n",
    "    ##      由于gemv中每个w值参与一次计算, 所以这一步可以节省中间缓存的内存搬运消耗，并可以节省memory, 理论上直接计算的中间结果会存在register，访存更快\n",
    "    ##      @TODO 但是这个计算合并的操作是不是`compute_inline`做的，还不清楚 -- 看`compute_inline`的example是这样的\n",
    "    ### 在compute_inline之前, block(b1).reads = [lv1622, var_decode_intermediate], writes = [var_matmul_intermediate]\n",
    "    sch.compute_inline(block=b0)\n",
    "    ## block(b1).reads = [lv1622, lv1611, lv1612], writes = [var_matmul_intermediate]\n",
    "    ## 从reads的顺序来看, compute_inline会把b0.reads插到b1.reads后面,  同理writes应该也是这样, 但是这里没有writes, 所以没体现\n",
    "    ### 也就是compute_inline把var_decode_intermediate中间变量给省去了，印证了之前的猜测\n",
    "    ## b19指向的是 var_matmul_intermediate的赋值过程，即从global memory -> local memroy\n",
    "    ## 也就是b19是一个block, 并且包含 loops, b19.loops(未经调整)=b19.shape\n",
    "    b19 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope=\"local\")\n",
    "    # reverse_compute_at 将一个消费者block(b19)移动到指定的loop(l11)中，并且重新生成该block包含的loops(暂时理解为绑定到该block的loops)\n",
    "    ##   这样该block(b19)消费的buffer(write index 0, 即var_matmul_intermediate)区域就可以覆盖\n",
    "    ##    指定loop(l11)下生产者blocks(一个或多个，这里指decode)生成的buffer区域(暂时理解为var_decode_intermediate)\n",
    "    # b19: [1, 1, 11008]\n",
    "    # b19插入后: [l10, l11, 1, 1, 1*1*11008/(l10*l11)] -> [43, 64, 1, 1, 4] 这个只是纯赋值的loop grid，在update时还要将reduce插入\n",
    "    sch.reverse_compute_at(block=b19, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    # 执行完reverse_compute_at, b1.reads和writes的内容没变\n",
    "    # 将lv1611[512,11008]设置为local内存\n",
    "    ## 这个内存设置奇怪, 只用一次, 不需要线放到local中\n",
    "    b20 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope=\"local\")\n",
    "    # 将lv1612[128,11008]设置为local内存\n",
    "    ## 这个内存设置奇怪, 只用一次, 不需要线放到local中\n",
    "    b21 = sch.cache_read(block=b1, read_buffer_index=2, storage_scope=\"local\")\n",
    "    # 将lv1622[1,1,4096]设置为local内存\n",
    "    b22 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope=\"shared\")\n",
    "    # 将一个生产者block(b22)移动到指定loop(l11)中，并且重新生成该block(b22)包含的loops, 这样该block(b22)生产的buffer区域就可以覆盖指定loop(l11)下消费者blocks消费的buffer区域\n",
    "    ## preserve_unit_loops的作用: 如将(128, 129) compute_at (128)下，若为true则会有 for 128 (for(1, 129)), 办即保留shape中value为1的loop, 若为False, 则有for 128 (for(129))\n",
    "    # b22: lv1622[1,1,4096]\n",
    "    ## 猜测shape: [l10, l11, 1, 1, 4096/(l10 * l11)] @TODO 无法整除咋整.. 所以这里有个-1, 表示找最后一个可能的loop\n",
    "    ## 观察到shape: [[l10, l11], [1,1,4096]] 即b22的初始化还在[l10,l11]的loop内，但是压根没按照这个计算, 这种情况得看下最终的cuda代码是如何处理的\n",
    "    ##### for i0_i1_i2_fused_1 in T.thread_binding(T.int64(64), thread=\"threadIdx.x\"):\n",
    "    #####     lv1622_shared = T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\", scope=\"shared\")\n",
    "    #####     for ax0, ax1, ax2 in T.grid(T.int64(1), T.int64(1), T.int64(4096)):\n",
    "    #####         with T.block(\"lv1622_shared\"):\n",
    "    #####             v0, v1, v2 = T.axis.remap(\"SSS\", [ax0, ax1, ax2])\n",
    "    #####             lv1622 = T.Buffer((T.int64(1), T.int64(1), T.int64(4096)), \"float16\")\n",
    "    #####             T.reads(lv1622[v0, v1, v2])\n",
    "    #####             T.writes(lv1622_shared[v0, v1, v2])\n",
    "    #####             lv1622_shared[v0, v1, v2] = lv1622[v0, v1, v2]\n",
    "    sch.compute_at(block=b22, loop=l11, preserve_unit_loops=True, index=-1)\n",
    "    # print(sch.get(l11))\n",
    "    v23 = sch.sample_categorical(\n",
    "        candidates=[1, 2, 4, 8], probs=[0.25, 0.25, 0.25, 0.25], decision=3\n",
    "    )\n",
    "    # 做标注，实际形式会在b22中生成 T.block_attr({\"ann_key\", \"ann_value\"})\n",
    "    ## 标注了 lv1622[4096] cooperative_fetch:8\n",
    "    sch.annotate(\n",
    "        block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\", ann_val=v23\n",
    "    )\n",
    "    # 这里有点奇怪，b20和b21是decode需要用到的buffer, 而l16 l17是reduce loop拆分的前两个subloop, 为什么会绑定到不同\n",
    "    sch.compute_at(block=b20, loop=l17, preserve_unit_loops=True, index=-1)\n",
    "    sch.compute_at(block=b21, loop=l16, preserve_unit_loops=True, index=-1)\n",
    "    # @TODO 猜测b20_0: b20的原始loop: [512, 11008], 插入到l17后变为: [l10, l11, l16, l17, 512, 11008]\n",
    "    # l10 = 43, l11 = 64, l16 = 128, l17 = 4 => [l10, l11, l16, l17, 1, 512*11008/(43*64*128*4)]\n",
    "    ## 实际情况: 插入l17后变为 [43, 64, 128, 4, 1, 4]\n",
    "    l24, l25, l26, l27, l28, l29 = sch.get_loops(block=b20)\n",
    "    # print(sch.get(l28))#, sch.get(l25),sch.get(l26), sch.get(l27), sch.get(l28),sch.get(l29))\n",
    "    # vectorize 4\n",
    "    sch.vectorize(loop=l29)\n",
    "    # b21: lv1612[128,11008]\n",
    "    # b21 插入后: [l10, l11, l16, 1, 128*11008/(l10*l11*l16)] => [43, 64, 128, 1, 4 ]\n",
    "    l30, l31, l32, l33, l34 = sch.get_loops(block=b21)\n",
    "    sch.vectorize(loop=l34)\n",
    "    # b19: [1, 1, 11008]\n",
    "    # b19插入后: [l10, l11, 1, 1, 11008/(l10*l11)] -> [43, 64, 1, 1, 4]\n",
    "    ## TODO暂时不知道为什么会多一个维度\n",
    "    l35, l36, l37, l38, l39 = sch.get_loops(block=b19)\n",
    "    # l39 = 4\n",
    "    sch.vectorize(loop=l39)\n",
    "    # l12 = 4\n",
    "    sch.vectorize(loop=l12)\n",
    "    # 包含reduction计算的即为reduction block\n",
    "    # 将reduction block中的 Init block和update block拆分， 即将matmul中var_matmul_intermediate = float16(0)的操作拆分出来\n",
    "    # 注意现在b1通过之前的compute_at包含了很多初始化的计算(@TODO 实际看只有b22的赋初始值的过程)\n",
    "    # b40是b22的赋初始值过程 @TODO 没找到b20 b21 b22的内存转移操作在哪儿\n",
    "    b40 = sch.decompose_reduction(block=b1, loop=l16)\n",
    "    # b41只有计算sigmoid的过程\n",
    "    b41 = sch.get_block(name=\"compute\", func_name=\"main\")\n",
    "    # 将compute(sigmoid(z), z is the result of matmul block) inline消费者block中\n",
    "    sch.compute_inline(block=b41)\n",
    "    # b42包含 x*sigmoid\n",
    "    b42 = sch.get_block(name=\"T_multiply\", func_name=\"main\")\n",
    "    # 将T_multiply(x*y, where y is sigmoid(z), z is the result of matmul block) inline到它的生产者中\n",
    "    ## 由于此时compute已经inline到T_multiply中了, 所以T_multiply = (x*sigmoid(z)), 整体移动到matmul中去\n",
    "    sch.reverse_compute_inline(block=b42)\n",
    "    # 标注scheudle的后处理开始，@TODO不知道作用 \n",
    "    sch.enter_postproc()\n",
    "    # 去掉 annotate, 从这里可以猜测`enter_postproc`应该对annotate信息做了处理\n",
    "    sch.unannotate(block_or_loop=b22, ann_key=\"meta_schedule.cooperative_fetch\")\n",
    "    # 此时b22只包含 global memory -> shared memory的过程\n",
    "    # 察到shape: [[l10, l11], [1,1,4096]] 即b22的初始化还在[l10,l11]的loop内\n",
    "    l43, l44, l45, l46, l47 = sch.get_loops(block=b22)\n",
    "    # print(sch.get(l43))\n",
    "    # preserve_unit_iters作用不大，仅是否考虑计算 value为1 的loop\n",
    "    l48, l49, l50 = sch.split(loop=l47, factors=[None, 64, 8], preserve_unit_iters=True)\n",
    "    # l49 = 64, l50 = 8\n",
    "    sch.vectorize(loop=l50)\n",
    "    sch.bind(loop=l49, thread_axis=\"threadIdx.x\")\n",
    "    return sch.mod[\"main\"].with_attr(\"tir.is_scheduled\", 1)\n",
    "\n",
    "\n",
    "sch_manual = tvm.tir.Schedule(ModuleToManual)\n",
    "# sch_fused_decode5_fused_matmul6_silu1(sch_manual.mod[func_name])\n",
    "sch_manual.mod['main'] = sch_fused_decode5_fused_matmul6_silu1(sch_manual.mod[func_name])\n",
    "# print(sch_manual.mod.script())\n",
    "# rt_mod = tvm.build(sch_manual.mod, target=\"opencl\")\n",
    "# print(rt_mod.imported_modules[0].get_source())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tvm.tir.stmt.For'>\n",
      "# from tvm.script import tir as T\n",
      "\n",
      "@T.prim_func\n",
      "def before_compute_at(A: T.Buffer((128, 129), \"float32\"), C: T.Buffer((128, 129), \"float32\")):\n",
      "    # with T.block(\"root\"):\n",
      "    B = T.alloc_buffer((128, 129))\n",
      "    for i in range(128):\n",
      "        for ax0, ax1 in T.grid(1, 129):\n",
      "            with T.block(\"B\"):\n",
      "                vi = T.axis.spatial(128, i + ax0)\n",
      "                vj = T.axis.spatial(129, ax1)\n",
      "                T.reads(A[vi, vj])\n",
      "                T.writes(B[vi, vj])\n",
      "                B[vi, vj] = A[vi, vj] * T.float32(2)\n",
      "        for j in range(129):\n",
      "            with T.block(\"C\"):\n",
      "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
      "                T.reads(B[vi, vj])\n",
      "                T.writes(C[vi, vj])\n",
      "                C[vi, vj] = B[vi, vj] + T.float32(1)\n"
     ]
    }
   ],
   "source": [
    "### test examples\n",
    "from tvm import tir\n",
    "@T.prim_func\n",
    "def before_compute_at(a: T.handle, c: T.handle) -> None:\n",
    "    A = T.match_buffer(a, (128, 129), \"float32\")\n",
    "    B = T.alloc_buffer((128, 129), \"float32\")\n",
    "    C = T.match_buffer(c, (128, 129), \"float32\")\n",
    "    for i, j in T.grid(128, 129):\n",
    "        with T.block(\"B\"):\n",
    "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "            B[vi, vj] = A[vi, vj] * 2.0\n",
    "    for i, j in T.grid(128, 129):\n",
    "        with T.block(\"C\"):\n",
    "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "            C[vi, vj] = B[vi, vj] + 1.0\n",
    "\n",
    "\n",
    "exam_sch = tir.Schedule(before_compute_at)\n",
    "block = exam_sch.get_block(\"B\")\n",
    "loop, _ = exam_sch.get_loops(exam_sch.get_block(\"C\"))\n",
    "print(type(exam_sch.get(loop)))\n",
    "exam_sch.compute_at(block, loop, preserve_unit_loops=True)\n",
    "print(exam_sch.mod[\"main\"].script())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "\n",
      "@T.prim_func\n",
      "def before_split(A: T.Buffer((128, 128), \"float32\"), B: T.Buffer((128, 128), \"float32\")):\n",
      "    # with T.block(\"root\"):\n",
      "    for i_0, i_1, i_2, j in T.grid(2, 64, 1, 128):\n",
      "        with T.block(\"B\"):\n",
      "            vi = T.axis.spatial(128, i_0 * 64 + i_1 + i_2)\n",
      "            vj = T.axis.spatial(128, j)\n",
      "            T.reads(A[vi, vj])\n",
      "            T.writes(B[vi, vj])\n",
      "            B[vi, vj] = A[vi, vj] * T.float32(2)\n"
     ]
    }
   ],
   "source": [
    "# exam 2\n",
    "@T.prim_func\n",
    "def before_split(a: T.handle, b: T.handle) -> None:\n",
    "    A = T.match_buffer(a, (128, 128))\n",
    "    B = T.match_buffer(b, (128, 128))\n",
    "    for i, j in T.grid(128, 128):\n",
    "        with T.block(\"B\"):\n",
    "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "            B[vi, vj] = A[vi, vj] * 2.0\n",
    "\n",
    "split_sch = tir.Schedule(before_split)\n",
    "i, j = split_sch.get_loops(split_sch.get_block(\"B\"))\n",
    "split_sch.split(i, factors=[2, 64,1], preserve_unit_iters =True)\n",
    "print(split_sch.mod[\"main\"].script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda -keys=cuda,gpu -arch=sm_61 -max_num_threads=1024 -max_shared_memory_per_block=49152 -max_threads_per_block=1024 -registers_per_block=65536 -thread_warp_size=32\n"
     ]
    }
   ],
   "source": [
    "# run and compare with cuda\n",
    "import numpy as np\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_w_np = np.random.uniform(size=(w_w_x, w_y)).astype(\"uint32\")\n",
    "W_s_np = np.random.uniform(size=(w_s_x, w_y)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, x_shape)).astype(\"float16\")\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_evaluator GEMV-Blocking: 197.832523 GFLOPS\n",
      "[[[-0. -0. -0. ... -0. -0. -0.]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cuda优化版本测试\n",
    "manual_rt_mod = tvm.build(sch_manual.mod, target=\"cuda\")\n",
    "manual_evaluator = manual_rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "print(\"manual_evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / manual_evaluator(W_w_nd, W_s_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(Output_nd.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_evaluator GEMV-Blocking: 196.482495 GFLOPS\n",
      "[[[-0. -0. -0. ... -0. -0. -0.]]]\n"
     ]
    }
   ],
   "source": [
    "# cuda未优化版本测试\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "rt_mod = tvm.build(src_gpu_mod, target=\"cuda\")\n",
    "# print(rt_mod.get_source())\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "print(\"manual_evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / manual_evaluator(W_w_nd, W_s_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(Output_nd.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TVM_NDK_CC\"]=\"/home/sensetime/Android/Sdk/ndk/25.2.9519653/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android33-clang++\"\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.4.236.32\"\n",
    "rpc_port = 9190\n",
    "comp_target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-android\")  # TODO: Only support arm64 for now\n",
    "\n",
    "def test_opencl(mod: tvm.IRModule, name_hint: str):\n",
    "    # mod = tvm.lower(sch_manual.mod)\n",
    "    print(\"Build ...\")\n",
    "    android_rt_mod = tvm.build(mod, target=\"opencl\", target_host=\"llvm -mtriple=aarch64-linux-android\")\n",
    "    # print(android_rt_mod.imported_modules[0].get_source())\n",
    "    temp = utils.tempdir()\n",
    "    path_dso_cl = temp.relpath(\"dev_lib_cl.so\")\n",
    "    android_rt_mod.export_library(path_dso_cl, ndk.create_shared)\n",
    "\n",
    "    print(\"Run GPU(OpenCL Flavor) test ...\")\n",
    "    # Establish remote connection with target hardware\n",
    "\n",
    "    tracker = rpc.connect_tracker(rpc_host, rpc_port)\n",
    "    remote = tracker.request(device_key, priority=0, session_timeout=60)\n",
    "    print(\"Connect to device done.\")\n",
    "    dev = remote.cl(0)\n",
    "    remote.upload(path_dso_cl)\n",
    "    f1 = remote.load_module(\"dev_lib_cl.so\")\n",
    "\n",
    "    W_w_nd = tvm.nd.array(W_w_np, dev)\n",
    "    W_s_nd = tvm.nd.array(W_s_np, dev)\n",
    "    Input_nd = tvm.nd.array(Input_np, dev)\n",
    "    Output_nd = tvm.nd.array(np.zeros((1, 1, w_y), dtype=\"float16\"), dev)\n",
    "    test_number=10\n",
    "    time_f = f1.time_evaluator(f1.entry_name, dev, number=test_number)\n",
    "    cost = time_f(W_w_nd, W_s_nd, Input_nd, Output_nd).mean\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %f ms with loop %d\" % (name_hint, cost * 1000, test_number))\n",
    "    print(\"evaluator[%s] GEMV-Blocking: %f GFLOPS\" % (name_hint, num_flop / cost / 1e9))\n",
    "\n",
    "    return Output_nd.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[source] GEMV-Blocking: 2.538496 ms with loop 10\n",
      "evaluator[source] GEMV-Blocking: 483.911255 GFLOPS\n",
      "[[[-0. -0. -0. ... -0. -0. -0.]]]\n"
     ]
    }
   ],
   "source": [
    "# 未优化版本opencl测试\n",
    "from tvm import dlight as dl\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "with target:\n",
    "    # src_gpu_mod = tvm.tir.transform.DefaultGPUSchedule()(sch.mod) ##\n",
    "    mod_deploy = dl.ApplyDefaultSchedule(  # pylint: disable=not-callable\n",
    "        dl.gpu.Matmul(),\n",
    "        dl.gpu.GEMV(),\n",
    "        dl.gpu.Reduction(),\n",
    "        dl.gpu.GeneralReduction(),\n",
    "        dl.gpu.Fallback(),\n",
    "    )(sch.mod)\n",
    "src_output = test_opencl(mod_deploy, \"source\")\n",
    "print(src_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build ...\n",
      "Run GPU(OpenCL Flavor) test ...\n",
      "Connect to device done.\n",
      "evaluator[opted] GEMV-Blocking: 0.516531 ms with loop 10\n",
      "evaluator[opted] GEMV-Blocking: 2378.185062 GFLOPS\n",
      "[[[-0. -0. -0. ... -0. -0. -0.]]]\n"
     ]
    }
   ],
   "source": [
    "#优化版本opencl测试\n",
    "opt_output = test_opencl(sch_manual.mod, \"opted\")\n",
    "print(opt_output)\n",
    "np.testing.assert_equal(opt_output, src_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "target = tvm.target.Target(\"opencl -device=adreno\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n",
    "device_key=\"android\"\n",
    "rpc_host = \"10.158.176.30\"\n",
    "rpc_port = 5001\n",
    "# remote = autotvm.measure.request_remote(device_key, \"10.158.176.30\", 5001, timeout=10000)\n",
    "# dev = remote.device(str(target), 0)\n",
    "\n",
    "# num_flop = 1228406784\n",
    "# W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "# S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "# Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# # Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# W_nd = tvm.nd.array(W_np, dev)\n",
    "# S_nd = tvm.nd.array(S_np, dev)\n",
    "# Input_nd = tvm.nd.array(Input_np, dev)\n",
    "# Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpc_config = ms.runner.RPCConfig(tracker_host=rpc_host, tracker_port=rpc_port, tracker_key = device_key)\n",
    "runner= ms.runner.RPCRunner(rpc_config)\n",
    "# ms.builder.LocalBuilder()\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "database = ms.tune_tir(\n",
    "    mod=ModuleSrc,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_first\",\n",
    "    cost_model=\"xgb\",\n",
    "    runner = runner\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, sch.mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.script import relax as R\n",
    "@I.ir_module\n",
    "class Module:\n",
    "    @R.function\n",
    "    def main(A: R.Tensor((3, 4), dtype=\"float32\"), B: R.Tensor((4, 5), dtype=\"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv: R.Tensor((3, 5), dtype=\"float32\") = R.matmul(A, B)\n",
    "            gv: R.Tensor((3, 5), dtype=\"float32\") = lv\n",
    "            R.output(gv)\n",
    "        return gv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## auto_scheduler test\n",
    "from tvm import auto_scheduler\n",
    "import numpy as np\n",
    "a_np = np.random.rand(3, 4).astype(\"float32\")\n",
    "b_np = np.random.rand(4, 5).astype(\"float32\")\n",
    "a_nd = tvm.runtime.NDArray(a_np)\n",
    "b_nd = tvm.runtime.NDArray(b_np)\n",
    "sch = tvm.tir.Schedule(Module)\n",
    "\n",
    "params = {\"A\": a_np, \"B\": b_np}\n",
    "## 报错，这里只支持relay\n",
    "# tasks = auto_scheduler.extract_tasks(sch.mod, params, target=target)\n",
    "tasks = ms.relax_integration.extract_tasks(sch.mod, target=target, params=params)\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mod_deploy import Module as ModuleAll\n",
    "params_all = {}\n",
    "tasks_all = auto_scheduler.extract_tasks(ModuleAll, params_all, target=target)\n",
    "print(len(tasks_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "log_file = \"tune.json\"\n",
    "def _detect_local_cuda():\n",
    "    dev = tvm.cuda()\n",
    "    if not dev.exist:\n",
    "        return None\n",
    "    return tvm.target.Target(\n",
    "        {\n",
    "            \"kind\": \"cuda\",\n",
    "            \"max_shared_memory_per_block\": dev.max_shared_memory_per_block,\n",
    "            \"max_threads_per_block\": dev.max_threads_per_block,\n",
    "            \"thread_warp_size\": dev.warp_size,\n",
    "            \"registers_per_block\": 65536,\n",
    "            \"arch\": \"sm_\" + tvm.cuda().compute_version.replace(\".\", \"\"),\n",
    "        }\n",
    "    )\n",
    "# target = tvm.target.Target(\"cuda\", host=\"llvm\")\n",
    "target = _detect_local_cuda()\n",
    "\n",
    "print(target)\n",
    "# 定义计算任务\n",
    "dev = tvm.cuda(0)\n",
    "\n",
    "num_flop = 1228406784\n",
    "W_np = np.random.uniform(size=(512, vocab_size)).astype(\"uint32\")\n",
    "S_np = np.random.uniform(size=(128, vocab_size)).astype(\"float16\")\n",
    "Input_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "# Output_np = np.random.uniform(size=(1, 1, 4096)).astype(\"float16\")\n",
    "W_nd = tvm.nd.array(W_np, dev)\n",
    "S_nd = tvm.nd.array(S_np, dev)\n",
    "Input_nd = tvm.nd.array(Input_np, dev)\n",
    "Output_nd = tvm.nd.array(np.zeros((1, 1, vocab_size), dtype=\"float32\"), dev)\n",
    "sch = tvm.tir.Schedule(ModuleSrc)\n",
    "new_mod = sch.mod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task = auto_scheduler.SearchTask(func=sch.mod['fused_fused_decode11_fused_matmul5_cast2'], args=sch.mod['fused_fused_decode11_fused_matmul5_cast2'].params, target=target)\n",
    "\n",
    "# tune_option = auto_scheduler.TuningOptions(\n",
    "#     num_measure_trials=10,\n",
    "#     measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "#     verbose=2,\n",
    "# )\n",
    "\n",
    "\n",
    "database = ms.tune_tir(\n",
    "    mod=new_mod,\n",
    "    target=target,\n",
    "    max_trials_global=64,\n",
    "    num_trials_per_iter=64,\n",
    "    work_dir=\"./tune_45593_1\",\n",
    "    cost_model=\"xgb\"\n",
    ")\n",
    "print(len(database))\n",
    "sch1 = ms.tir_integration.compile_tir(database, new_mod, target)\n",
    "print(type(sch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sch1.trace)\n",
    "# print(sch1.mod.script())\n",
    "rt_mod = tvm.build(sch1.mod, target=\"cuda\")\n",
    "\n",
    "evaluator = rt_mod.time_evaluator(\"main\", dev, number=100)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (1228406784 / evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "record_database = ms.Database.create(kind='json', work_dir='./tune_45593_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_sch = ms.tir_integration.compile_tir(record_database, new_mod, target)\n",
    "\n",
    "record_rt_mod = tvm.build(record_sch.mod, target=\"cuda\")\n",
    "\n",
    "record_evaluator = record_rt_mod.time_evaluator(\"main\", dev, number=20)\n",
    "\n",
    "print(\"evaluator GEMV-Blocking: %f GFLOPS\" % (num_flop / record_evaluator(W_nd, S_nd, Input_nd, Output_nd).mean / 1e9))\n",
    "print(record_sch.trace)\n",
    "print(record_sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING, Dict, List, Optional, Union, Callable\n",
    "from tvm import runtime\n",
    "if TYPE_CHECKING:\n",
    "    import numpy as np  # type: ignore\n",
    "    from tvm.ir import IRModule\n",
    "    from tvm.meta_schedule.runner import EvaluatorConfig, RPCConfig\n",
    "    from tvm.runtime import Device, Module, NDArray\n",
    "    from tvm.target import Target\n",
    "    from tvm.runtime.vm import Executable\n",
    "\n",
    "\n",
    "def f_measurement(\n",
    "    rt_mod: runtime.Module, device: runtime.ndarray.Device, input_data: Dict[str, runtime.NDArray]\n",
    "):\n",
    "    vm = relax.VirtualMachine(rt_mod, device=device)\n",
    "    vm.save_function(\"main\", \"measure_func\", **input_data, include_return=False)\n",
    "    evaluator = vm.time_evaluator(\n",
    "        func_name=\"measure_func\",\n",
    "        dev=device,\n",
    "        repeat=100,\n",
    "        number=1,\n",
    "        min_repeat_ms=500,\n",
    "    )\n",
    "    return evaluator()\n",
    "\n",
    "def run_module_via_rpc(\n",
    "    rpc_config: \"RPCConfig\",\n",
    "    lib: Union[\"Module\", \"Executable\"],\n",
    "    dev_type: str,\n",
    "    args: Union[Dict[int, \"np.ndarray\"], Dict[str, \"np.ndarray\"]],\n",
    "    continuation: Callable,\n",
    "    backend: Optional[str] = \"graph\",\n",
    "):\n",
    "    \"\"\"Execute a tvm.runtime.Module on RPC remote\"\"\"\n",
    "    # pylint: disable=import-outside-toplevel\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    from tvm.contrib.tar import tar\n",
    "    from tvm.runtime import ndarray\n",
    "\n",
    "    # pylint: enable=import-outside-toplevel\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        # filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + tar.output_format)\n",
    "        filename = os.path.join(tmp_dir, \"tvm_tmp_mod.\" + \"so\")\n",
    "        if backend == \"vm\":\n",
    "            code, lib = lib.save(filename, fmt=\"so\")\n",
    "        from tvm.contrib import ndk\n",
    "        lib.export_library(filename, ndk.create_shared)\n",
    "        session = rpc_config.connect_server()\n",
    "        print(type(session._sess))\n",
    "        session.upload(filename)\n",
    "        _, filename = os.path.split(filename)\n",
    "        rt_mod = session.load_module(filename)\n",
    "        \n",
    "        if backend == \"vm\":\n",
    "            rt_mod = session.get_function(\"runtime.Load_Executable\")(code, rt_mod)\n",
    "            # rt_mod = session.get_function(\"runtime.module.loadfile_relax.Executable\")(filename)\n",
    "        dev = session.device(dev_type=dev_type, dev_id=0)\n",
    "        # print(dev)\n",
    "        # create the remote runtime module\n",
    "        print(rt_mod)\n",
    "        print(rt_mod['main'])\n",
    "        from tvm.contrib import graph_executor as runtime\n",
    "        module = runtime.GraphModule(rt_mod[\"main\"](dev))\n",
    "        print(module)\n",
    "        for k, v in args.items():\n",
    "            module.set_input(k, tvm.nd.array(v))\n",
    "        return module.run()\n",
    "        # nd_args = {k: ndarray.array(v, dev) for k, v in args.items()}\n",
    "        nd_args = {k: ndarray.empty(v.shape, v.dtype, dev) for k, v in args.items()}\n",
    "        return continuation(rt_mod, dev, nd_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
